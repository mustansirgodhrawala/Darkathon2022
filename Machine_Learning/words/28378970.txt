







    planet


    Skip Quicknav

     About Debian
     Getting Debian
     Support
     Developers' Corner


  Planet Debian





    July 16, 2022










      Steinar H. Gunderson






        Rust GUI advice


        The piece is largely about Rust, but Raph Levien's blog post about Rust GUI toolkits
contains some of the most thoughtful writings on GUI toolkits that I've seen
in a while, regardless of language. Recommended.


        16 July, 2022 09:15AM







    July 15, 2022






      Mike Hommey






        Announcing git-cinnabar 0.5.9


        Git-cinnabar is a git remote helper to interact with mercurial repositories. It allows to clone, pull and push from/to mercurial remote repositories, using git.
Get it on github.
These release notes are also available on the git-cinnabar wiki.
What’s new since 0.5.8?

Updated git to 2.37.1 for the helper.
Various python 3 fixes.
Fixed stream bundle
Added python and py.exe as executables tried on top of python3 and python2.
Improved handling of ill-formed local urls.
Fixed using old mercurial libraries that don’t support bundlev2 with a server that does.
When fsck reports the metadata as broken, prevent further updates to the repo.
When issue #207 is detected, mark the metadata as broken
Added support for logging redirection to a file
Now ignore refs/cinnabar/replace/ refs, and always use the corresponding metadata instead.
Various git cinnabar fsck fixes.



        15 July, 2022 10:11PM

         by glandium


















      Bits from Debian






        (Unofficial) Debian Perl Sprint 2022


        Three members of the Debian Perl Group
met in Hamburg between May 23 and May 30 2022 as part of the
Debian Reunion Hamburg
to continue perl development work for Bookworm and to work on QA tasks across
our 3800+ packages.
The participants had a good time and met other Debian friends. The
sprint was also productive:

pkg-perl-tools and dh-make-perl were improved and extended.
More than 50 uploads were done, and more than 30 bugs were fixed or at least triaged.
autopkgtests were added to lots of packages.
Some requests to remove obsolete packages were filed as well.

The more detailed report
was posted to the Debian Perl mailing list.
The participants would like to thank the Debian Reunion Hamburg organizers
for providing the framework for our sprint, all sponsors of the event, and
all donors to the Debian project who helped to cover parts of our expenses.



        15 July, 2022 03:35PM

         by gregor herrmann
















      Steve Kemp






        So we come to Lisp


        Recently I've been working with simple/trivial scripting languages, and I guess I finally reached a point where I thought "Lisp?  Why not".  One of the reasons for recent experimentation was thinking about the kind of minimalism that makes implementing a language less work - being able to actually use the language to write itself.

FORTH is my recurring example, because implementing it mostly means writing a virtual machine which consists of memory ("cells") along with a pair of stacks, and some primitives for operating upon them.   Once you have that groundwork in place you can layer the higher-level constructs (such as "for", "if", etc).

Lisp allows a similar approach, albeit with slightly fewer low-level details required, and far less tortuous thinking.  Lisp always feels higher-level to me anyway, given the explicit data-types ("list", "string", "number", etc).

Here's something that works in my toy lisp:

;; Define a function, `fact`, to calculate factorials (recursively).
(define fact (lambda (n)
  (if (<= n 1)
    1
      (* n (fact (- n 1))))))

;; Invoke the factorial function, using apply
(apply (list 1 2 3 4 5 6 7 8 9 10)
  (lambda (x)
    (print "%s! => %s" x (fact x))))


The core language doesn't have helpful functions to filter lists, or build up lists by applying a specified function to each member of a list, but adding them is trivial using the standard car, cdr, and simple recursion.  That means you end up writing lots of small functions like this:

(define zero? (lambda (n) (if (= n 0) #t #f)))
(define even? (lambda (n) (if (zero? (% n 2)) #t #f)))
(define odd?  (lambda (n) (! (even? n))))
(define sq    (lambda (x) (* x x)))


Once you have them you can use them in a way that feels simple and natural:

(print "Even numbers from 0-10: %s"
  (filter (nat 11) (lambda (x) (even? x))))

(print "Squared numbers from 0-10: %s"
  (map (nat 11) (lambda (x) (sq x))))


This all feels very sexy and simple, because the implementations of map, apply, filter are all written using the lisp - and they're easy to write.

Lisp takes things further than some other "basic" languages because of the (infamous) support for Macros.  But even without them writing new useful functions is pretty simple.  Where things struggle?  I guess I don't actually have a history of using lisp to actually solve problems - although it's great for configuring my editor..

Anyway I guess the journey continues.  Having looked at the obvious "minimal core" languages I need to go further afield:


Simple TCL-like scripting language.
Simple FORTH-like scripting language.
Yet another Lisp, in golang.


I'll make an attempt to look at some of the esoteric programming languages, and see if any of those are fun to experiment with.


        15 July, 2022 01:00AM












      Reproducible Builds (diffoscope)






        diffoscope 219 released


        The diffoscope maintainers are pleased to announce the release of diffoscope
version 219. This version includes the following changes:

* Don't traceback if we encounter an invalid Unicode character in Haskell
  versioning headers. (Closes: reproducible-builds/diffoscope#307)
* Update various copyright years.


You find out more by visiting the project homepage.


        15 July, 2022 12:00AM







    July 14, 2022










      Patryk Cisek






        Playing with NitroKey 3 -- PC runner using USBIP


        I’ve been wanting to use my brand new NitroKey 3, but TOTP is not supported yet. So, I’m looking to implement it myself, since firmware and tooling are open-source.
NitroKey 3’s firmware is based on Trussed framework. In essence, it’s been designed so that anyone can implement an independent Trussed application. Each such application is like a module that can be added to Trussed-based product. So if I write a Trussed app, I’d be able to add it to NK3’s firmware.


        14 July, 2022 11:01PM

         by l (Patryk Cisek (patryk@cisek.emai)







    July 13, 2022










      Dirk Eddelbuettel






        rfoaas 2.3.2: New upstream accessors



FOAAS by now moved to version 2.3.2 in its repo. This releases 2.3.2 of rfoaas catches up, and brings the first release in about two and a half years.
This 2.3.2 release of FOAAS brings us six new REST access points: absolutely(), dense(), dumbledore(), lowpoly(), understand(), and yeah(). Along with these new functions, documentation and tests were updated.
My CRANberries service provides a diff to the previous CRAN release. Questions, comments etc should go to the GitHub issue tracker. More background information is on the project page as well as on the github repo
If you like this or other open-source work I do, you can sponsor me at GitHub.

This post by Dirk Eddelbuettel originated on his Thinking inside the box blog. Please report excessive re-aggregation in third-party for-profit settings.



        13 July, 2022 11:08PM














      Reproducible Builds






        Reproducible Builds in June 2022




Welcome to the June 2022 report from the Reproducible Builds project. In these reports, we outline the most important things that we have been up to over the past month. As a quick recap, whilst anyone may inspect the source code of free software for malicious flaws, almost all software is distributed to end users as pre-compiled binaries.



Save the date!

Despite several delays, we are pleased to announce dates for our in-person summit this year:

November 1st 2022 — November 3rd 2022


The event will happen in/around Venice (Italy), and we intend to pick a venue reachable via the train station and an international airport. However, the precise venue will depend on the number of attendees.

Please see the announcement mail from Mattia Rizzolo, and do keep an eye on the mailing list for further announcements as it will hopefully include registration instructions.



News



David Wheeler filed an issue against the Rust programming language to report that builds are “not reproducible because full path to the source code is in the panic and debug strings”. Luckily, as one of the responses mentions: “the --remap-path-prefix solves this problem and has been used to great effect in build systems that rely on reproducibility (Bazel, Nix) to work at all” and that “there are efforts to teach cargo about it here”.



The Python Security team announced that:


  The ctx hosted project on PyPI was taken over via user account compromise and replaced with a malicious project which contained runtime code which collected the content of os.environ.items() when instantiating Ctx objects. The captured environment variables were sent as a base64 encoded query parameter to a Heroku application […]


As their announcement later goes onto state, version-pinning using “hash-checking mode” can prevent this attack, although this does depend on specific installations using this mode, rather than a prevention that can be applied systematically.





Developer vanitasvitae published an interesting and entertaining blog post detailing the blow-by-blow steps of debugging a reproducibility issue in PGPainless, a library which “aims to make using OpenPGP in Java projects as simple as possible”.

Whilst their in-depth research into the internals of the .jar may have been unnecessary given that diffoscope would have identified the, it must be said that there is something to be said with occasionally delving into seemingly “low-level” details, as well describing any debugging process. Indeed, as vanitasvitae writes:


  Yes, this would have spared me from 3h of debugging 😉 But I probably would also not have gone onto this little dive into the JAR/ZIP format, so in the end I’m not mad.




Kees Cook published a short and practical blog post detailing how he uses reproducibility properties to aid work to replace one-element arrays in the Linux kernel. Kees’ approach is based on the principle that if a (small) proposed change is considered equivalent by the compiler, then the generated output will be identical… but only if no other arbitrary or unrelated changes are introduced. Kees mentions the “fantastic” diffoscope tool, as well as various kernel-specific build options (eg. KBUILD_BUILD_TIMESTAMP) in order to “prepare my build with the ‘known to disrupt code layout’ options disabled”.





Stefano Zacchiroli gave a presentation at GDR Sécurité Informatique based in part on a paper co-written with Chris Lamb titled Increasing the Integrity of Software Supply Chains. (Tweet)



Debian



In Debian in this month, 28 reviews of Debian packages were added, 35 were updated and 27 were removed this month adding to our knowledge about identified issues. Two issue types were added:  nondeterministic_checksum_generated_by_coq and nondetermistic_js_output_from_webpack.

After Holger Levsen found hundreds of packages in the bookworm distribution that lack .buildinfo files, he uploaded 404 source packages to the archive (with no meaningful source changes). Currently bookworm now shows only 8 packages without .buildinfo files, and those 8 are fixed in unstable and should migrate shortly. By contrast, Debian unstable will always have packages without .buildinfo files, as this is how they come through the NEW queue. However, as these packages were not built on the official build servers (ie. they were uploaded by the maintainer) they will never migrate to Debian testing. In the future, therefore, testing should never have packages without .buildinfo files again.

Roland Clobus posted yet another in-depth status report about his progress making the Debian Live images build reproducibly to our mailing list. In this update, Roland mentions that “all major desktops build reproducibly with bullseye, bookworm and sid” but also goes on to outline the progress made with automated testing of the generated images using openQA.



GNU Guix



Vagrant Cascadian made a significant number of contributions to GNU Guix:



    Submitted patches to fix reproducibility issues in keyutils and isl as well as reported two bugs affecting reproducibility testing […][…].


    23 specific fixes related to reproducibility. [1][2][3][4][5][6][7][8][9][10][11][12][13][14][15][16][17][18][19][20][21][22][23]


    Proposed setting FORCE_SOURCE_DATE=1 in the environment of all builds in order to fix numerous timestamp issues in documentation generation tools.


    Identified reproducibility issues in the maradns package as it appears to embed a random prime number. (Patch)


    Responded in a thread to point out that GNU Guix already has the infrastructure in place to verify the reproducibility of downloaded substitutes for the vast majority of packages.


    Lastly, Vagrant performed an evaluation of the unreproducible packages that remain in the distribution.



Elsewhere in GNU Guix, Ludovic Courtès published a paper in the journal The Art, Science, and Engineering of Programming called Building a Secure Software Supply Chain with GNU Guix:


  This paper focuses on one research question: how can [Guix]((https://www.gnu.org/software/guix/) and similar systems allow users to securely update their software? […] Our main contribution is a model and tool to authenticate new Git revisions. We further show how, building on Git semantics, we build protections against downgrade attacks and related threats. We explain implementation choices. This work has been deployed in production two years ago, giving us insight on its actual use at scale every day. The Git checkout authentication at its core is applicable beyond the specific use case of Guix, and we think it could benefit to developer teams that use Git.


A full PDF of the text is available.



openSUSE



In the world of openSUSE, SUSE announced at SUSECon that they are preparing to meet SLSA level 4. (SLSA (Supply chain Levels for Software Artifacts) is a new industry-led standardisation effort that aims to protect the integrity of the software supply chain.)

However, at the time of writing, timestamps within RPM archives are not normalised, so bit-for-bit identical reproducible builds are not possible. Some in-toto provenance files published for SUSE’s SLE-15-SP4 as one result of the SLSA level 4 effort. Old binaries are not rebuilt, so only new builds (e.g. maintenance updates) have this metadata added.

Lastly, Bernhard M. Wiedemann posted his usual monthly openSUSE reproducible builds status report.



diffoscope



diffoscope is our in-depth and content-aware diff utility. Not only can it locate and diagnose reproducibility issues, it can provide human-readable diffs from many kinds of binary formats. This month, Chris Lamb prepared and uploaded versions 215, 216 and 217 to Debian unstable. Chris Lamb also made the following changes:



    New features:


      Print profile output if we were called with --profile and we were killed via a TERM signal. This should help in situations where diffoscope is terminated due to some sort of timeout. […]
      Support both PyPDF 1.x and 2.x. […]



    Bug fixes:


      Also catch IndexError exceptions (in addition to ValueError) when parsing .pyc files. (#1012258)
      Correct the logic for supporting different versions of the argcomplete module. […]



    Output improvements:


      Don’t leak the (likely-temporary) pathname when comparing PDF documents. […]



    Logging improvements:


      Update test fixtures for GNU readelf 2.38 (now in Debian unstable). […][…]
      Be more specific about the minimum required version of readelf (ie. binutils), as it appears that this ‘patch’ level version change resulted in a change of output, not the ‘minor’ version. […]
      Use our @skip_unless_tool_is_at_least decorator (NB. at_least) over @skip_if_tool_version_is (NB. is) to fix tests under Debian stable. […]
      Emit a warning if/when we are handling a UNIX TERM signal. […]



    Codebase improvements:


      Clarify in what situations the main finally block gets called with respect to TERM signal handling. […]
      Clarify control flow in the diffoscope.profiling module. […]
      Correctly package the scripts/ directory. […]




In addition, Edward Betts updated a broken link to the RSS on the diffoscope homepage and Vagrant Cascadian updated the diffoscope package in GNU Guix […][…][…].



Upstream patches

The Reproducible Builds project detects, dissects and attempts to fix as many currently-unreproducible packages as possible. We endeavour to send all of our patches upstream where appropriate. This month, we wrote a large number of such patches, including:



    Bernhard M. Wiedemann:


      build-compare caused a regression for a few days.
      python-fasttext (CPU-related issue).



    Chris Lamb:


      #1012614 filed against node-dommatrix.
      #1012766 filed against rtpengine.
      #1012790 filed against sphinxcontrib-mermaid.
      #1012792 filed against yaru-theme.
      #1012836 filed against mapproxy (forwarded upstream).
      #1013257 filed against libxsmm.
      #1014041 filed against yt-dlp (forwarded upstream).
      #891263 was filed against puppet in February 2018 and the patch was finally proposed for inclusion upstream.






Testing framework



The Reproducible Builds project runs a significant testing framework at tests.reproducible-builds.org, to check packages and other artifacts for reproducibility. This month, the following changes were made:



    Holger Levsen:


      Add a package set for packages that use the R programming language […] as well as one for Rust […].
      Improve package set matching for Python […] and font-related […] packages.
      Install the lz4, lzop and xz-utils packages on all nodes in order to detect running kernels. […]
      Improve the cleanup mechanisms when testing the reproducibility of Debian Live images. […][…]
      In the automated node health checks, deprioritise the “generic kernel warning”. […]



    Roland Clobus (Debian Live image reproducibility):


      Add various maintenance jobs to the Jenkins view. […]
      Cleanup old workspaces after 24 hours. […]
      Cleanup temporary workspace and resulting directories. […]
      Implement a number of fixes and improvements around publishing files. […][…][…]
      Don’t attempt to preserve the file timestamps when copying artifacts. […]




And finally, node maintenance was also performed by Mattia Rizzolo […].



Mailing list and website

On our mailing list this month:



    David Wheeler started a thread stating his desire that reproducible builds and GitBOM are able to work together simultaneously. David first describes the goals of both GitBOM and reproducibility, outlines the potential problems and even outlines a number of prospective solutions.


    In a similar vein, David Wheeler also posted about the problems with Profile-Guided Optimisation (PGO) in relation to reproducible builds.


    Roland Clobus “copied in” our mailing list with a question about whether enabling link-time optimisations (LTO) in Debian as a whole might cause reproducibility problems.


    Mattia Rizzolo posted a request for assistance regarding the translations of our website.



Lastly, Chris Lamb updated the main Reproducible Builds website and documentation in a number of small ways, but primarily published an interview with Hans-Christoph Steiner of the F-Droid project. Chris Lamb also added a Coffeescript example for parsing and using the SOURCE_DATE_EPOCH environment variable […]. In addition, Sebastian Crane very-helpfully updated the screenshot of salsa.debian.org’s “request access” button on the How to join the Salsa group. […]



Contact

If you are interested in contributing to the Reproducible Builds project, please visit our Contribute page on our website. However, you can get in touch with us via:



    IRC: #reproducible-builds on irc.oftc.net.


    Twitter: @ReproBuilds


    Mailing list: rb-general@lists.reproducible-builds.org




        13 July, 2022 01:58PM







    July 12, 2022










      Matthew Garrett






        Responsible stewardship of the UEFI secure boot ecosystem


        After I mentioned that Lenovo are now shipping laptops that only boot Windows by default, a few people pointed to a Lenovo document that says:Starting in 2022 for Secured-core PCs it is a Microsoft requirement for the 3rd Party Certificate to be disabled by default."Secured-core" is a term used to describe machines that meet a certain set of Microsoft requirements around firmware security, and by and large it's a good thing - devices that meet these requirements are resilient against a whole bunch of potential attacks in the early boot process. But unfortunately the 2022 requirements don't seem to be publicly available, so it's difficult to know what's being asked for and why. But first, some background.Most x86 UEFI systems that support Secure Boot trust at least two certificate authorities:1) The Microsoft Windows Production PCA - this is used to sign the bootloader in production Windows builds. Trusting this is sufficient to boot Windows.2) The Microsoft Corporation UEFI CA - this is used by Microsoft to sign non-Windows UEFI binaries, including built-in drivers for hardware that needs to work in the UEFI environment (such as GPUs and network cards) and bootloaders for non-Windows.The apparent secured-core requirement for 2022 is that the second of these CAs should not be trusted by default. As a result, drivers or bootloaders signed with this certificate will not run on these systems. This means that, out of the box, these systems will not boot anything other than Windows[1].Given the association with the secured-core requirements, this is presumably a security decision of some kind. Unfortunately, we have no real idea what this security decision is intended to protect against. The most likely scenario is concerns about the (in)security of binaries signed with the third-party signing key - there are some legitimate concerns here, but I'm going to cover why I don't think they're terribly realistic.The first point is that, from a boot security perspective, a signed bootloader that will happily boot unsigned code kind of defeats the point. Kaspersky did it anyway. The second is that even a signed bootloader that is intended to only boot signed code may run into issues in the event of security vulnerabilities - the Boothole vulnerabilities are an example of this, covering multiple issues in GRUB that could allow for arbitrary code execution and potential loading of untrusted code.So we know that signed bootloaders that will (either through accident or design) execute unsigned code exist. The signatures for all the known vulnerable bootloaders have been revoked, but that doesn't mean there won't be other vulnerabilities discovered in future. Configuring systems so that they don't trust the third-party CA means that those signed bootloaders won't be trusted, which means any future vulnerabilities will be irrelevant. This seems like a simple choice?There's actually a couple of reasons why I don't think it's anywhere near that simple. The first is that whenever a signed object is booted by the firmware, the trusted certificate used to verify that object is measured into PCR 7 in the TPM. If a system previously booted with something signed with the Windows Production CA, and is now suddenly booting with something signed with the third-party UEFI CA, the values in PCR 7 will be different. TPMs support "sealing" a secret - encrypting it with a policy that the TPM will only decrypt it if certain conditions are met. Microsoft make use of this for their default Bitlocker disk encryption mechanism. The disk encryption key is encrypted by the TPM, and associated with a specific PCR 7 value. If the value of PCR 7 doesn't match, the TPM will refuse to decrypt the key, and the machine won't boot. This means that attempting to attack a Windows system that has Bitlocker enabled using a non-Windows bootloader will fail - the system will be unable to obtain the disk unlock key, which is a strong indication to the owner that they're being attacked.The second is that this is predicated on the idea that removing the third-party bootloaders and drivers removes all the vulnerabilities. In fact, there's been rather a lot of vulnerabilities in the Windows bootloader. A broad enough vulnerability in the Windows bootloader is arguably a lot worse than a vulnerability in a third-party loader, since it won't change the PCR 7 measurements and the system will boot happily. Removing trust in the third-party CA does nothing to protect against this.The third reason doesn't apply to all systems, but it does to many. System vendors frequently want to ship diagnostic or management utilities that run in the boot environment, but would prefer not to have to go to the trouble of getting them all signed by Microsoft. The simple solution to this is to ship their own certificate and sign all their tooling directly - the secured-core Lenovo I'm looking at currently is an example of this, with a Lenovo signing certificate. While everything signed with the third-party signing certificate goes through some degree of security review, there's no requirement for any vendor tooling to be reviewed at all. Removing the third-party CA does nothing to protect the user against the code that's most likely to contain vulnerabilities.Obviously I may be missing something here - Microsoft may well have a strong technical justification. But they haven't shared it, and so right now we're left making guesses. And right now, I just don't see a good security argument.But let's move on from the technical side of things and discuss the broader issue. The reason UEFI Secure Boot is present on most x86 systems is that Microsoft mandated it back in 2012. Microsoft chose to be the only trusted signing authority. Microsoft made the decision to assert that third-party code could be signed and trusted.We've certainly learned some things since then, and a bunch of things have changed. Third-party bootloaders based on the Shim infrastructure are now reviewed via a community-managed process. We've had a productive coordinated response to the Boothole incident, which also taught us that the existing revocation strategy wasn't going to scale. In response, the community worked with Microsoft to develop a specification for making it easier to handle similar events in future. And it's also worth noting that after the initial Boothole disclosure was made to the GRUB maintainers, they proactively sought out other vulnerabilities in their codebase rather than simply patching what had been reported. The free software community has gone to great lengths to ensure third-party bootloaders are compatible with the security goals of UEFI Secure Boot.So, to have Microsoft, the self-appointed steward of the UEFI Secure Boot ecosystem, turn round and say that a bunch of binaries that have been reviewed through processes developed in negotiation with Microsoft, implementing technologies designed to make management of revocation easier for Microsoft, and incorporating fixes for vulnerabilities discovered by the developers of those binaries who notified Microsoft of these issues despite having no obligation to do so, and which have then been signed by Microsoft are now considered by Microsoft to be insecure is, uh, kind of impolite? Especially when unreviewed vendor-signed binaries are still considered trustworthy, despite no external review being carried out at all.If Microsoft had a set of criteria used to determine whether something is considered sufficiently trustworthy, we could determine which of these we fell short on and do something about that. From a technical perspective, Microsoft could set criteria that would allow a subset of third-party binaries that met additional review be trusted without having to trust all third-party binaries[2]. But, instead, this has been a decision made by the steward of this ecosystem without consulting major stakeholders.If there are legitimate security concerns, let's talk about them and come up with solutions that fix them without doing a significant amount of collateral damage. Don't complain about a vendor blocking your apps and then do the same thing yourself.[Edit to add: there seems to be some misunderstanding about where this restriction is being imposed. I bought this laptop because I'm interested in investigating the Microsoft Pluton security processor, but Pluton is not involved at all here. The restriction is being imposed by the firmware running on the main CPU, not any sort of functionality implemented on Pluton][1] They'll also refuse to run any drivers that are stored in flash on Thunderbolt devices, which means eGPU setups may be more complicated, as will netbooting off Thunderbolt-attached NICs[2] Use a different leaf cert to sign the new trust tier, add the old leaf cert to dbx unless a config option is set, leave the existing intermediate in db comments


        12 July, 2022 05:50AM







    July 09, 2022






      Andrew Cater






        20220709 2100 UTC - Finished Debian media testing for the day


         I've just finished my last test: Sledge is finishing his and will then push the release out. Today's been a bit slow and steady - but we've finally got there.Thanks, as ever, due to the release team for actually giving us an update, the press team for announcements - and, of course, the various sponsors, administrators and maintainers of Debian infrastructure like cdimage.debian.org and the CD building machines.It's been a quiet release for the media team in terms of participation - we've not had our usual tester for debian-edu and it's been a bit subdued altogether.Not even as many blog posts as usual: I suppose I'll make up for it in August at the BBQ in Cambridge - if we don't all get another lockdown / COVID-19 variants / fuel prices at ££££ per litre to dissuade us.


        09 July, 2022 09:07PM

         by Andrew Cater (noreply@blogger.com)















        Testing 11.4 Debian media images - almost finished - 20220709 1933 UTC


         We're flagging a bit now, I think but close to the end. The standard Debian images caused no problems: Sledge and I are just finishing up the last few live images to test now.Thanks, as ever, to the crew: RattusRattus and Isy, Sledge struggling through feeling awful. No debian-edu testing today, unfortunately, but that almost never breaks anyway.Everyone's getting geared up for Kosovo - you'll see the other three there with any luck - and you'd catch all of us at the BBQ in Cambridge. It's going to be a hugely busy month and a bit for Steve and the others. :)


        09 July, 2022 07:34PM

         by Andrew Cater (noreply@blogger.com)













        As has become traditional - blogging as part of the media release for Debian 11.4 - 202207091436 UTC


         A lower profile release today: Sledge working in the background as affected by COVID. RattusRattus and Isy doing sterling service on the other side of Cambridge, /me over here.Testing on the standard install media is pretty much done: Isy, Andy and Sledge have moved on to testing the live images.Stupidly hot for UK - it's 28 degrees indoors with windows open.All good so far :)


        09 July, 2022 02:37PM

         by Andrew Cater (noreply@blogger.com)
















      Dirk Eddelbuettel






        Rcpp 1.0.9 on CRAN: Regular Updates



The Rcpp team is please to announce the newest release 1.0.9 of Rcpp which hit CRAN late yesterday, and has been uploaded to Debian as well. Windows and macOS builds should appear at CRAN in the next few days, as will builds in different Linux distribution and of course at r2u. The release was prepared om July 2, but it took a few days to clear a handful of spurious errors as false positives with CRAN — this can when the set of reverse dependencies is so large, and the CRAN team remains busy. This release continues with the six-months cycle started with release 1.0.5 in July 2020. (This time, CRAN had asked for an interim release to silence a C++ warning; we then needed a quick follow-up to tweak tests.) As a reminder, interim ‘dev’ or ‘rc’ releases should generally be available in the Rcpp drat repo. These rolling release tend to work just as well, and are also fully tested against all reverse-dependencies.
Rcpp has become the most popular way of enhancing R with C or C++ code. Right now, around 2559 packages on CRAN depend on Rcpp for making analytical code go faster and further, along with 252 in BioConductor. On CRAN, 13.9% of all packages depend (directly) on CRAN, and 58.5% of all compiled packages do. From the cloud mirror of CRAN (which is but a subset of all CRAN downloads), Rcpp has been downloaded 61.5 million times.
This release is incremental and extends Rcpp with a number of small improvements all detailed in the NEWS file as well as below. We want to highlight the external contributions: a precious list tag is cleared on removal, and a move constructor and assignment for strings has been added (thanks to Dean Scarff), and (thanks to Bill Denney and Marco Colombo) two minor errors are corrected in the vignette documentation. A big Thank You! to everybody who contributed pull request, opened or answered issues, or questions at StackOverflow or on the mailing list.
The full list of details follows.

Changes in Rcpp hotfix release version 1.0.9 (2022-07-02)

Changes in Rcpp API:

Accomodate C++98 compilation by adjusting attributes.cpp (Dirk in #1193 fixing #1192)
Accomodate newest compilers replacing deprecated std::unary_function and std::binary_function with std::function (Dirk in #1202 fixing #1201 and CRAN request)
Upon removal from precious list, the tag is set to null (Iñaki in #1205 fixing #1203)
Move constructor and assignment for strings have been added (Dean Scarff in #1219).

Changes in Rcpp Documentation:

Adjust one overflowing column (Bill Denney in #1196 fixing #1195)
Correct a typo in the FAQ (Marco Colombo in #1217)

Changes in Rcpp Deployment:

Accomodate four digit version numbers in unit test (Dirk)
Do not run complete test suite to limit test time to CRAN preference (Dirk in #1206)
Small updates to the CI test containers have been made
Some of changes also applied to an interim release 1.0.8.3 made for CRAN on 2022-03-14.



Thanks to my CRANberries, you can also look at a diff to the previous release. Questions, comments etc should go to the rcpp-devel mailing list off the R-Forge page. Bugs reports are welcome at the GitHub issue tracker as well (where one can also search among open or closed issues); questions are also welcome under rcpp tag at StackOverflow which also allows searching among the (currently) 2886 previous questions.
If you like this or other open-source work I do, you can sponsor me at GitHub.

This post by Dirk Eddelbuettel originated on his Thinking inside the box blog. Please report excessive re-aggregation in third-party for-profit settings.



        09 July, 2022 02:08PM







    July 08, 2022










      Bits from Debian






        DebConf22 Cheese and Wine Party


        In less than two days we will be in Prizren to start
DebCamp and
DebConf22 \o/
This C&W is the 18th official DebConf Cheese and Wine party. The first C&W was
improvised in Helsinki during DebConf 5, in the so-called "French" room. Cheese
and Wine parties are now a tradition for DebConf.
The event is very simple: bring good edible stuff from your country. We like
cheese and wine, but we love the surprising stuff that people bring from all
around the world or regions of Kosovo. So, you can bring non-alcoholic drinks
or a typical food that you would like to share as well. Even if you don't
bring anything, feel free to participate: our priorities are our attendants
and free cheese.
We have to organize for a great party. An important part is planning - We want
to know what you are bringing, in order to prepare the labels and organizing
other things.
So, please go to our
wiki page
and add what you will bring!
If you don't have time to buy before travel, we list some places where you can
buy cheese and wine in . There are more information about C&W, what
you can bring, vegan cheese, Kosovo customs regulations and non-alcoholic
drinks at our site.
C&W will happen on July 19th, 2022 (Tuesday) after 19h30min.
We are looking forward to seeing you all here!



        08 July, 2022 04:00PM

         by The Debian Publicity Team


















      Junichi Uekawa






        Created a HTML app to generate my weekly notes.


        Created a HTML app to generate my weekly notes.
	  For writing TODO items and log of what I did each hour.
	  I print this out every week and write notes on it, mostly to help myself remember what I was doing at that time.
	  I don't seem to have good short term memory and this helps me focus, at least tells me when I am not.
	  I rewrote this in SVG instead of a drawing app because I didn't feel like changing anything in that drawing app considering there's lots of repetition.
	  However, I'm not sure if this is any better.
	  I don't know how not to print the second empty page either.



        08 July, 2022 12:30PM

         by Junichi Uekawa
















      Matthew Garrett






        Lenovo shipping new laptops that only boot Windows by default


        I finally managed to get hold of a Thinkpad Z13 to examine a functional implementation of Microsoft's Pluton security co-processor. Trying to boot Linux from a USB stick failed out of the box for no obvious reason, but after further examination the cause became clear - the firmware defaults to not trusting bootloaders or drivers signed with the Microsoft 3rd Party UEFI CA key. This means that given the default firmware configuration, nothing other than Windows will boot. It also means that you won't be able to boot from any third-party external peripherals that are plugged in via Thunderbolt.There's no security benefit to this. If you want security here you're paying attention to the values measured into the TPM, and thanks to Microsoft's own specification for measurements made into PCR 7, switching from booting Windows to booting something signed with the 3rd party signing key will change the measurements and invalidate any sealed secrets. It's trivial to detect this. Distrusting the 3rd party CA by default doesn't improve security, it just makes it harder for users to boot alternative operating systems.Lenovo, this isn't OK. The entire architecture of UEFI secure boot is that it allows for security without compromising user choice of OS. Restricting boot to Windows by default provides no security benefit but makes it harder for people to run the OS they want to. Please fix it. comments


        08 July, 2022 06:49AM












      Reproducible Builds (diffoscope)






        diffoscope 218 released


        The diffoscope maintainers are pleased to announce the release of diffoscope
version 218. This version includes the following changes:

* Improve output of Markdown and reStructuredText to use code blocks with
  syntax highlighting. (Closes: reproducible-builds/diffoscope#306)


You find out more by visiting the project homepage.


        08 July, 2022 12:00AM







    July 07, 2022










      Jonathan Dowland






        Musick To Play In The Dark 2


        This took a while to arrive! After the success of the reissue of
Coil's Musick To Play In The Dark, it was inevitable that the
second edition would also be reissued. The pre-order opened late
November 2021 and mine arrived in late April this year.









I was toying with the idea of ordering one of the most exclusive editions
direct from DAIS Records, in particular the
glow in the dark one, but with international shipping the cost was pretty high.
I went with a UK distributor
(Boomkat)
instead, who had their own exclusive edition: black-in-purple-in-clear.









I'm happy with my decision: it's one of the most interesting variants I own,
and from what I've read, glow in the dark vinyl never sounds great anyway.
(I think I have at least one glow in the dark 7" single somewhere)

Stand-out track: Tiny Golden Books

Since I didn't say so last time, the stand-out track on volume one is Red
Birds Will Fly Out of the East and Destroy Paris in a
Night,
but both volumes are full of really interesting moments (🎵 One day, your eggs
are going to hatch and some very strange birds are going to emerge. 🎵)


        07 July, 2022 01:54PM







    July 06, 2022






      Russ Allbery






        Review: A Master of Djinn


        Review: A Master of Djinn, by P. Djèlí Clark



    Series:
    Dead Djinn Universe #1


    Publisher:
    Tordotcom


    Copyright:
    2021


    ISBN:
    1-250-26767-6


    Format:
    Kindle


    Pages:
    391



A Master of Djinn is the first novel in the Dead Djinn Universe,
but (as you might guess from the series title) is a direct sequel to the
novelette "A Dead Djinn in Cairo".  The
novelette is not as good as the novel, but I recommend reading it first
for the character introductions and some plot elements that carry over.
Reading The Haunting of Tram Car 015
first is entirely optional.



In 1912 in a mansion in Giza, a secret society of (mostly) British men is
meeting.  The Hermetic Brotherhood of Al-Jahiz is devoted to unlocking the
mysteries of the Soudanese mystic al-Jahiz.  In our world, these men would
likely be colonialist plunderers.  In this world, they still aspire to
that role, but they're playing catch-up.  Al-Jahiz bored into the Kaf,
releasing djinn and magic into the world and making Egypt a world power in
its own right.  Now, its cities are full of clockwork marvels, djinn walk
the streets as citizens, and British rule has been ejected from India and
Africa by local magic.  This group of still-rich romantics and crackpots
hopes to discover the knowledge lost when al-Jahiz disappeared.  They have
not had much success.



This will not save their lives.



Fatma el-Sha'arawi is a special investigator for the Ministry of Alchemy,
Enchantments, and Supernatural Entities.  Her job is sorting out the
problems caused by this new magic, such as a couple of young thieves with
a bottle full of sleeping djinn whose angry reaction to being unexpectedly
woken has very little to do with wishes.  She is one of the few female
investigators in a ministry that is slowly modernizing with the rest of
society (Egyptian women just got the vote).  She's also the one called to
investigate the murder of a secret society of British men and a couple of
Cairenes by a black-robed man in a golden mask.



The black-robed man claims to be al-Jahiz returned, and proves to be
terrifyingly adept at manipulating crowds and sparking popular dissent.
Fatma and the Ministry's first attempt to handle him is a poorly-judged
confrontation stymied by hostile crowds, the man's duplicating bodyguard,
and his own fighting ability.  From there, it's a race between Fatma's
pursuit of linear clues and the black-robed man's efforts to destabilize
society.



This, like the previous short stories, is a police procedural, but it has
considerably more room to breathe at novel length.  That serves it well,
since as with "A Dead Djinn in Cairo" the procedural part is linear,
reactive vehicle for plot exposition.  I was more invested in Fatma's
relationships with the supporting characters.  Since the previous story,
she's struck up a romance with Siti, a highly competent follower of the
old Egyptian gods (Hathor in particular) and my favorite character in the
book.  She's also been assigned a new partner, Hadia, a new graduate and
another female agent.  The slow defeat of Fatma's irritation at not being
allowed to work alone by Hadia's cheerful competence and persistence (and
willingness to do paperwork) adds a lot to the characterization.



The setting felt a bit less atmospheric than The Haunting of Tram
Car 015, but we get more details of international politics, and they're a
delight.  Clark takes obvious (and warranted) glee in showing how the
reintroduction of magic has shifted the balance of power away from the
colonial empires.  Cairo is a bustling steampunk metropolis and capital of
a world power, welcoming envoys from West African kingdoms alongside the
(still racist and obnoxious but now much less powerful) British and other
Europeans.  European countries were forced to search their own mythology
for possible sources of magic power, which leads to the hilarious scene of
the German Kaiser carrying a sleepy goblin on his shoulder to monitor his
diplomacy.



The magic of the story was less successful for me, although still
enjoyable.  The angels from "A Dead Djinn in Cairo" make another
appearance and again felt like the freshest bit of world-building, but we
don't find out much more about them.  I liked the djinn and their
widely-varied types and magic, but apart from them and a few glimpses of
Egypt's older gods, that was the extent of the underlying structure.
There is a significant magical artifact, but the characters are
essentially handed an instruction manual, use it according to its
instructions, and it then does what it was documented to do.  It was a bit
unsatisfying.  I'm the type of fantasy reader who always wants to read the
sourcebook for the magic system, but this is not that sort of a book.



Instead, it's the kind of book where the investigator steadily follows a
linear trail of clues and leads until they reach the final confrontation.
Here, the confrontation felt remarkably like cut scenes from a Japanese
RPG: sudden vast changes in scale, clockwork constructs, massive monsters,
villains standing on mobile platforms, and surprise combat reversals.  I
could almost hear the fight music and see the dialog boxes pop up.  This
isn't exactly a complaint — I love Japanese RPGs — but it did add to the
feeling that the plot was on rails and didn't require many decisions from
the protagonist.  Clark also relies on an overused plot cliche in the
climactic battle, which was a minor disappointment.



A Master of Djinn won the Nebula for best 2021 novel, I suspect
largely on the basis of its setting and refreshingly non-European magical
system.  I don't entirely agree; the writing is still a bit clunky, with
unnecessary sentences and stock phrases showing up here and there, and I
think it suffers from the typical deficiencies of SFF writers writing
mysteries or police procedurals without the plot sophistication normally
found in that genre.  But this is good stuff for a first novel, with fun
supporting characters (loved the librarian) and some great world-building.
I would happily read more in this universe.


Rating: 7 out of 10


        06 July, 2022 03:04AM







    July 05, 2022






      Alberto García






        Running the Steam Deck’s OS in a virtual machine using QEMU



Introduction
The Steam Deck is a handheld gaming computer that runs a Linux-based operating system called SteamOS. The machine comes with SteamOS 3 (code name “holo”), which is in turn based on Arch Linux.
Although there is no SteamOS 3 installer for a generic PC (yet), it is very easy to install on a virtual machine using QEMU. This post explains how to do it.
The goal of this VM is not to play games (you can already install Steam on your computer after all) but to use SteamOS in desktop mode. The Gamescope mode (the console-like interface you normally see when you use the machine) requires additional development to make it work with QEMU and will not work with these instructions.
A SteamOS VM can be useful for debugging, development, and generally playing and tinkering with the OS without risking breaking the Steam Deck.
Running the SteamOS desktop in a virtual machine only requires QEMU and the OVMF UEFI firmware and should work in any relatively recent distribution. In this post I’m using QEMU directly, but you can also use virt-manager or some other tool if you prefer, we’re emulating a standard x86_64 machine here.
General concepts
SteamOS is a single-user operating system and it uses an A/B partition scheme, which means that there are two sets of partitions and two copies of the operating system. The root filesystem is read-only and system updates happen on the partition set that is not active. This allows for safer updates, among other things.
There is one single /home partition, shared by both partition sets. It contains the games, user files, and anything that the user wants to install there.
Although the user can trivially become root, make the root filesystem read-write and install or change anything (the pacman package manager is available), this is not recommended because

it increases the chances of breaking the OS, and
any changes will disappear with the next OS update.

A simple way for the user to install additional software that survives OS updates and doesn’t touch the root filesystem is Flatpak. It comes preinstalled with the OS and is integrated with the KDE Discover app.
Preparing all necessary files
The first thing that we need is the installer. For that we have to download the Steam Deck recovery image from here: https://store.steampowered.com/steamos/download/?ver=steamdeck&snr=
Once the file has been downloaded, we can uncompress it and we’ll get a raw disk image called steamdeck-recovery-4.img (the number may vary).
Note that the recovery image is already SteamOS (just not the most up-to-date version). If you simply want to have a quick look you can play a bit with it and skip the installation step. In this case I recommend that you extend the image before using it, for example with ‘truncate -s 64G steamdeck-recovery-4.img‘ or, better, create a qcow2 overlay file and leave the original raw image unmodified: ‘qemu-img create -f qcow2 -F raw -b steamdeck-recovery-4.img steamdeck-recovery-extended.qcow2 64G‘
But here we want to perform the actual installation, so we need a destination image. Let’s create one:
$ qemu-img create -f qcow2 steamos.qcow2 64G
Installing SteamOS
Now that we have all files we can start the virtual machine:
$ qemu-system-x86_64 -enable-kvm -smp cores=4 -m 8G \
    -device usb-ehci -device usb-tablet \
    -device intel-hda -device hda-duplex \
    -device VGA,xres=1280,yres=800 \
    -drive if=pflash,format=raw,readonly=on,file=/usr/share/ovmf/OVMF.fd \
    -drive if=virtio,file=steamdeck-recovery-4.img,driver=raw \
    -device nvme,drive=drive0,serial=badbeef \
    -drive if=none,id=drive0,file=steamos.qcow2

Note that we’re emulating an NVMe drive for steamos.qcow2 because that’s what the installer script expects. This is not strictly necessary but it makes things a bit easier. If you don’t want to do that you’ll have to edit ~/tools/repair_device.sh and change DISK and DISK_SUFFIX.

Once the system has booted we’ll see a KDE Plasma session with a few tools on the desktop. If we select “Reimage Steam Deck” and click “Proceed” on the confirmation dialog then SteamOS will be installed on the destination drive. This process should not take a long time.
Now, once the operation finishes a new confirmation dialog will ask if we want to reboot the Steam Deck, but here we have to choose “Cancel”. We cannot use the new image yet because it would try to boot into the Gamescope session, which won’t work, so we need to change the default desktop session.
SteamOS comes with a helper script that allows us to enter a chroot after automatically mounting all SteamOS partitions, so let’s open a Konsole and make the Plasma session the default one in both partition sets:
$ sudo steamos-chroot --disk /dev/nvme0n1 --partset A
# steamos-readonly disable
# echo '[Autologin]' > /etc/sddm.conf.d/zz-steamos-autologin.conf
# echo 'Session=plasma.desktop' >> /etc/sddm.conf.d/zz-steamos-autologin.conf
# steamos-readonly enable
# exit

$ sudo steamos-chroot --disk /dev/nvme0n1 --partset B
# steamos-readonly disable
# echo '[Autologin]' > /etc/sddm.conf.d/zz-steamos-autologin.conf
# echo 'Session=plasma.desktop' >> /etc/sddm.conf.d/zz-steamos-autologin.conf
# steamos-readonly enable
# exit

After this we can shut down the virtual machine. Our new SteamOS drive is ready to be used. We can discard the recovery image now if we want.
Booting SteamOS and first steps
To boot SteamOS we can use a QEMU line similar to the one used during the installation. This time we’re not emulating an NVMe drive because it’s no longer necessary.
$ cp /usr/share/OVMF/OVMF_VARS.fd .
$ qemu-system-x86_64 -enable-kvm -smp cores=4 -m 8G \
   -device usb-ehci -device usb-tablet \
   -device intel-hda -device hda-duplex \
   -device VGA,xres=1280,yres=800 \
   -drive if=pflash,format=raw,readonly=on,file=/usr/share/ovmf/OVMF.fd \
   -drive if=pflash,format=raw,file=OVMF_VARS.fd \
   -drive if=virtio,file=steamos.qcow2 \
   -device virtio-net-pci,netdev=net0 \
   -netdev user,id=net0,hostfwd=tcp::2222-:22

(the last two lines redirect tcp port 2222 to port 22 of the guest to be able to SSH into the VM. If you don’t want to do that you can omit them)
If everything went fine, you should see KDE Plasma again, this time with a desktop icon to launch Steam and another one to “Return to Gaming Mode” (which we should not use because it won’t work). See the screenshot that opens this post.
Congratulations, you’re running SteamOS now. Here are some things that you probably want to do:

(optional) Change the keyboard layout in the system settings (the default one is US English)
Set the password for the deck user: run ‘passwd‘ on a terminal
Enable / start the SSH server: ‘sudo systemctl enable sshd‘ and/or ‘sudo systemctl start sshd‘.
SSH into the machine: ‘ssh -p 2222 deck@localhost‘

Updating the OS to the latest version
The Steam Deck recovery image doesn’t install the most recent version of SteamOS, so now we should probably do a software update.

First of all ensure that you’re giving enought RAM to the VM (in my examples I run QEMU with -m 8G). The OS update might fail if you use less.
(optional) Change the OS branch if you want to try the beta release: ‘sudo steamos-select-branch beta‘ (or main, if you want the bleeding edge)
Check the currently installed version in /etc/os-release (see the BUILD_ID variable)
Check the available version: ‘steamos-update check‘
Download and install the software update: ‘steamos-update‘

Note: if the last step fails after reaching 100% with a post-install handler error then go to Connections in the system settings, rename Wired Connection 1 to something else (anything, the name doesn’t matter), click Apply and run steamos-update again. This works around a bug in the update process. Recent images fix this and this workaround is not necessary with them.
As we did with the recovery image, before rebooting we should ensure that the new update boots into the Plasma session, otherwise it won’t work:
$ sudo steamos-chroot --partset other
# steamos-readonly disable
# echo '[Autologin]' > /etc/sddm.conf.d/zz-steamos-autologin.conf
# echo 'Session=plasma.desktop' >> /etc/sddm.conf.d/zz-steamos-autologin.conf
# steamos-readonly enable
# exit

After this we can restart the system.
If everything went fine we should be running the latest SteamOS release. Enjoy!
Reporting bugs
SteamOS is under active development. If you find problems or want to request improvements please go to the SteamOS community tracker.
Edit 06 Jul 2022: Small fixes, mention how to install the OS without using NVMe.


        05 July, 2022 07:11PM

         by berto














      Russ Allbery






        Review: A Mirror Mended


        Review: A Mirror Mended, by Alix E. Harrow



    Series:
    Fractured Fables #2


    Publisher:
    Tordotcom


    Copyright:
    2022


    ISBN:
    1-250-76665-6


    Format:
    Kindle


    Pages:
    129



This is a direct sequel to A Spindle
Splintered and will completely spoil that story, so start there rather
than here.



A Mirror Mended opens with a glimpse at yet another version of the
Sleeping Beauty story, this one (delightfully) a Spanish telenovela.
Zinnia is world-hopping, something that's lost some of the meaning from
A Spindle Splintered and become an escape from other problems.
She's about ready to leave this world as well when she sees a face that is
not hers in the bathroom mirror, pleading for help.  Zinnia assumes this
is yet another sleeping beauty, albeit an unusual one.  Zinnia is wrong.



Readers of A Spindle Splintered are going to groan when I tell you
that Zinnia has managed to damage most of the relationships that she made
in the first story, which means we get a bit of an episodic reset of
unhappiness mixed with an all-new glob of guilt.  Not only is this a
depressing way to start a new story, it also means there are no snarky
text messages and side commentary.  Grumble.  Harrow is isolating Zinnia
to set up a strange and fraught alliance that turns into a great story,
but given that Zinnia's friend network was my favorite part of the first
novella, the start of this story made me grumpy.



Stick with it, though, since Harrow does more than introduce another fairy
tale.  She also introduces a villain, one who wishes to be more
complicated than her story allows and who knows rather more about the
structure of the world than she should.  This time, the fairy tale goes
off the rails in a more directly subversive way that prods at the bones of
Harrow's world-building.



This may or may not be what you want, and I admit I liked the first story
better.  A Spindle Splintered took fairy tales just seriously
enough to make a plot, but didn't poke at its premises deeply enough to
destabilize them.  It played off of fairy tales themselves; A Mirror
Mended instead plays off of Harrow's previous story by looking directly
at the invented metaphysics of parallel worlds playing out fairy tale
archetypes.  Some of this worked for me: Eva is a great character and the
dynamic between her and Zinnia is highly entertaining.  Some of it didn't:
the impact on universal metaphysics of Zinnia's adventuring is a bit
cliched and inadequately explained.  A Mirror Mended is a character
exploration with a bit more angst and ambiguity, which means it isn't as
delightfully balanced and free-wheeling.



I will reassure you with the minor spoiler that Zinnia does eventually
pull her head out of her ass when she has to, and while there is nowhere
near enough Charm in this book for my taste, there is some.  In exchange
for the relationship screw-ups, we get the Zinnia/Eva dynamic, which I was
really enjoying by the end.  One of my favorite tropes is accidental
empathy, where someone who is being flippant and sarcastic stumbles into a
way of truly helping someone else and is wise enough to notice it.  There
are several great moments of that.  I like Zinnia, even this older, more
conflicted, and less cavalier version.



Recommended if you liked the first story, although be warned that this
replaces the earlier magic with some harder relationship work and the
payoff is more hinted at than fully shown.


Rating: 7 out of 10


        05 July, 2022 02:27AM







    July 04, 2022







        Review: She Who Became the Sun


        Review: She Who Became the Sun, by Shelley Parker-Chan



    Series:
    Radiant Emperor #1


    Publisher:
    Tor


    Copyright:
    2021


    Printing:
    2022


    ISBN:
    1-250-62179-8


    Format:
    Kindle


    Pages:
    414



In 1345 in Zhongli village, in fourth year of a drought, lived a man with
his son and his daughter, the last surviving of seven children.  The son
was promised by his father to the Wuhuang Monastery on his twelfth
birthday if he survived.  According to the fortune-teller, that son, Zhu
Chongba, will be so great that he will bring a hundred generations of
pride to the family name.  When the girl dares ask her fate, the
fortune-teller says, simply, "Nothing."



Bandits come looking for food and kill their father.  Zhu goes catatonic
rather than bury his father, so the girl digs a grave, only to find her
brother dead inside it with her father.  It leaves her furious:  he had a
great destiny and he gave it up without a fight, choosing to become
nothing.  At that moment, she decides to seize his fate for her own, to
become Zhu so thoroughly that Heaven itself will be fooled.
Through sheer determination and force of will, she stays at the gates of
Wuhuang Monastery until the monks are impressed enough with her
stubbornness that they let her in under Zhu's name.  That puts her on a
trajectory that will lead her to the Red Turbans and the civil war over
the Mandate of Heaven.



She Who Became the Sun is historical fiction with some alternate
history and a touch of magic.  The closest comparison I can think of is
Guy Gavriel Kay: a similar touch of magic that is slight enough to have
questionable impact on the story, and a similar starting point of history
but a story that's not constrained to follow the events of our world.
Unlike Kay, Parker-Chan doesn't change the names of places and people.
It's therefore not difficult to work out the history this story is based
on (late Yuan dynasty), although it may not be clear at first what role
Zhu will play in that history.



The first part of the book focuses on Zhu, her time in the monastery, and
her (mostly successful) quest to keep her gender secret.  The end of that
part introduces the second primary protagonist, the eunuch general Ouyang
of the army of the Prince of Henan.  Ouyang is Nanren, serving a Mongol
prince or, more precisely, his son Esen.  On the surface, Ouyang is
devoted to Esen and serves capably as his general.  What lies beneath that
surface is far darker and more complicated.



I think how well you like this book will depend on how well you get along
with the characters.  I thought Zhu was a delight.  She spends the first
half of the book proving herself to be startlingly competent and
unpredictable while outwitting Heaven and pursuing her assumed destiny.  A
major hinge event at the center of the book could have destroyed her
character, but instead makes her even stronger, more relaxed, and more
comfortable with herself.  Her story's exploration of gender identity only
made that better for me, starting with her thinking of herself as a woman
pretending to be a man and turning into something more complex and
self-chosen (and, despite some sexual encounters, apparently asexual,
which is something you still rarely see in fiction).  I also appreciated
how Parker-Chan varies Zhu's pronouns depending on the perspective of the
narrator.



That said, Zhu is not a good person.  She is fiercely ambitious to the
point of being a sociopath, and the path she sees involves a lot of
ruthlessness and some cold-blooded murder.  This is less of a heroic
journey than a revenge saga, where the target of revenge is the entire
known world and Zhu is as dangerous as she is competent.  If you want your
protagonist to be moral, this may not work for you.  Zhu's scenes are
partly told from her perspective and partly from the perspective of a
woman named Ma who is a good person, and who is therefore
intermittently horrified.  The revenge story worked for me, and as a
result I found Ma somewhat irritating.  If your tendency is to agree with
Ma, you may find Zhu too amoral to root for.



Ouyang's parts I just hated, which is fitting because Ouyang loathes
himself to a degree that is quite difficult to read.  He is
obsessed with being a eunuch and therefore not fully male.  That
internal monologue is disturbing enough that it drowned out the moderately
interesting court intrigue that he's a part of.  I know some people like
reading highly dramatic characters who are walking emotional disaster
zones.  I am not one of those people; by about three quarters of the way
through the book I was hoping someone would kill Ouyang already and put
him out of everyone's misery.



One of the things I disliked about this book is that, despite the complex
gender work with Zhu, gender roles within the story have a modern gloss
while still being highly constrained.  All of the characters except Zhu
(and the monk Xu, who has a relatively minor part but is the most likable
character in the book) feel like they're being smothered in oppressive
gender expectations.  Ouyang has a full-fledged case of toxic masculinity
to fuel his self-loathing, which Parker-Chan highlights with some weirdly
disturbing uses of BDSM tropes.



So, I thought this was a mixed bag, and I suspect reactions will differ.
I thoroughly enjoyed Zhu's parts despite her ruthlessness and struggled
through Ouyang's parts with a bad taste in my mouth.  I thought the pivot
Parker-Chan pulls off in the middle of the book with Zhu's self-image and
destiny was beautifully done and made me like the character even more, but
I wish the conflict between Ma's and Zhu's outlooks hadn't been so
central.  Because of that, the ending felt more tragic than triumphant,
which I think was intentional but which wasn't to my taste.



As with Kay's writing, I suspect there will be some questions about
whether She Who Became the Sun is truly fantasy.  The only obvious
fantastic element is the physical manifestation of the Mandate of Heaven,
and that has only a minor effect on the plot.  And as with Kay, I think
this book needed to be fantasy, not for the special effects, but because
it needs the space to take fate literally.  Unlike Kay, Parker-Chan does
not use the writing style of epic fantasy, but Zhu's campaign to assume a
destiny which is not her own needs to be more than a metaphor for the
story to work.



I enjoyed this with some caveats.  For me, the Zhu portions made up for
the Ouyang portions.  But although it's clearly the first book of a
series, I'm not sure I'll read on.  I felt like Zhu's character arc
reached a satisfying conclusion, and the sequel seems likely to be full of
Ma's misery over ethical conflicts and more Ouyang, neither of which sound
appealing.



So far as I can tell, the sequel I assume is coming has not yet been
announced.


Rating: 7 out of 10


        04 July, 2022 02:58AM







    July 03, 2022






      Thorsten Alteholz






        My Debian Activities in June 2022


        FTP master
This month I accepted 305 and rejected 59 packages. The overall number of packages that got accepted was 310.
From time to time I am also looking at the list of packages to be removed. If you would like to make life easier for the people who remove packages, please make sure that the resulting dak command really makes sense. If this command consists of garbage, please adapt the Subject: of your bug report accordingly.
Also it does not make sense to file bugs to remove packages from NEW. Please don’t hesitate to close such bugs again …
Debian LTS
This was my ninety-sixth month that I did some work for the Debian LTS initiative, started by Raphael Hertzog at Freexian.
This month my all in all workload has been 30.25h. During that time I did LTS and normal security uploads of:

[DLA 3058-1] libsndfile security update for two CVEs
[DLA 3060-1] blender security update for three CVEs
[#1008577] bullseye-pu: golang-github-russellhaering-goxmldsig/1.1.0-1+deb11u1  package has been accepted
[#1009077] bullseye-pu: minidlna/1.3.0+dfsg-2+deb11u1  package has been accepted
upload of blender to buster-security, no DSA yet
upload of blender to bullseye-security, no DSA yet, this upload seems to have failed

I have to admit that I totally ignored the EOL of Stretch LTS, so my upload of ncurses needs to go to Stretch ELTS now.
This month I also moved/refactored the current LTS documentation to a new repository and started to move the LTS Wiki as well.
I also continued to work on security support for golang packages.
Last but not least I did some days of frontdesk duties and took care of issues on security-master.
At this point I also need to mention my first “business trip”. I drove the short distance between Chemnitz and Freiberg and met Anton to have a face to face talk about LTS/ELTS. It was a great pleasure and definitely more fun than a meeting on IRC.
Debian ELTS
This month was the forty-seventh ELTS month.
During my allocated time I uploaded:

ELS-629-1 for libsndfile

Due to the delay of my ncurses upload to Stretch LTS, the ELTS upload got delayed as well. Now I will do both uploads to ELTS in July.
Last but not least I did some days of frontdesk duties.
Debian Printing
This month I uploaded new upstream versions or improved packaging of:

… pappl
… ipp-usb

Debian Astro
As there has been a new indi release arriving in Debian, I uploaded new upstream versions of most of the indi-3rdparty packages. Don’t hesitate to tell me whether you really use one of them :-).
Other stuff
This month I uploaded new upstream versions or improved packaging of:

… pipexec
… chktex
… gnupg-pkcs11-scd
… libctl



        03 July, 2022 11:37AM

         by alteholz


















      Martin-Éric Racine






        Refactoring Debian's dhcpcd packaging


        Given news that ISC's DHCP suite is getting deprecated by upstream and seeing how dhclient has never worked properly for DHCPv6, I decided to look into alternatives. ISC itself recommends Roy Maple's dhcpcd as a migration path. Sadly, Debian's package had been left unattended for a good 2 years. After refactoring the packaging, updating to the latest upstream and performing one NMU, I decided to adopt the package.

Numerous issues were exposed in the process:


  Upstream's ./configure makes BSD assumptions. No harm done, but still...
  Upstream's ./configure is broken. --prefix does not propagate to all components. For instance, I had to manually specify the full path for manual pages. Patches are welcome.
  Debian had implemented custom exit hooks for all its NTP packages. Since then, upstream has implemented this in a much more concise way. All that's missing upstream is support for timesyncd. Patches are welcome.
  I'm still undecided on whether --prefix should assume / or /usr for networking binaries on a Debian system. Feedback is welcome.
  The previous maintainer had implemented plenty of transitional measures in maintainer scripts such as symbolically linking /sbin/dhcpcd and /usr/sbin/dhcpcd. Most of this can probably be removed, but I haven't gotten around verifying this. Feedback and patches are welcome.
  The previous maintainer had created an init.d script and systemd unit. Both of these interfere with launching dhcpcd using ifupdown via /etc/network/interfaces which I really need for configuring a router for IPv4 MASQ and IPv6 bridge. I solved this by putting them in a separate package and shipping the rest via a new binary target called dhcpcd-base along a logic similar to dnsmasq.
  DHCPv6 Prefix Delegation mysteriously reports enp4s0: no global addresses for default route after a reboot. Yet if I manually restart the interface, none of this appears. Help debuging this is welcome.
  Support for Predictable Interface Names was missing because Debian's package didn't Build-Depends on libudev-dev. Fixed.
  Support for priviledge separation was missing because Debian's package did not ./configure this or create a system user for this. Fixed.
  I am pondering moving the Debian package out of the dhcpcd5 namespace back into the dhcpcd namespace. The 5 was the result of an upstream fork that happened a long time ago and the original dhcpcd package no longer is in the Debian archive. Feedback is welcome on whether this would be desirable.


The key advantage of dhcpcd over dhclient is that works as a dual-stack DHCP client by design. With privilege separation enabled, this means separate child processes handling IPv4 and IPv6 configuration and passing the received information to the parent process to configure networking and update /etc/resolv.conf with nameservers for both stacks. Additionally, /etc/network/interfaces no longer needs separate inet and inet6 lines for each DHCP interface, which makes for much cleaner configuration files.

A secondary advantage is that the dual-stack includes built-in fallback to Bonjour for IPv4 and SLAAC for IPv6. Basically, unless the interface needs a static IP address, this client handles network configuration in a smart and transparent way.

A third advantage is built-in support for DHCPv6 Prefix Delegation. Enabling this requires just two lines in the configuration file.

In the long run, I feel that dhcpcd-base should probably replace isc-dhcp-client as the default DHCP client with priority Important. Adequate IPv6 support should come out of the box on a standard Debian installation, yet dhclient never got around implementing that properly.


        03 July, 2022 08:57AM

         by Martin-Éric (noreply@blogger.com)












      Russ Allbery






        Review: The Haunting of Tram Car 015


        Review: The Haunting of Tram Car 015, by P. Djèlí Clark



    Publisher:
    Tordotcom


    Copyright:
    February 2019


    ASIN:
    B07H796G2Z


    Format:
    Kindle


    Pages:
    65



The Haunting of Tram Car 015 is a novella and the second story in
the Dead Djinn universe, after "A Dead Djinn
in Cairo".  While there are a few references to the previous story, it's
not a direct sequel and has different main characters.  Order of reading
is not important.



Agents Hamed and Onsi of the Ministry of Alchemy, Enchantments, and
Supernatural Entities have been called by the Superintendent of Tram
Safety & Maintenance at Ramses Station because one of the tram cars is
haunted.  The aerial tram system of Cairo (technically a telpher system
since the cars move independently) is one of the modern wonders of the
1912 city after al-Jahiz breached the boundaries between universes and
allowed djinn to return to the world.  The trams are elaborate magical
clockwork machines created by djinn to travel their routes, but tram car
015 had to be taken out of service after a magical disturbance.  Some
supernatural creature has set up residence in its machinery and has been
attacking passengers.



Like "A Dead Djinn in Cairo," this is a straightforward police procedural
in an alternate history with magic and steampunk elements.  There isn't
much in the way of mystery, and little about the plot will come as a
surprise.  The agents show up, study the problem, do a bit of research,
and then solve the problem with some help.  Unlike the previous story,
though, it does a far better job at setting.



My main complaint about Clark's first story in this universe was that it
had a lot of infodumps and not much atmosphere.  The Haunting of
Tram Car 015 is more evocative, starting with the overheated, windowless
office of the superintendent and its rattling fan and continuing with a
glimpse of the city's aerial tram network spreading out from the dirigible
mooring masts of Ramses Station.  While the agents puzzle through
identifying the unwanted tram occupant, they have to deal with
bureaucratic funding fights and the expense of djinn specialists.  In the
background, the women of Cairo are agitating for the vote, and Islam,
Coptic Christianity, and earlier Egyptian religions mingle warily.



The story layered on top of this background is adequate but not great.
It's typical urban fantasy fare built on random bits of obscure magical
trivia, and feels akin to the opening problem in a typical urban fantasy
novel (albeit with a refreshingly non-European magical system).  It also
features an irritatingly cliched bit of costuming at the conclusion.  But
you wouldn't read this for the story; you read it to savor the world
background, and I thought that was successful.



This is not a stand-out novella for me and I wouldn't have nominated it
for the various awards it contended for, but it's also not my culture and
by other online accounts it represents the culture well.  The world
background was interesting enough that I might have kept reading even if
the follow-on novel had not won a Nebula award.



Followed by the novel A Master of Djinn, although the continuity
link is not strong.


Rating: 7 out of 10


        03 July, 2022 04:50AM







    July 02, 2022










      Junichi Uekawa






        Watching my raspberry pi update I noticed I have nodejs 12 in them.


        Watching my raspberry pi update I noticed I have nodejs 12 in them.
	  Most systems I work on have nodejs 16 installed. I don't remember what changed ...



        02 July, 2022 09:07PM

         by Junichi Uekawa














      François Marier






        Remote logging of Turris Omnia log messages using syslog-ng and rsyslog


        As part of debugging an upstream connection problem I've been seeing
recently, I wanted to be able to monitor the logs from my Turris
Omnia router. Here's how I
configured it to send its logs to a server I already had on the local
network.

Server setup

The first thing I did was to open up my server's
rsyslog (Debian's default syslog server) to
remote connections since it's going to be the destination host for the
router's log messages.

I added the following to /etc/rsyslog.d/router.conf:

module(load="imtcp")
input(type="imtcp" port="514")

if $fromhost-ip == '192.168.1.1' then {
    if $syslogseverity <= 5 then {
        action(type="omfile" file="/var/log/router.log")
    }
    stop
}


This is using the latest rsyslog configuration method: a handy scripting
language called
RainerScript.
Severity level 5
maps to "notice" which consists of unusual non-error conditions, and
192.168.1.1 is of course the IP address of the router on the LAN side.
With this, I'm directing all router log messages to a separate file,
filtering out anything less important than severity 5.

In order for rsyslog to pick up this new configuration file, I restarted it:

systemctl restart rsyslog.service


and checked that it was running correctly (e.g. no syntax errors in the new
config file) using:

systemctl status rsyslog.service


Since I added a new log file, I also setup log rotation for it by putting
the following in /etc/logrotate.d/router:

/var/log/router.log
{
    rotate 4
    weekly
    missingok
    notifempty
    compress
    delaycompress
    sharedscripts
    postrotate
        /usr/lib/rsyslog/rsyslog-rotate
    endscript
}


In addition, since I use
logcheck to monitor my server
logs and email me errors, I had to add /var/log/router.log to
/etc/logcheck/logcheck.logfiles.

Finally I opened the rsyslog port to the router in my server's firewall by
adding the following to /etc/network/iptables.up.rules:

# Allow logs from the router
-A INPUT -s 192.168.1.1 -p tcp --dport 514 -j ACCEPT


and ran iptables-apply.

With all of this in place, it was time to get the router to send messages.

Router setup

As suggested on the Turris
forum, I
ssh'ed into my router and added this in /etc/syslog-ng.d/remote.conf:

destination d_loghost { tcp("192.168.1.200"); };
log {
        source(src);
        source(kernel);
        destination(d_loghost);
};


To ensure that the destination host always gets the same IP address
(192.168.1.200), I went to the advanced DHCP configuration
page and added a
static lease for the server's MAC address so that it always gets assigned
192.168.1.200. If that wasn't already the server's IP address, you'll have
to restart it for this to take effect.

Finally, I restarted the syslog-ng daemon on the router to pick up the new
config file:

/etc/init.d/syslog-ng restart


Testing

In order to test this configuration, I opened three terminal windows:


tail -f /var/log/syslog on the server
tail -f /var/log/router.log on the server
tail -f /var/log/messages on the router



I immediately started to see messages from the router in the third window
and some of these, not all because of my severity-5 filter, were flowing to
the second window as well. Also important is that none of the messages make
it to the first window, otherwise log messages from the router would be mixed
in with the server's own logs. That's the purpose of the stop command in
/etc/rsyslog.d/router.conf.

To force a log messages to be emitted by the router, simply ssh into it and
issue the following command:

logger Test


It should show up in the second and third windows immediately if you've got
everything setup correctly


        02 July, 2022 03:45AM












      Russ Allbery






        Review: Overdue


        Review: Overdue, by Amanda Oliver



    Publisher:
    Chicago Review Press


    Copyright:
    2022


    ISBN:
    1-64160-534-0


    Format:
    Kindle


    Pages:
    190



Like many lifetime readers, I adored the public library.  I read my way
through three different children's libraries at the rate of a grocery sack
of books per week, including numerous re-readings, and then moved on to
the adult section as my introduction to science fiction.  But once I had a
regular job, I discovered the fun of filling shelves with books without
having to return them or worry about what the library had available.  I've
always supported my local library, but it's been decades since I spent
much time in it.  When I last used one heavily, the only computers were at
the checkout desk and the only books were physical, normally hardcovers.



Overdue: Reckoning with the Public Library therefore caught my eye
when I saw a Twitter thread about it before publication.  It promised to
be a picture of the modern public library and its crises from the
perspective of the librarian.  The author's primary topic was the drafting
of public libraries as de facto homeless service centers, but I hoped it
would also encompass technological change, demand for new services, and
the shifting meaning of what a public library is for.



Overdue does... some of that.  The author was a children's
librarian in a Washington DC public school and then worked at a downtown
branch of the Washington DC public library, and the book includes a few
anecdotes from both experiences.  Most of the book, though, is Oliver's
personal memoir of how she got into field, why she chose to leave it, and
how she is making sense of her feelings about the profession.  Intermixed
with that memoir is wide-ranging political commentary on topics ranging
from gentrification to mental health care.  This material is relevant to
the current challenges libraries face, but it wandered far afield from
what I was hoping to get from the book.



I think of non-fiction books as coming in a few basic shapes.  One is
knowledge from an expert: the author has knowledge about a topic that is
not widely shared, and they write a book to share it.  Another is
popularization: an author, possibly without prior special expertise in the
topic, does research the reader could have done but doesn't have time to
do and then summarizes the results in a format that's easier to understand
than the original material.  And a third is memoir, in which the author
tells the story of their own life.  This is a variation of the first type,
since the author is obviously an expert in their own life, but most
people's lives are not interesting.  (Mine certainly isn't!)  Successful
memoir therefore depends on either having an unusual life or being a
compelling storyteller, and ideally both.



Many non-fiction books fall into multiple categories, but it's helpful for
an author to have a clear idea of which of these goals they're pursuing
since they result in different books.  If the author is writing primarily
from a position of special expertise, the book should focus on that
expertise.  I am interested in librarians and libraries and would like to
know more about that job, so I will read with interest your personal
stories about being a librarian.  I am somewhat interested in your policy
suggestions for how to make libraries work better, although more so if you
can offer context and analysis beyond your personal experiences.  I am
less interested in your opinions on, say, gentrification.  That's not
because I doubt it is a serious problem (it is) or that it impacts
libraries (it does).  It's because working in a library doesn't provide
any special expertise in gentrification beyond knowing that it exists,
something that I can see by walking around the corner.  If I want to know
more, I will read books by urban planners, sociologists, and housing
rights activists.



This is a long-winded way of saying that I wish Overdue had about
four times as many stories about libraries, preferably framed by general
research and background that extended beyond the author's personal
experience, or at least more specific details of the politics of the
Washington DC library system.  The personal memoir outside of the library
stories failed to hold my interest.



This is not intended as a slam on the author.  Oliver seems like a
thoughtful and sincere person who is struggling with how to do good in the
world without burning out, which is easy for me to sympathize with.  I
suspect I broadly agree with her on many political positions.  But I have
read all of this before, and personally lived through some of the same
processing, and I don't think Oliver offered new insight.  The library
stories were memorable enough to form the core of a good book, but the
memoir structure did nothing for them and they were strangled by the
unoriginal and too-general political analysis.



At the risk of belaboring a negative review, there are two other things
in Overdue that I've also seen in other writing and seem worth
commenting on.



The first is the defensive apology that the author may not have the best
perspective to write the book.  It's important to be clear: I am glad that
the Oliver has thought about the ways her experiences as a white woman may
not be representative of other people.  This is great; the world is a
better place when more people consider that.  I'm less fond of putting
that observation in the book, particularly at length.



As the author, rather than writing paragraphs vaguely acknowledging that
other people have different experiences, she could instead fix the
problem: go talk to librarians of other ethnic and social backgrounds and
put their stories in this book.  The book would then represent broader
experiences and not require the apology.  Overdue desperately
needed more library-specific content, so that would have improved the book
in more than one way.  Or if Oliver is ideologically opposed to speaking
for other people (she makes some comments to that effect), state up-front,
once, that this is a personal memoir and, as a memoir, represents only her
own experience.  But the author should do something with this observation
other than dump its awkwardness on the reader, if for no other reason than
that lengthy disclaimers about the author's limited perspective are
boring.



The second point is about academic jargon and stock phrasing.  I work in a
field that relies on precise distinctions of meaning (between identity,
authentication, and authorization, for example), and therefore I rely on
jargon.  Its purpose is to make those types of fine distinctions.  But
authors who read heavily in fields with jargon tend to let that phrasing
slip into popular writing where it's not necessary.  The result is, to
quote
Orwell, "gumming together long strips of words which have already been
set in order by someone else."  The effect may be small in a single
sentence but, when continued throughout a book, the overuse of jargon is
leaden, belabored, and confusing.



Any example I choose will be minor since the effect is cumulative, but one
of several I noticed in Overdue is "lived experience."  This is
jargon from philosophy that, within the field, draws a useful distinction
between one's direct experiences of living in the world, and academic or
scientific experience with a field.  Both types of experience are valuable
in different situations, but they're not equivalent.  This is a useful
phrase when the distinction matters and is unclear.  When the type of
experience one is discussing is obvious in context (the case in at least
three of the four uses in this book), the word "lived" adds nothing but
verbosity.  If too much of this creeps into writing, it becomes clunky and
irritating to read.



The best (and not coincidentally the least clunky) part of this book is
Oliver's stories of the patrons and other employees of the Northwest One
branch of the Washington DC library system and her experiences with them.
The picture was not as vivid as I was hoping for, but I came away with
some new understanding of typical interactions and day-to-day
difficulties.  The same was true to a lesser extent for her experiences as
a school librarian.  For both, I wish there had been more context and
framing so that I could see how her experiences fit into a whole system,
but those parts of the book were worth reading.



Unfortunately, they weren't enough of those parts in the book for me to
recommend Overdue.  But I'm still interested in reading the book I
hoped I was getting!


Rating: 5 out of 10


        02 July, 2022 02:50AM







    July 01, 2022










      Steve Kemp






        An update on my simple golang TCL interpreter


        So my previous post introduced a trivial interpreter for a TCL-like language.

In the past week or two I've cleaned it up, fixed a bunch of bugs, and added 100% test-coverage.  I'm actually pretty happy with it now.

One of the reasons for starting this toy project was to experiment with how easy it is to extend the language using itself

Some things are simple, for example replacing this:

puts "3 x 4 = [expr 3 * 4]"


With this:

puts "3 x 4 = [* 3 4]"


Just means defining a function (proc) named *.  Which we can do like so:

proc * {a b} {
    expr $a * $b
}


(Of course we don't have lists, or variadic arguments, so this is still a bit of a toy example.)

Doing more than that is hard though without support for more primitives written in the parent language than I've implemented.  The obvious thing I'm missing is a native implementation of upvalue, which is TCL primitive allowing you to affect/update variables in higher-scopes.  Without that you can't write things as nicely as you would like, and have to fall back to horrid hacks or be unable to do things.

# define a procedure to run a body N times
proc repeat {n body} {
    set res ""
    while {> $n 0} {
        decr n
        set res [$body]
    }
    $res
}

# test it out
set foo 12
repeat 5 { incr foo }

#  foo is now 17 (i.e. 12 + 5)


A similar story implementing the loop word, which should allow you to set the contents of a variable and run a body a number of times:

proc loop {var min max bdy} {
    // result
    set res ""

    // set the variable.  Horrid.
    // We miss upvalue here.
    eval "set $var [set min]"

    // Run the test
    while {<= [set "$$var"] $max } {
        set res [$bdy]

        // This is a bit horrid
        // We miss upvalue here, and not for the first time.
        eval {incr "$var"}
    }

    // return the last result
    $res
}


loop cur 0 10 { puts "current iteration $cur ($min->$max)" }
# output is:
# => current iteration 0 (0-10)
# => current iteration 1 (0-10)
# ...


That said I did have fun writing some simple test-cases, and implementing assert, assert_equal, etc.

In conclusion I think the number of required primitives needed to implement your own control-flow, and run-time behaviour, is a bit higher than I'd like.  Writing switch, repeat, while, and similar primitives inside TCL is harder than creating those same things in FORTH, for example.


        01 July, 2022 07:00PM


















      Ben Hutchings






        Debian LTS work, June 2022



  In June I was not assigned additional hours of work by Freexian's
  Debian LTS initiative, but carried over 16 hours from May and worked
  all of those hours.


  I spent some time triaging security issues for Linux.  I tested
  several security fixes for Linux 4.9 and 4.19 and submitted them for
  inclusion in the upstream stable branches.


  I rebased the Linux 4.9 (linux) package on the latest stable update
  (4.9.320), uploaded this and issued the final DLA for stretch,
  DLA-3065-1.



        01 July, 2022 01:12PM












      Paul Wise






        FLOSS Activities June 2022


        Focus

This month I didn't have any particular focus.
I just worked on issues in my info bubble.

Changes


duck:
add more obsolete domains
(1
2),
indicator phrases
(1
2)
lintian:
add more obsolete domains
Debian BTS usertags:
fix old release goal tags,
reassign edos tags to debian-qa,
reassign sphinx tag to debian-python
Debian userdir-ldap:
modernise and cleanup welcome message wording
Debian website:
consultants fix
Debian package uploads:
sptag
Debian wiki pages:
AutomaticPackagingTools,
Avahi,
DebianJr,
DebianPorts,
Derivatives/Census,
DeveloperNews,
Firmware,
Firmware/Open,
Hardware/Database,
PortsDocs/New,
Ports
(loongarch64,
riscv64
(1,
2)),
PortTemplate,
RISC-V/32,
SystemPrinting,
Teams
(DebianRakudoGroup,
Lintian
(1
2),
pkg-fonts)
FOSSjobs wiki pages:
Resources


Issues


New contributors needed for
lintian
Conffile removal needed in
dkms
Features in
reportbug
Deprecated module usage in
SPTAG
Embedded deps in
SPTAG


Review


Spam: reported
5 Debian bug reports and
45 Debian mailing list posts
Debian wiki:
RecentChanges for the month
Debian BTS usertags:
changes for the month
Debian screenshots:

approved
pforth
timeshift
virtualjaguar
rejected
gcc (blank),
gcc (GitHub repo card template),
vagrant (low quality)



Administration


Debian wiki:
unblock IP addresses,
assist with account recovery,
approve accounts


Communication


Initiate discussion about
conveying Debian package porting details
Respond to queries from Debian users and contributors on the mailing lists and IRC


Sponsors

The sptag work was sponsored.
All other work was done on a volunteer basis.


        01 July, 2022 02:51AM







    June 30, 2022






      Russell Coker






        Links June 2022


        Google did some interesting research on the impact of discrimination on code reviers [1]. It turns out that this is a bigger problem than most white men would have ever suspected and it even has an adverse effect on Asian people.
nothello.net is an amusing site to make the point that you shouldn’t use IM to say hello separately from asking the question [2]. A good link to share on your corporate IM system.
TechCrunch has an amusing article about the Facebook farewell to Sheryl Sandburg [3].
BleepingComputer has an interesting article about a bug-bunty program from a crime syndicate offering up to $1M in crypto-currency [4]. Among other things finding the real first and last names of the crime lord gets you $1M.
BleepingComputer has an interesting article about how “deepfakes” are being used to apply for work from home jobs [5]. I wonder whether the people doing that intend to actually do any of the work or just get paid for doing nothing while delaying getting sacked for as long as possible. I have read about people getting a job they don’t want to do that has a long training period so that they can quit at the end of training without working – apparently call center work is a good option for this.
BleepingComputer has an interesting article about phishing attacks that use a VNC remote desktop connection to trick a user into authenticating using the attacker’s PC [6]. The real problem here is getting humans to do things that computers do better, which is recognising the correct foreign party.
Fortune has an interesting article about the problems with Tesla self-driving and the possibility of a recall [7]. The main issue is apparently Teslas driving at full speed into emergency services vehicles that are parked while attending an incident. Having a police car unexpectedly occupying a lane of traffic is something you just have to deal with, either stop or change lanes. Teslas have been turning off autopilot less than one second before impact so Telsa can claim that it didn’t happen with autopilot engaged but in reality a human can’t take over in less than one second, a pilot I know says it takes 2-3 seconds to take over the controls in a plane.
BonAppetit has an interesting and amusing article about protest foods [8] which starts by explaining why Ukrainians are throwing pasta at the Russian consulate.
The NVidia blog has an informative post about how Pony.ai optimised their pipeline for sensor data for autonomous cars [9].
Matt Crump wrote an educational and amusing blog post about his battle with cheaters in university tests he administered [10].
The Cricket Monthly has an insightful article about how a batsman manages to see and hit a cricket ball that’s going well in excess of 100KM/h [11]. One particularly noteworthy part of this article is the comparison of what amateur cricketers do with what anyone who wants to be a contender for the national team must do.
Darker Shades of Blue is an insightful paper by Tony Kern about the needless crash of a B52 at Fairchild air base in 1994 [12]. This is specifically written to teach people about correct and effective leadership.

[1] https://tinyurl.com/2699msyj
[2] https://nohello.net/en/
[3] https://tinyurl.com/2aqw4jq9
[4] https://tinyurl.com/2boyx72h
[5] https://tinyurl.com/29zm7m4c
[6] https://tinyurl.com/y9v9vme2
[7] https://tinyurl.com/2yah3vez
[8] https://tinyurl.com/ye96qhoc
[9] https://tinyurl.com/22kmmpjg
[10] https://tinyurl.com/2ntfw3ow
[11] https://tinyurl.com/2aprn3w6
[12] https://tinyurl.com/29bexyah



Related posts:
Links Jan 2022  Washington Post has an interesting article on how gender neutral...
Links March 2022  Anarcat wrote a great blog post about switching from OpenNTP...
Links May 2022  dontkillmyapp.com is a web site about Android phone vendors who...




        30 June, 2022 01:38PM

         by etbe







    June 29, 2022










      Aigars Mahinovs






        Long travel in an electric car


        Since the first week of April 2022 I have (finally!) changed my company car from
a plug-in hybrid to a fully electic car. My new ride, for the next two years, is
a BMW i4 M50 in Aventurine Red metallic.
An ellegant car with very deep and
memorable color, insanely powerful (544 hp/795 Nm), sub-4 second 0-100 km/h, large
84 kWh battery (80 kWh usable), charging up to 210 kW, top speed of 225 km/h
and also very efficient (which came out best in this trip) with WLTP range of 510 km
and EVDB real range of 435 km. The car
also has performance tyres (Hankook Ventus S1 evo3 245/45R18 100Y XL in front and
255/45R18 103Y XL in rear all at recommended 2.5 bar) that have reduced efficiency.
So I wanted to document and describe how was it for me to travel ~2000 km (one way)
with this, electric, car from south of Germany to north of Latvia. I have done
this trip many times before since I live in Germany now and travel back to my
relatives in Latvia 1-2 times per year. This was the first time I made this trip in
an electric car. And as this trip includes both travelling in Germany (where BEV
infrastructure is best in the world) and across Eastern/Northen Europe, I believe
that this can be interesting to a few people out there.
Normally when I travelled this trip with a gasoline/diesel car I would normally drive
for two days with an intermediate stop somewhere around Warsaw with about 12 hours
of travel time in each day. This would normally include a couple bathroom stops in each
day, at least one longer lunch stop and 3-4 refueling stops on top of that. Normally
this would use at least 6 liters of fuel per 100 km on average with total usage of about
270 liters for the whole trip (or about 540€ just in fuel costs, nowadays). My
(personal) quirk is that both fuel and recharging of my (business) car inside Germany
is actually paid by my employer, so it is useful
for me to charge up (or fill up) at the last station in Gemany before driving on.
The plan for this trip was made in a similar way as when travelling with a gasoline car:
travelling as fast as possible on German Autobahn network to last chargin stop on the A4
near Görlitz, there charging up as much as reasonable and then travelling to a hotel
in Warsaw, charging there overnight and travelling north towards Ionity chargers in
Lithuania from where reaching the final target in north of Latvia should be possible.
How did this plan meet the reality?
Travelling inside Germany with an electric car was basically perfect. The most efficient
way would involve driving fast and hard with top speed of even 180 km/h (where possible
due to speed limits and traffic). BMW i4 is very efficient at high speeds with consumption
maxing out at 28 kWh/100km when you actually drive at this speed all the time. In real
situation in this trip we saw consumption of 20.8-22.2 kWh/100km in the first legs of the trip.
The more traffic there is, the more speed limits and roadworks, the lower is the average
speed and also the lower the consumption. With this kind of consumption we could comfortably
drive 2 hours as fast as we could and then pick any fast charger along the route and in
26 minutes at a charger (50 kWh charged total) we'd be ready to drive for another 2 hours.
This lines up very well with recommended rest stops for biological reasons (bathroom, water
or coffee, a bit of movement to get blood circulating) and very close to what I had to do
anyway with a gasoline car. With a gasoline car I had to refuel first, then park, then go to
bathroom and so on. With an electric car I can do all of that while the car is charging and
in the end the total time for a stop is very similar. Also not that there was a crazy heat
wave going on and temperature outside was at about 34C minimum the whole day and hitting
40C at one point of the trip, so a lot of power was used for cooling. The car has a heat pump
standard, but it still was working hard to keep us cool in the sun.
The car was able to plan a charging route with all the charging stops required and had all
the good options (like multiple intermediate stops) that many other cars (hi Tesla) and
mobile apps (hi Google and Apple) do not have yet. There are a couple bugs with charging
route and display of current route guidance, those are already fixed and will be delivered
with over the air update with July 2022 update. Another good alterantive is the ABRP (A
Better Route Planner) that was specifically designed for electric car routing along the
best route for charging. Most phone apps (like Google Maps) have no idea about your specific
electric car - it has no idea about the battery capacity, charging curve and is missing key
live data as well - what is the current consumption and remaining energy in the battery. ABRP
is different - it has data and profiles for almost all electric cars and can also be linked to
live vehicle data, either via a OBD dongle or via a new Tronity cloud service. Tronity reads
data from vehicle-specific cloud service, such as MyBMW service, saves it, tracks history and
also re-transmits it to ABRP for live navigation planning. ABRP allows for options and settings
that no car or app offers, for example, saying that you want to stop at a particular place for
an hour or until battery is charged to 90%, or saying that you have specific charging cards and
would only want to stop at chargers that support those. Both the car and the ABRP also support
alternate routes even with multiple intermediate stops. In comparison, route planning by Google
Maps or Apple Maps or Waze or even Tesla does not really come close.
After charging up in the last German fast charger, a more interesting part of the trip started.
In Poland the density of high performance chargers (HPC) is much lower than in Germany. There are
many chargers (west of Warsaw), but vast majority of them are (relatively) slow 50kW chargers.
And that is a difference between putting 50kWh into the car in 23-26 minutes or in 60 minutes. It
does not seem too much, but the key bit here is that for 20 minutes there is easy to find stuff
that should be done anyway, but after that you are done and you are just waiting for the car and
if that takes 4 more minutes or 40 more minutes is a big, perceptual, difference. So using
HPC is much, much preferable. So we put in the Ionity charger near Lodz as our intermediate target
and the car suggested an intermediate stop at a Greenway charger by Katy Wroclawskie. The location
is a bit weird - it has 4 charging stations with 150 kW each. The weird bits are that each station
has two CCS connectors, but only one parking place (and the connectors share power, so if two cars
were to connect, each would get half power). Also from the front of the location one can only see
two stations, the otehr two are semi-hidden around a corner. We actually missed them on the way
to Latvia and one person actually waited for the charger behind us for about 10 minutes. We only
discovered the other two stations on the way back. With slower speeds in Poland the consumption
goes down to 18 kWh/100km which translates to now up to 3 hours driving between stops.
At the end of the first day we drove istarting from Ulm from 9:30 in the morning until about 23:00 in the evening with
total distance of about 1100 km, 5 charging stops, starting with 92% battery, charging for
26 min (50 kWh), 33 min (57 kWh + lunch), 17 min (23 kWh), 12 min (17 kWh) and 13 min (37 kW).
In the last two chargers you can see the difference between a good and fast 150 kW charger at high
battery charge level and a really fast Ionity charger at low battery charge level, which makes
charging faster still.
Arriving to hotel with 23% of battery. Overnight the car charged from a Porsche Destination
Charger to 87% (57 kWh). That was a bit less than I would expect from a full power 11kW charger,
but good enough. Hotels should really install 11kW Type2 chargers for their guests, it is a really
significant bonus that drives more clients to you.
The road between Warsaw and Kaunas is the most difficult part of the trip for both driving itself
and also for charging. For driving the problem is that there will be a new highway going from
Warsaw to Lithuanian border, but it is actually not fully ready yet. So parts of the way one drives
on the new, great and wide highway and parts of the way one drives on temporary roads or on old
single lane undivided roads. And the most annoying part is navigating between parts as signs are
not always clear and the maps are either too old or too new. Some maps do not have the new roads and
others have on the roads that have not been actually build or opened to traffic yet. It's really easy
to loose ones way and take a significant detour. As far as charging goes, basically there is only
the slow 50 kW chargers between Warsaw and Kaunas (for now). We chose to charge on the last charger
in Poland, by Suwalki Kaufland. That was not a good idea - there is only one 50 kW CCS and many people
decide the same, so there can be a wait. We had to wait 17 minutes before we could charge for
30 more minutes just to get 18 kWh into the battery. Not the best use of time. On the way back we chose
a different charger in Lomza where would have a relaxed dinner while the car was charging. That
was far more relaxing and a better use of time.
We also tried charging at an Orlen charger that was not recommended by our car and we found out why.
Unlike all other chargers during our entire trip, this charger did not accept our universal BMW Charging
RFID card. Instead it demanded that we download their own Orlen app and register there. The app is only
available in some countries (and not in others) and on iPhone it is only available in Polish. That is a
bad exception to the rule and a bad example. This is also how most charging works in USA. Here in Europe
that is not normal. The normal is to use a charging card - either provided from the car maker or from
another supplier (like PlugSufring or Maingau Energy). The providers then make roaming arrangements with
all the charging networks, so the cards just work everywhere. In the end the user gets the prices and the
bills from their card provider as a single monthly bill. This also saves all any credit card charges for
the user. Having a clear, separate RFID card also means that one can easily choose how to pay for each
charging session. For example, I have a corporate RFID card that my company pays for (for charging in
Germany) and a private BMW Charging card that I am paying myself for (for charging abroad). Having the
car itself authenticate direct with the charger (like Tesla does) removes the option to choose how to pay.
Having each charge network have to use their own app or token bring too much chaos and takes too much setup.
The optimum is having one card that works everywhere and having the option to have additional card
or cards for specific purposes.
Reaching Ionity chargers in Lithuania is again a breath of fresh air - 20-24 minutes to charge 50 kWh is
as expected. One can charge on the first Ionity just enough to reach the next one and then on the second
charger one can charge up enough to either reach the Ionity charger in Adazi or the final target in Latvia.
There is a huge number of CSDD (Road Traffic and Safety Directorate) managed chargers all over Latvia,
but they are 50 kW chargers. Good enough for local travel, but not great for long distance trips. BMW i4
charges at over 50 kW on a HPC even at over 90% battery state of charge (SoC). This means that it is always
faster to charge up in a HPC than in a 50 kW charger, if that is at all possible. We also tested the CSDD
chargers - they worked without any issues. One could pay with the BMW Charging RFID card, one could use
the CSDD e-mobi app or token and one could also use Mobilly - an app that you can use in Latvia for
everything from parking to public transport tickets or museums or car washes.
We managed to reach our final destination near Aluksne with 17% range remaining after just 3 charging stops:
17+30 min (18 kWh), 24 min (48 kWh), 28 min (36 kWh). Last stop we charged to 90% which took a few extra
minutes that would have been optimal.
For travel around in Latvia we were charging at our target farmhouse from a normal 3 kW Schuko EU socket.
That is very slow. We charged for 33 hours and went from 17% to 94%, so not really full. That was perfectly
fine for our purposes. We easily reached Riga, drove to the sea and then back to Aluksne with 8% still
in reserve and started charging again for the next trip. If it were required to drive around more and charge
faster, we could have used the normal 3-phase 440V connection in the farmhouse to have a red CEE 16A plug
installed (same as people use for welders). BMW i4 comes standard with a new BMW Flexible Fast Charger
that has changable socket adapters. It comes by default with a Schucko connector in Europe, but for 90€
one can buy an adapter for blue CEE plug (3.7 kW) or red CEE 16A or 32A plugs (11 kW). Some public charging
stations in France actually use the blue CEE plugs instead of more common Type2 electric car charging stations.
The CEE plugs are also common in camping parking places.
On the way back the long distance BEV travel was already well understood and did not cause us any problem. From
our destination we could easily reach the first Ionity in Lithuania, on the Panevezhis bypass road where
in just 8 minutes we got 19 kWh and were ready to drive on to Kaunas, there a longer 32 minute stop before
the charging desert of Suwalki Gap that gave us 52 kWh to 90%. That brought us to a shopping mall in Lomzha
where we had some food and charged up 39 kWh in lazy 50 minutes. That was enough to bring us to our return hotel
for the night - Hotel 500W in Strykow by Lodz that has a 50kW charger on site, while we were having late
dinner and preparing for sleep, the car easily recharged to full (71 kWh in 95 minutes), so I just moved
it from charger to a parking spot just before going to sleep. Really easy and well flowing day.
Second day back went even better as we just needed an 18 minute stop at the same Katy Wroclawskie charger
as before to get 22 kWh and that was enough to get back to Germany. After that we were again flying on the
Autobahn and charging as needed, 15 min (31 kWh), 23 min (48 kWh) and 31 min (54 kWh + food). We started the
day on about 9:40 and were home at 21:40 after driving just over 1000 km on that day. So less than 12 hours
for 1000 km travelled, including all charging, bio stops, food and some traffic jams as well. Not bad.
Now let's take a look at all the apps and data connections that a technically minded customer can have
for their car. Architecturally the car is a network of computers by itself, but it is very secured and
normally people do not have any direct access. However, once you log in into the car with your BMW account
the car gets your profile info and preferences (seat settings, navigation favorites, ...) and the car then
also can start sending information to the BMW backend about its status. This information is then available
to the user over multiple different channels. There is no separate channel for each of those data flow.
The data only goes once to the backend and then all other communication of apps happens with the backend.
First of all the MyBMW app.
This is the go-to for everything about the car - seeing its current status and location (when not driving),
sending commands to the car (lock, unlock, flash lights, pre-condition, ...) and also monitor and control
charging processes. You can also plan a route or destination in the app in advance and then just send it over
to the car so it already knows where to drive to when you get to the car. This can also integrate with calendar
entries, if you have locations for appointments, for example. This also shows full charging history and
allows a very easy export of that data, here I exported all charging sessions from June and then
trimmed it back to only sessions relevant to the trip and cut off some design elements to have the data more
visible.
So one can very easily see when and where we were charging, how much power we got at each spot and
(if you set prices for locations) can even show costs.
I've already mentioned the Tronity service and its ABRP integration, but it also saves the information that
it gets from the car and gathers that data over time. It has nice aspects, like showing the driven routes
on a map, having ways to do business trip accounting and having good calendar view. Sadly it does not correctly
capture the data for charging sessions (the amounts are incorrect).
Update: after talking to Tronity support, it looks like the bug was in the incorrect value for the usable
battery capacity for my car. They will look into getting th eright values there by default, but as a workaround
one can edit their car in their system (after at least one charging session) and directly set the expected
battery capacity (usable) in the car properties on the Tronity web portal settings.
One other fun way to see data from your BMW is using the BMW integration in Home Assistant.
This brings the car as a device in your own smart home. You can read all the variables from the car current status
(and Home Asisstant makes cute historical charts) and you can even see
interesting trends, for example for remaining range shows much
higher value in Latvia as its prediction is adapted to Latvian road speeds and during the trip it adapts to Polish
and then to German road speeds and thus to higher consumption and thus lower maximum predicted remaining range.
Having the car attached to the Home Assistant also allows you to attach the car to automations, both as data and event
source (like detecting when car enters the "Home" zone) and also as target, so you could flash car lights or even
unlock or lock it when certain conditions are met.
So, what in the end was the most important thing - cost of the trip? In total we charged up 863 kWh, so that would
normally cost one about 290€, which is close to half what this trip would have costed with a gasoline car. Out of
that 279 kWh in Germany (paid by my employer) and 154 kWh in the farmhouse (paid by our wonderful relatives :D) so
in the end the charging that I actually need to pay adds up to 430 kWh or about 150€. Typically, it took about 400€
in fuel that I had to pay to get to Latvia and back. The difference is really nice!
In the end I believe that there are three different ways of charging:


incidental charging - this is wast majority of charging in the normal day-to-day life. The car gets charged when
and where it is convinient to do so along the way. If we go to a movie or a shop and there is a chance to leave
the car at a charger, then it can charge up. Works really well, does not take extra time for charging from us.


fast charging - charging up at a HPC during optimal charging conditions - from relatively low level to no more
than 70-80% while you are still doing all the normal things one would do in a quick stop in a long travel
process: bio things, cleaning the windscreen, getting a coffee or a snack.


necessary charging - charging from a whatever charger is available just enough to be able to reach the next
destination or the next fast charger.


The last category is the only one that is really annoying and should be avoided at all costs. Even by shifting
your plans so that you find something else useful to do while necessary charging is happening and thus, at least
partially, shifting it over to incidental charging category. Then you are no longer just waiting for the car,
you are doing something else and the car magically is charged up again.
And when one does that, then travelling with an electric car becomes no more annoying than travelling with
a gasoline car. Having more breaks in a trip is a good thing and makes the trips actually easier and less
stressfull - I was more relaxed during and after this trip than during previous trips. Having the car air
conditioning always be on, even when stopped, was a godsend in the insane heat wave of 30C-38C that we were
driving trough.
Final stats: 4425 km driven in the trip. Average consumption: 18.7 kWh/100km. Time driving: 2 days and 3 hours.
Car regened 152 kWh. Charging stations recharged 863 kWh.
Questions? You can use this i4talk forum thread or this Twitter thread to ask them to me.


        29 June, 2022 06:37PM

         by Aigars Mahinovs














      Tim Retout






        Git internals and SHA-1


        LWN reminds us that Git still uses SHA-1 by default.
Commit or tag signing is not a mitigation, and to understand why you need to
know a little about Git’s internal structure.
Git internally looks rather like a content-addressable filesystem, with four
object types: tags, commits, trees and blobs.
Content-addressable means changing the content of an object changes the way
you address or reference it, and this is achieved using a cryptographic hash
function.  Here is an illustration of the internal structure of an example
repository I created,  containing two files (./foo.txt and ./bar/bar.txt)
committed separately, and then tagged:

You can see how ‘trees’ represent directories, ‘blobs’ represent files, and so
on. Git can avoid internal duplication of files or directories which remain
identical. The hash function allows very efficient lookup of each object
within git’s on-disk storage.
Tag and commit signatures do not directly sign the files in the repository;
that is, the input to the signature function is the content of the tag/commit
object, rather than the files themselves. This is analogous to the way that
GPG signatures actually sign a cryptographic hash of your email, and there
was a time when this too defaulted to SHA-1. An attacker who can break that
hash function can bypass the guarantees of the signature function.
A motivated attacker might be able to replace a blob, commit or tree in a git
repository using a SHA-1 collision.  Replacing a blob seems easier to me than a
commit or tree, because there is no requirement that the content of the files
must conform to any particular format.
There is one key technical mitigation to this in Git, which is the SHA-1DC algorithm;
this aims to detect and prevent known collision attacks.  However, I will have
to leave the cryptanalysis of this to the cryptographers!
So, is this in your threat model?  Do we need to lobby GitHub for SHA-256
support?  Either way, I look forward to the future operational challenge of
migrating the entire world’s git repositories across to SHA-256.


        29 June, 2022 05:54PM












      Russell Coker






        Philips 438P1 43″ 4K Monitor


        I have just returned a Philips 438P1 43″ 4K Monitor [1] and gone back to my Samsung 28″ 4K monitor model LU28E590DS/XY AKA UE590.
The main listed differences are the size and the fact that the Samsung is TN but the Philips is IPS. Here’s a comparison of TN and IPS technologies [2]. Generally I think that TN is probably best for a monitor but in theory IPS shouldn’t be far behind.
The Philips monitor has a screen with a shiny surface which may be good for a TV but isn’t good for a monitor. Also it seemed to blur the pixels a bit which again is probably OK for a TV that is trying to emulate curved images but not good for a monitor where it’s all artificial straight lines. The most important thing for me in a monitor is how well it displays text in small fonts, for that I don’t really want the round parts of the letters to look genuinely round as a clear octagon or rectangle is better than a fuzzy circle.
There is some controversy about the ideal size for monitors. Some people think that nothing larger than 28″ is needed and some people think that a 43″ is totally usable. After testing I determined that 43″ is really too big, I had to move to see it all. Also for my use it’s convenient to be able to turn a monitor slightly to allow someone else to get a good view and a 43″ monitor is too large to move much (maybe future technology for lighter monitors will change this).
Previously I had been unable to get my Samsung monitor to work at 4K resolution with 60Hz and had believed it was due to cheap video cards. I got the Philips monitor to work with HDMI so it’s apparent that the Samsung monitor doesn’t do 4K@60Hz on HDMI. This isn’t a real problem as the Samsung monitor doesn’t have built in speakers. The Philips monitor has built in speakers for HDMI sound which means one less cable to my PC and no desk space taken by speakers.
I bought the Philips monitor on eBay in “opened unused” condition. Inside the box was a sheet with a printout stating that the monitor blanks the screen periodically, so the seller knew that it wasn’t in unused condition, it was tested and failed the test. If the Philips monitor had been as minimally broken as described then I might have kept it. However it seems that certain patterns of input caused it to reboot. For example I could be watching Netflix and have it drop out, I would press the left arrow to watch that bit again and have it drop out again. On one occasion I did a test and found that a 5 second section of Netflix content caused the monitor to reboot on 6/8 times I viewed it. The workaround I discovered was to switch between maximised window and full-screen mode when it had a dropout. So I just press left-arrow and then ‘F’ and I can keep watching. That’s not what I expect from a $700 monitor!
I considered checking for Philips firmware updates but decided against it because I didn’t want to risk voiding the warranty if it didn’t work correctly and I decided I just didn’t like the monitor that much.
Ideally for my next monitor I’ll get a 4K screen of about 35″, TN, and a screen that’s not shiny. At the moment there doesn’t seem to be many monitors between 32″ and 43″ in size, so 32″ may do. I am quite happy with the Samsung monitor so getting the same but slightly larger is fine. It’s a pity they stopped making 5K displays.

[1] https://www.philips.com.au/c-p/438P1_75/brilliance-4k-ultra-hd-lcd-display-with-multiview
[2] https://history-computer.com/tn-vs-ips/



Related posts:
cheap big TFT monitor  I just received the latest Dell advert, they are offering...
Desklab Portable USB-C Monitor  I just got a 15.6″ 4K resolution Desklab portable touchscreen...
Sound Device Order with ALSA  One problem I have had with my new Dell PowerEdge...




        29 June, 2022 12:00PM

         by etbe












      Michael Ablassmeier






        More work on virtnbdbackup


        Had some time to add more features to my libvirt backup utility, now it
supports:


  Added backup mode differencial.
  Save virtual domains UEFI bios, initrd and kernel images if defined.
  virtnbdmap now uses the nbdkit COW plugin to map the backups as regular
NBD device. This allows users to replay complete backup chains
(full+inc/diff) to recover single files. As the resulting device is
writable, one can directly boot the virtual machine from the backup
images.


Check out my last article on that
topic or watch it in action.

Also, the dirty bitmap (incremental backup) feature now seems to be enabled by
default as of newer qemu and libvirt (8.2.x) versions.

As a side note: still there’s an RFP open,
if one is interested in maintaining, as i find myself not having a valid
key in the keyring.. laziness.


        29 June, 2022 12:00AM













        More work on virtnbdbackup


        Had some time to add more features to my libvirt backup utility, now it
supports:


  backing up virtual domains UEFI bios, initrd and kernel images if
defined.
  virtnbdmap now uses the nbdkit COW plugin to map the backups as regular
NBD device. This allows users to replay complete backup chains
(full+inc/diff) to recover single files. Also makes the mapped device
writable, as such one can directly boot the virtual machine from the backup
images.


Check out my last article on that
topic or watch it in action.

As a side note: still there’s an RFP open,
if one is interested in maintaining, as i find myself not having a valid
key in the keyring.. laziness.


        29 June, 2022 12:00AM







    June 28, 2022






      Dima Kogan






        vnlog 1.33 released



This is a minor release to the vnlog toolkit that adds a few convenience options
to the vnl-filter tool. The new options are



vnl-filter -l


Prints out the existing columns, and exits. I've been low-level wanting this for
years, but never acutely-enough to actually write it. Today I finally did it.





vnl-filter --sub-abs


Defines an absolute-value abs() function in the default awk mode. I've been
low-level wanting this for years as well. Previously I'd use --perl just to
get abs(), or I'd explicitly define it: =–sub 'abs(x) {return x>0?x:-x;}'=.
Typing all that out was becoming tiresome, and now I don't need to anymore.





vnl-filter --begin ... and vnl-filter --end ...


Theses add BEGIN and END clauses. They're useful to, for instance, use a
perl module in BEGIN, or to print out some final output in END. Previously
you'd add these inside the --eval block, but that was awkward because BEGIN
and END would then appear inside the while(<>) { } loop. And there was no
clear was to do it in the normal -p mode (no --eval).




Clearly these are all minor, since the toolkit is now mature. It does everything
I want it to, that doesn't require lots of work to implement. The big missing
features that I want would patch the underlying GNU coreutils instead of vnlog:



The sort tool can select different sorting modes, but join works only
with alphanumeric sorting. join should have similarly selectable sorting
modes. In the vnlog wrappe I can currently do something like vnl-join
  --vnl-sort n. This would pre-sort the input alphanumerically, and then
post-sort it numerically. That is slow for big datasets. If join could
handle numerically-sorted data directly, neither the pre- or post-sorts would
be needed


When joining on a numerical field, join should be able to do some sort of
interpolation when given fields that don't match exactly.




Both of these probably wouldn't take a ton of work to implement, and I'll look
into it someday.





        28 June, 2022 04:47PM

         by Dima Kogan


















      Jonathan Dowland






        WadC 3.1





Example map with tuneables on the right




WadC — the procedural programming environment for generating Doom
maps — version 3.1 has been released. The majority of this was done
a long time ago, but I've dragged my feet in releasing it. I've said
this before, but this is intended to be the last release I do of WadC.

The headline feature for this release is the introduction of a tuning
concept I had for the UI. It occurred to me that a beginner to WadC
might want to load up an example program which is potentially very
complex and hard to unpick to figure out how it works. If the author
could mark certain variables as "tuneable", the UI could provide an
easy way for someone to tweak parameters and then see what happened.

I had in mind the walls of patch panels and knobs you see with analog
synthesizers: tweak this thing over here and see what happens over
there.

I think this kind of feature would be useful in other, similar programming
environments, like OpenSCAD: I don't think they do yet, but I could be wrong.







This release of WadC is dedicated to the memory of Kayvan Walker (1983-2022).
Kayvan was a childhood friend who committed suicide in March this year. Back
in the nineties, Kayvan was responsible for introducing me to Doom in the
first place: I used to visit his house on the way home to mine, as it was
on the walk back from School. His mum works in IT and always encouraged us into it.
Doom was so far ahead, in technical terms, of any other computer game I'd
ever seen, and was the closest thing we had to virtual reality: we could
create our own worlds. It's in no small part thanks to Kayvan — and his
Mum — that I'm still creating worlds, nearly thirty years later. I owe my
career and most of my hobbies to those pivotal moments. Thank you both.

Kayvan did a lot more for me than just introduce me to Doom, or computing. He
was one of a set of friends that I had every confidence that, no matter what,
we would always be friends, through thick and thin. I miss him terribly.

Please, if you reading this, are suffering, talk to someone. In the UK you can
talk to Samaritans on 116
123.


        28 June, 2022 02:18PM







    June 25, 2022






      Jamie McClelland






        Deleting an app won't bring back Roe v Wade


        In some ways it feels like 2016 all over again.
I’m seeing panic-stricken calls for everyone to delete their period apps, close
their Facebook accounts, de-Google their cell phones and, generally speaking,
turn their entire online lives upside down to avoid the techno-surveillance
dragnet unleashed by the overturning of Roe v. Wade.
I’m sympathetic and generally agree that many of us should do most of those
things on any given day. But, there is a serious problem with this cycle of
repression and panic: it’s very bad for organizing.
In our rush to give people concrete steps they can take to feel safer, we’re
fueling a frenzy of panic and fear, which seriously inhibits activism.
Now is the time to remind people that, over the last 20 years, a growing
movement of organizers and technologists have been building user-driven,
privacy-respecting, consentful technology platforms as well as organizations
and communities to develop them.
We have an entire eco system of:

technology platform cooperatives,
movement aligned Internet providers that pre-date the founding of Twitter
and are still going strong (May First,
Riseup, and Autistici just
to name a few),
the fediverse, a well developed,
de-centralized alterntiave to corporate social media (try
Social.coop if you want to get started),
powerful open source, privacy respecting software geared for organizing and movement providers
hosting it (see CiviCRM and Progressive Technology
Project),
multi-year campaigns targeting poor tech practices of corporate technology
giants (see Mijente’s No Tech for
ICE),
so many more examples, far too numerous to name.

All of these projects need our love and support over the long haul.  Please
help spread the word - rather then just deleting an app, let’s encourage people
to join an organziation or try out a new kind of technology that will serve us
down the road when we may need it even more then today.


        25 June, 2022 10:27PM














      Ryan Kavanagh






        Routable network addresses with OpenIKED and systemd-networkd


        I’ve been using OpenIKED for some time now to configure my VPN.
One of its features is that it can dynamically assign addresses on the internal network to clients, and clients can assign these addresses and routes to interfaces.
However, these interfaces must exist before iked can start.
Some months ago I switched my Debian laptop’s configuration from the traditional ifupdown to systemd-networkd.
It took me some time to figure out how to have systemd-networkd create dummy interfaces on which iked can install addresses, but also not interfere with iked by trying to manage these interfaces.
Here is my working configuration.
First, I have systemd create the interface dummy1 by creating a systemd.netdev(5) configuration file at /etc/systemd/network/20-dummy1.netdev:
[NetDev]
Name=dummy1
Kind=dummy

Then I tell systemd not to manage this interface by creating a systemd.network(5) configuration file at /etc/systemd/network/20-dummy1.network:
[Match]
Name=dummy1
Unmanaged=yes

Restarting systemd-networkd causes these interfaces to get created, and we can then check their status using networkctl(8):
$ systemctl restart systemd-networkd.service
$ networkctl
IDX LINK     TYPE     OPERATIONAL SETUP
  1 lo       loopback carrier     unmanaged
  2 enp2s0f0 ether    off         unmanaged
  3 enp5s0   ether    off         unmanaged
  4 dummy1   ether    degraded    configuring
  5 dummy3   ether    degraded    configuring
  6 sit0     sit      off         unmanaged
  8 wlp3s0   wlan     routable    configured
  9 he-ipv6  sit      routable    configured

8 links listed.
Finally, I configure my flows in /etc/iked.conf, making sure to assign the received address to the interface dummy1.
ikev2 'hades' active esp \
        from dynamic to 10.0.1.0/24 \
        peer hades.rak.ac \
        srcid '/CN=asteria.rak.ac' \
        dstid '/CN=hades.rak.ac' \
        request address 10.0.1.103 \
        iface dummy1

Restarting openiked and checking the status of the interface reveals that it has been assigned an address on the internal network and that it is routable:
$ systemctl restart openiked.service
$ networkctl status dummy1
● 4: dummy1
                     Link File: /usr/lib/systemd/network/99-default.link
                  Network File: /etc/systemd/network/20-dummy1.network
                          Type: ether
                          Kind: dummy
                         State: routable (configured)
                  Online state: online
                        Driver: dummy
              Hardware Address: 22:50:5f:98:a1:a9
                           MTU: 1500
                         QDisc: noqueue
  IPv6 Address Generation Mode: eui64
          Queue Length (Tx/Rx): 1/1
                       Address: 10.0.1.103
                                fe80::2050:5fff:fe98:a1a9
                           DNS: 10.0.1.1
                 Route Domains: .
             Activation Policy: up
           Required For Online: yes
             DHCP6 Client DUID: DUID-EN/Vendor:0000ab11aafa4f02d6ac68d40000
I’d be happy to hear if there are simpler or more idiomatic ways to configure this under systemd.


        25 June, 2022 11:41AM







    June 24, 2022










      Kees Cook






        finding binary differences


        As part of the continuing work to replace 1-element arrays in the Linux kernel, it’s very handy to show that a source change has had no executable code difference. For example, if you started with this:
struct foo {
    unsigned long flags;
    u32 length;
    u32 data[1];
};

void foo_init(int count)
{
    struct foo *instance;
    size_t bytes = sizeof(*instance) + sizeof(u32) * (count - 1);
    ...
    instance = kmalloc(bytes, GFP_KERNEL);
    ...
};

And you changed only the struct definition:
-    u32 data[1];
+    u32 data[];

The bytes calculation is going to be incorrect, since it is still subtracting 1 element’s worth of space from the desired count. (And let’s ignore for the moment the open-coded calculation that may end up with an arithmetic over/underflow here; that can be solved separately by using the struct_size() helper or the size_mul(), size_add(), etc family of helpers.)
The missed adjustment to the size calculation is relatively easy to find in this example, but sometimes it’s much less obvious how structure sizes might be woven into the code. I’ve been checking for issues by using the fantastic diffoscope tool. It can produce a LOT of noise if you try to compare builds without keeping in mind the issues solved by reproducible builds, with some additional notes. I prepare my build with the “known to disrupt code layout” options disabled, but with debug info enabled:
$ KBF="KBUILD_BUILD_TIMESTAMP=1970-01-01 KBUILD_BUILD_USER=user KBUILD_BUILD_HOST=host KBUILD_BUILD_VERSION=1"
$ OUT=gcc
$ make $KBF O=$OUT allmodconfig
$ ./scripts/config --file $OUT/.config \
        -d GCOV_KERNEL -d KCOV -d GCC_PLUGINS -d IKHEADERS -d KASAN -d UBSAN \
        -d DEBUG_INFO_NONE -e DEBUG_INFO_DWARF_TOOLCHAIN_DEFAULT
$ make $KBF O=$OUT olddefconfig

Then I build a stock target, saving the output in “before”. In this case, I’m examining drivers/scsi/megaraid/:
$ make -jN $KBF O=$OUT drivers/scsi/megaraid/
$ mkdir -p $OUT/before
$ cp $OUT/drivers/scsi/megaraid/*.o $OUT/before/

Then I patch and build a modified target, saving the output in “after”:
$ vi the/source/code.c
$ make -jN $KBF O=$OUT drivers/scsi/megaraid/
$ mkdir -p $OUT/after
$ cp $OUT/drivers/scsi/megaraid/*.o $OUT/after/

And then run diffoscope:
$ diffoscope $OUT/before/ $OUT/after/

If diffoscope output reports nothing, then we’re done. 🥳
Usually, though, when source lines move around other stuff will shift too (e.g. WARN macros rely on line numbers, so the bug table may change contents a bit, etc), and diffoscope output will look noisy. To examine just the executable code, the command that diffoscope  used is reported in the output, and we can run it directly, but with possibly shifted line numbers not reported. i.e. running objdump without --line-numbers:
$ ARGS="--disassemble --demangle --reloc --no-show-raw-insn --section=.text"
$ for i in $(cd $OUT/before && echo *.o); do
        echo $i
        diff -u <(objdump $ARGS $OUT/before/$i | sed "0,/^Disassembly/d") \
                <(objdump $ARGS $OUT/after/$i  | sed "0,/^Disassembly/d")
done

If I see an unexpected difference, for example:
-    c120:      movq   $0x0,0x800(%rbx)
+    c120:      movq   $0x0,0x7f8(%rbx)

Then I'll search for the pattern with line numbers added to the objdump output:
$ vi <(objdump --line-numbers $ARGS $OUT/after/megaraid_sas_fp.o)

I'd search for "0x0,0x7f8", find the source file and line number above it, open that source file at that position, and look to see where something was being miscalculated:
$ vi drivers/scsi/megaraid/megaraid_sas_fp.c +329

Once tracked down, I'd start over at the "patch and build a modified target" step above, repeating until there were no differences. For example, in the starting example, I'd also need to make this change:
-    size_t bytes = sizeof(*instance) + sizeof(u32) * (count - 1);
+    size_t bytes = sizeof(*instance) + sizeof(u32) * count;

Though, as hinted earlier, better yet would be:
-    size_t bytes = sizeof(*instance) + sizeof(u32) * (count - 1);
+    size_t bytes = struct_size(instance, data, count);

But sometimes adding the helper usage will add binary output differences since they're performing overflow checking that might saturate at SIZE_MAX. To help with patch clarity, those changes can be done separately from fixing the array declaration.
© 2022, Kees Cook. This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 License.


        24 June, 2022 08:11PM

         by kees














      Reproducible Builds






        Supporter spotlight: Hans-Christoph Steiner of the F-Droid project




The Reproducible Builds project relies on several projects, supporters and sponsors for financial support, but they are also valued as ambassadors who spread the word about our project and the work that we do.

This is the fifth instalment in a series featuring the projects, companies and individuals who support the Reproducible Builds project. We started this series by featuring the Civil Infrastructure Platform project and followed this up with a post about the Ford Foundation as well as a recent ones about ARDC, the Google Open Source Security Team (GOSST) and  Jan Nieuwenhuizen on Bootstrappable Builds, GNU Mes and GNU Guix.

Today, however, we will be talking with Hans-Christoph Steiner from the F-Droid project.






Chris: Welcome, Hans! Could you briefly tell me about yourself?

Hans: Sure. I spend most of my time trying to private communications software usable by everyone, designing interactive software with a focus on human perceptual capabilities and building networks with free software. I’ve been involved in Debian since approximately 2008 and became an official Debian developer in 2011. In the little time left over from that, I sometimes compose music with computers from my home in Austria.



Chris: For those who have not heard of it before, what is the F-Droid project?

Hans: F-Droid is a community-run app store that provides free software applications for Android phones. First and foremost our goal is to represent our users. In particular, we review all of the apps that we distribute, and these reviews not only check for whether something is definitely free software or not, we additionally look for ‘ethical’ problems with applications as well — issues that we call ‘Anti-Features’. Since the project began in 2010, F-Droid now offers almost 4,000 free-software applications.

F-Droid is also a ‘app store kit’ as well, providing all the tools that are needed to operate an free app store of your own. It also includes complete build and release tools for managing the process of turning app source code into published builds.



Chris: How exactly does F-Droid differ from the Google Play store? Why might someone use F-Droid over Google Play?



Hans: One key difference to the Google Play Store is that F-Droid does not ship proprietary software by default. All apps shipped from f-droid.org are built from source on our own builders. This is partly because F-Droid is backed by the free software community; that is, people who have engaged in the free software community long before Android was conceived, and, in particular, share many — if not all — of its values. Using F-Droid will therefore feel very familiar to anyone familiar with a modern Linux distribution.



Chris: How do you describe reproducibility from the F-Droid point of view?

Hans: All centralised software repositories are extremely tempting targets for exploitation by malicious third parties, and the kinds of personal or otherwise sensitive data on our phones only increase this risk. In F-Droid’s case, not only could the software we distribute be theoretically replaced with nefarious copies, the build infrastructure that generates that software could be compromised as well.

F-Droid having reproducible builds is extremely important as it allows us to verify that our build systems have not been compromised and distributing malware to our users. In particular, if an independent build infrastructure can produce precisely the same results from a build, then we can be reasonably sure that they haven’t been compromised. Technically-minded users can also validate their builds on their own systems too, further increasing trust in our build infrastructure. (This can be performed using fdroid verify command.)

Our signature & trust scheme means that F-Droid can verify that an app is 100% free software whilst still using the developer’s original .APK file. More details about this may be found in our reproducibility documentation and on the page about our Verification Server.



Chris: How do you see F-Droid fitting into the rest of the modern security ecosystem?



Hans: Whilst F-Droid inherits all of the social benefits of free software, F-Droid takes special care to respect your privacy as well — we don’t attempt to track your device in any way. In particular, you don’t need an account to use the F-Droid client, and F-Droid doesn’t send any device-identifying information to our servers… other than its own version number.

What is more, we mark all apps in our repository that track you, and users can choose to hide any apps that has been tagged with a specific Anti-Feature in the F-Droid settings. Furthermore, any personal data you decide to give us (such as your email address when registering for a forum account) goes no further than us as well, and we pledge that it will never be used for anything beyond merely maintaining your account.



Chris: What would ‘fully reproducible’ mean to F-Droid? What it would look like if reproducibility was a ‘solved problem’? Or, rather, what would be your ‘ultimate’ reproducibility goals?

Hans: In an ideal world, every application submitted to F-Droid would not only build reproducibly, but would come with a cryptographically-signed signature from the developer as well. Then we would only distribute an compiled application after a build had received a number of matching signatures from multiple, independent third parties. This would mean that our users were not placing their trust solely in software developers’ machines, and wouldn’t be trusting our centralised build servers as well.



Chris: What are the biggest blockers to reaching this state? Are there any key steps or milestones to get there?



Hans: Time is probably the main constraint to reaching this goal. Not only do we need system administrators on an ongoing basis but we also need to incorporate reproducibly checks into our Continuous Integration (CI) system. We are always looking for new developers to join our effort, as well as learning about how to better bring them up to speed.

Separate to this, we often experience blockers with reproducibility-related bugs in the Android build tooling. Luckily, upstreams do ultimately address these issues, but in some cases this has taken three or four years to reach end-users and developers. Unfortunately, upstream is not chiefly concerned with the security aspects of reproducibility builds; they care more about how it can minimise and optimise download size and speed.



Chris: Are you tracking any statistics about reproducibility in F-Droid over time? If so, can you share them? Does F-Droid track statistics about its own usage?

Hans: To underline a topic touched on above, F-Droid is dedicated to preserving the privacy of its users; we therefore do not  record usage statistics. This is, of course, in contrast to other application stores.

However, we are in a position to track whether packages in F-Droid are reproducible or not. As in: what proportion of APKs in F-Droid have been independently verified over time? Unfortunately, due to time constraints, we do not yet automatically publish charts for this.

We do publish some raw data that is related, though, and we naturally welcome contributions of visualizations based on any and all of our data. The “All our APIs” page on our wiki is a good place to start for someone wanting to contribute, everything about reproducible F-Droid apps is available in JSON format, what’s missing are apps or dashboards making use of the available raw data.



Chris: Many thanks for this interview, Hans, and for all of your work on F-Droid and elsewhere. If someone wanted to get in touch or learn more about the project, where might someone go?



Hans: The best way to find out about F-Droid is, of course, the main F-Droid homepage, but we are also on Twitter @fdroidorg. We have many avenues to participate and to learn more! We have an About page on our website and a thriving forum. We also have part of our documentation specifically dedicated to reproducible builds.






For more information about the Reproducible Builds project, please see our website at
reproducible-builds.org. If you are interested in
ensuring the ongoing security of the software that underpins our civilisation
and wish to sponsor the Reproducible Builds project, please reach out to the
project by emailing
contact@reproducible-builds.org.


        24 June, 2022 10:00AM







    June 23, 2022










      Raphaël Hertzog






        Freexian’s report about Debian Long Term Support, May 2022






Like each month, have a look at the work funded by Freexian’s Debian LTS offering.



Debian project funding



Two [1, 2] projects are in the pipeline now. Tryton project is in a final phase. Gradle projects is fighting with technical difficulties.



In May, we put aside 2233 EUR to fund Debian projects.



We’re looking forward to receive more projects from various Debian teams! Learn more about the rationale behind this initiative in this article.



Debian LTS contributors



In May, 14 contributors have been paid to work on Debian LTS, their reports are available:



Abhijith PA did 14.0h (out of 14h assigned).Andreas Rönnquist did 14.5h (out of 25.0h assigned), thus carrying over 10.5h to June.Anton Gladky did 19h (out of 19h assigned).Ben Hutchings did 8h (out of 11h assigned and 13h from April), thus carrying over 16h to June.Chris Lamb did 18h (out of 18h assigned).Dominik George did 2h (out of 20.0h assigned), thus carrying over 18h to June.Enrico Zini did 9.5h (out of 16.0h assigned), thus carrying over 6.5h to June.Emilio Pozuelo Monfort did 28h (out of 13.75h assigned and 35.25h from April), thus carrying over 21h to June.Markus Koschany did 40h (out of 40h assigned).Roberto C. Sánchez did 13.5h (out of 32h assigned), thus carrying over 18.5h to June.Sylvain Beucler did 23.5h (out of 20h assigned and 20h from April), thus carrying over 16.5h to June.Stefano Rivera did 5h in April and 14h in May (out of 17.5h assigned), thus anticipating 1.5h for June.Thorsten Alteholz did 40h (out of 40h assigned).Utkarsh Gupta did 35h (out of 19h assigned and 30h from April), thus carrying over 14h to June.



Evolution of the situation



In May we released 30 DLAs. The security tracker currently lists 71 packages with a known CVE and the dla-needed.txt file has 65 packages needing an update.



The number of paid contributors increased significantly, we are pleased to welcome our latest team members: Andreas Rönnquist, Dominik George, Enrico Zini and Stefano Rivera.



It is worth pointing out that we are getting close to the end of the LTS period for Debian 9. After June 30th, no new security updates will be made available on security.debian.org. We are preparing to overtake Debian 10 Buster for the next two years and to make this process as smooth as possible.



But Freexian and its team of paid Debian contributors will continue to maintain Debian 9 going forward for the customers of the Extended LTS offer. If you have Debian 9 servers to keep secure, it’s time to subscribe!



You might not have noticed, but Freexian formalized a mission statement where we explain that our purpose is to help improve Debian. For this, we want to fund work time for the Debian developers that recently joined Freexian as collaborators. The Extended LTS and the PHP LTS offers are built following a model that will help us to achieve this if we manage to have enough customers for those offers. So consider subscribing: you help your organization but you also help Debian!



Thanks to our sponsors



Sponsors that joined recently are in bold.



Platinum sponsors:TOSHIBA (for 81 months)GitHub (for 72 months)Civil Infrastructure Platform (CIP) (for 49 months)Gold sponsors:Roche Diagnostics International AG (for 92 months)Linode (for 86 months)Babiel GmbH (for 75 months)Plat’Home (for 75 months)University of Oxford (for 31 months)Deveryware (for 18 months)VyOS Inc (for 13 months)EDF SASilver sponsors:The Positive Internet Company (for 97 months)Domeneshop AS (for 96 months)Nantes Métropole (for 90 months)Univention GmbH (for 82 months)Université Jean Monnet de St Etienne (for 82 months)Ribbon Communications, Inc. (for 76 months)Exonet B.V. (for 66 months)Leibniz Rechenzentrum (for 60 months)CINECA (for 49 months)Ministère de l’Europe et des Affaires Étrangères (for 43 months)Cloudways Ltd (for 33 months)Dinahosting SL (for 31 months)Bauer Xcel Media Deutschland KG (for 25 months)Platform.sh (for 25 months)Moxa Intelligence Co., Ltd. (for 19 months)sipgate GmbH (for 16 months)OVH US LLC (for 14 months)Tilburg University (for 14 months)GSI Helmholtzzentrum für Schwerionenforschung GmbH (for 6 months)Telecats BV (for 4 months)Soliton Systems K.K. (for 3 months)Bronze sponsors:Evolix (for 97 months)Seznam.cz, a.s. (for 97 months)Intevation GmbH (for 94 months)Linuxhotel GmbH (for 94 months)Daevel SARL (for 92 months)Bitfolk LTD (for 91 months)Megaspace Internet Services GmbH (for 91 months)Greenbone Networks GmbH (for 90 months)NUMLOG (for 90 months)WinGo AG (for 90 months)Ecole Centrale de Nantes – LHEEA (for 86 months)Entr’ouvert (for 81 months)Adfinis AG (for 78 months)GNI MEDIA (for 73 months)Laboratoire LEGI – UMR 5519 / CNRS (for 73 months)Tesorion (for 73 months)Bearstech (for 64 months)LiHAS (for 64 months)People Doc (for 61 months)Catalyst IT Ltd (for 59 months)Supagro (for 54 months)Demarcq SAS (for 53 months)Université Grenoble Alpes (for 39 months)TouchWeb SAS (for 31 months)SPiN AG (for 28 months)CoreFiling (for 23 months)Institut des sciences cognitives Marc Jeannerod (for 18 months)Observatoire des Sciences de l’Univers de Grenoble (for 15 months)Tem Innovations GmbH (for 10 months)WordFinder.pro (for 9 months)CNRS DT INSU Résif (for 8 months)Alter Way


        23 June, 2022 12:15PM

         by Raphaël Hertzog














      Reproducible Builds (diffoscope)






        diffoscope 217 released


        The diffoscope maintainers are pleased to announce the release of diffoscope
version 217. This version includes the following changes:

* Update test fixtures for GNU readelf 2.38 (now in Debian unstable).
* Be more specific about the minimum required version of readelf (ie.
  binutils) as it appears that this "patch" level version change resulted in
  a change of output, not the "minor" version. (Closes: #1013348)
* Don't leak the (likely-temporary) pathname when comparing PDF documents.


You find out more by visiting the project homepage.


        23 June, 2022 12:00AM







    June 22, 2022






      John Goerzen






        I Finally Found a Solid Debian Tablet: The Surface Go 2


        I have been looking for a good tablet for Debian for… well, years.  I want thin, light, portable, excellent battery life, and a servicable keyboard.
For a while, I tried a Lenovo Chromebook Duet.  It meets the hardware requirements, well sort of.  The problem is with performance and the OS.  I can run Debian inside the ChromeOS Linux environment.  That works, actually pretty well.  But it is slow.  Terribly, terribly, terribly slow.  Emacs takes minutes to launch.  apt-gets also do.  It has barely enough RAM to keep its Chrome foundation happy, let alone a Linux environment also.  But basically it is too slow to be servicable.  Not just that, but I ran into assorted issues with having it tied to a Google account – particularly being unable to login unless I had Internet access after an update.  That and my growing concern over Google’s privacy practices led me sort of write it off.
I have a wonderful System76 Lemur Pro that I’m very happy with.  Plenty of RAM, a good compromise size between portability and screen size at 14.1″, and so forth.  But a 10″ goes-anywhere it’s not.
I spent quite a lot of time looking at thin-and-light convertible laptops of various configurations.  Many of them were quite expensive, not as small as I wanted, or had dubious Linux support.  To my surprise, I wound up buying a Surface Go 2 from the Microsoft store, along with the Type Cover.  They had a pretty good deal on it since the Surface Go 3 is out; the highest-processor model of the Go 2 is roughly similar to the Go 3 in terms of performance.
There is an excellent linux-surface project out there that provides very good support for most Surface devices, including the Go 2 and 3.
I put Debian on it.  I had a fair bit of hassle with EFI, and wound up putting rEFInd on it, which mostly solved those problems.  (I did keep a Windows partition, and if it comes up for some reason, the easiest way to get it back to Debian is to use the Windows settings tool to reboot into advanced mode, and then select the appropriate EFI entry to boot from there.)
Researching on-screen keyboards, it seemed like Gnome had the most mature.  So I wound up with Gnome (my other systems are using KDE with tiling, but I figured I’d try Gnome on it.)  Almost everything worked without additional tweaking, the one exception being the cameras.  The cameras on the Surfaces are a known point of trouble and I didn’t bother to go to all the effort to get them working.
With 8GB of RAM, I didn’t put ZFS on it like I do on other systems.  Performance is quite satisfactory, including for Rust development.  Battery life runs about 10 hours with light use; less when running a lot of cargo builds, of course.
The 1920×1280 screen is nice at 10.5″.  Gnome with Wayland does a decent job of adjusting to this hi-res configuration.
I took this as my only computer for a trip from the USA to Germany.  It was a little small at times; though that was to be expected.  It let me take a nicely small bag as a carryon, and being light, it was pleasant to carry around in airports.  It served its purpose quite well.
One downside is that it can’t be powered by a phone charger like my Chromebook Duet can.  However, I found a nice slim 65W Anker charger that could charge it and phones simultaneously that did the job well enough (I left the Microsoft charger with the proprietary connector at home).
The Surface Go 2 maxes out at a 128GB SSD.  That feels a bit constraining, especially since I kept Windows around.  However, it also has a micro SD slot, so you can put LUKS and ext4 on that and use it as another filesystem.  I popped a micro SD I had lying around into there and that felt a lot better storage-wise.  I could also completely zap Windows, but that would leave no way to get firmware updates and I didn’t really want to do that.  Still, I don’t use Windows and that could be an option also.
All in all, I’m pretty pleased with it.  Around $600 for a fully-functional Debian tablet, with a keyboard is pretty nice.
I had been hoping for months that the Pinetab would come back into stock, because I’d much rather support a Linux hardware vendor, but for now I think the Surface Go series is the most solid option for a Linux tablet.


        22 June, 2022 11:46PM

         by John Goerzen







    June 21, 2022










      Louis-Philippe Véronneau






        Montreal's Debian & Stuff - June 2022


        As planned, we held our second local Debian meeting of the year last Sunday. We
met at the lovely Eastern Bloc (an artists' hacklab) to work on Debian
(and other stuff!), chat and socialise.
Although there were fewer people than at our last meeting1, we still did
a lot of work!
I worked on fixing a bunch of bugs in Clojure packages2, LeLutin worked
on podman and packaged libinfluxdb-http-perl and anarcat worked on
internetarchive, trocla and moneta. Olivier also came by and worked on
debugging his Kali install.
We are planning to have our next meeting at the end of August. If you are
interested, the best way to stay in touch is either to subscribe to our
mailing list or to join our IRC channel (#debian-quebec on OFTC). Events
are also posted on Quebec's Agenda du libre.
Many thanks to Debian for providing us a budget to rent the venue for the day
and for the pizza! Here is a nice picture anarcat took of (one of) the glasses
of porter we had afterwards, at the next door brewery:





Summer meetings are always less populous and it also happened to be
  Father's Day... ↩


#1012824, #1011856, #1011837, #1011844, #1011864
  and #1011967. ↩





        21 June, 2022 05:15PM

         by Louis-Philippe Véronneau


















      Steve Kemp






        Writing a simple TCL interpreter in golang


        Recently I was reading Antirez's piece TCL the Misunderstood again, which is a nice defense of the utility and value of the TCL language.

TCL is one of those scripting languages which used to be used a hell of a lot in the past, for scripting routers, creating GUIs, and more.  These days it quietly lives on, but doesn't get much love.  That said it's a remarkably simple language to learn, and experiment with.

Using TCL always reminds me of FORTH, in the sense that the syntax consists of "words" with "arguments", and everything is a string (well, not really, but almost.  Some things are lists too of course).

A simple overview of TCL would probably begin by saying that everything is a command, and that the syntax is very free.  There are just a couple of clever rules which are applied consistently to give you a remarkably flexible environment.

To get started we'll set a string value to a variable:

  set name "Steve Kemp"
  => "Steve Kemp"


Now you can output that variable:

  puts "Hello, my name is $name"
  => "Hello, my name is Steve Kemp"


OK, it looks a little verbose due to the use of set, and puts is less pleasant than print or echo, but it works.  It is readable.

Next up?  Interpolation.  We saw how $name expanded to "Steve Kemp" within the string.  That's true more generally, so we can do this:

 set print pu
 set me    ts

 $print$me "Hello, World"
 => "Hello, World"


There "$print" and "$me" expanded to "pu" and "ts" respectively.  Resulting in:

 puts "Hello, World"


That expansion happened before the input was executed, and works as you'd expect.  There's another form of expansion too, which involves the [ and ] characters.  Anything within the square-brackets is replaced with the contents of evaluating that body.  So we can do this:

 puts "1 + 1 = [expr 1 + 1]"
 => "1 + 1 = 2"


Perhaps enough detail there, except to say that we can use { and } to enclose things that are NOT expanded, or executed, at parse time.  This facility lets us evaluate those blocks later, so you can write a while-loop like so:

 set cur 1
 set max 10

 while { expr $cur <= $max } {
       puts "Loop $cur of $max"
       incr cur
 }


Anyway that's enough detail.  Much like writing a FORTH interpreter the key to implementing something like this is to provide the bare minimum of primitives, then write the rest of the language in itself.

You can get a usable scripting language with only a small number of the primitives, and then evolve the rest yourself.  Antirez also did this, he put together a small TCL interpreter in C named picol:


picol, a Tcl interpreter in 550 lines of C code


Other people have done similar things, recently I saw this writeup which follows the same approach:


Partcl - a tiny command language


So of course I had to do the same thing, in golang:


https://github.com/skx/critical/


My code runs the original code from Antirez with only minor changes, and was a fair bit of fun to put together.

Because the syntax is so fluid there's no complicated parsing involved, and the core interpreter was written in only a few hours then improved step by step.

Of course to make a language more useful you need I/O, beyond just writing to the console - and being able to run the list-operations would make it much more useful to TCL users, but that said I had fun writing it, it seems to work, and once again I added fuzz-testers to the lexer and parser to satisfy myself it was at least somewhat robust.

Feedback welcome, but even in quiet isolation it's fun to look back at these "legacy" languages and recognize their simplicity lead to a lot of flexibility.


        21 June, 2022 01:00PM












      John Goerzen






        Lessons of Social Media from BBSs


        In the recent article The Internet Origin Story You Know Is Wrong, I was somewhat surprised to see the argument that BBSs are a part of the Internet origin story that is often omitted.  Surprised because I was there for BBSs, and even ran one, and didn’t really consider them part of the Internet story myself.  I even recently enjoyed a great BBS documentary and still didn’t think of the connection on this way.
But I think the argument is a compelling one.
In truth, the histories of Arpanet and BBS networks were interwoven—socially and materially—as ideas, technologies, and people flowed between them. The history of the internet could be a thrilling tale inclusive of many thousands of networks, big and small, urban and rural, commercial and voluntary. Instead, it is repeatedly reduced to the story of the singular Arpanet.
Kevin Driscoll goes on to highlight the social aspects of the “modem world”, how BBSs and online services like AOL and CompuServe were ways for people to connect.  And yet, AOL members couldn’t easily converse with CompuServe members, and vice-versa.  Sound familiar?
Today’s social media ecosystem functions more like the modem world of the late 1980s and early 1990s than like the open social web of the early 21st century. It is an archipelago of proprietary platforms, imperfectly connected at their borders. Any gateways that do exist are subject to change at a moment’s notice. Worse, users have little recourse, the platforms shirk accountability, and states are hesitant to intervene.
Yes, it does.  As he adds, “People aren’t the problem. The problem is the platforms.”
A thought-provoking article, and I think I’ll need to buy the book it’s excerpted from!


        21 June, 2022 01:52AM

         by John Goerzen







    June 20, 2022






      Niels Thykier






        wrap-and-sort with experimental support for comments in devscripts/2.22.2


        In the devscripts package currently in Debian testing (2.22.2), wrap-and-sort has opt-in support for preserving comments in deb822 control files such as  debian/control and debian/tests/control.  Currently, this is an opt-in feature to provide some exposure without breaking anything.



To use the feature, add --experimental-rts-parser to the command line. A concrete example being (adjust to your relevant style):



wrap-and-sort --experimental-rts-parser -tabk



Please provide relevant feedback to #820625 if you have any. If you experience issues, please remember to provide the original control file along with the concrete command line used.



As hinted above, the option is a temporary measure and will be removed again once the testing phase is over, so please do not put it into scripts or packages.  For the same reason, wrap-and-sort will emit a slightly annoying warning when using the option.



Enjoy.


        20 June, 2022 08:00PM

         by Niels Thykier














      John Goerzen






        Pipe Issue Likely a Kernel Bug


        Saturday, I wrote in Pipes, deadlocks, and strace annoyingly fixing them about an issue where a certain pipeline seems to have a deadlock.  I described tracing it into kernel code.  Indeed, it appears to be kernel bug 212295, which has had a patch for over a year that has never been merged.
After continuing to dig into the issue, I eventually reported it as a bug in ZFS.  One of the ZFS people connected this to an older issue my searching hadn’t uncovered.
rincebrain summarized:
I believe, if I understand the bug correctly, it only triggers if you F_SETPIPE_SZ when the writer has put nonzero but not a full unit’s worth in yet, which is why the world isn’t on fire screaming about this – you need to either have a very slow but nonzero or otherwise very strange write pattern to hit it, which is why it doesn’t come up in, say, the CI or most of my testbeds, but my poor little SPARC (440 MHz, 1c1t) and Raspberry Pis were not so fortunate.
You might recall in Saturday’s post that I explained that Filespooler reads a few bytes from the gpg/zstdcat pipeline before spawning and connecting it to zfs receive.  I think this is the critical piece of the puzzle; it makes it much more likely to encounter the kernel bug.  zfs receive is calls F_SETPIPE_SZ when it starts.  Let’s look at how this could be triggered:
In the pre-Filespooler days, the gpg|zstdcat|zfs pipeline was all being set up at once.  There would be no data sent to zfs receive until gpg had initialized and begun to decrypt the data, and then zstdcat had begun to decompress it.  Those things almost certainly took longer than zfs receive’s initialization, meaning that usually F_SETPIPE_SZ would have been invoked before any data entered the pipe.
After switching to Filespooler, the particular situation here has Filespooler reading somewhere around 100 bytes from the gpg|zstdcat part of the pipeline before ever invoking zfs receive.  zstdcat generally emits more than 100 bytes at a time.  Therefore, when Filespooler invokes zfs receive and hooks the pipeline up to it, it has a very high chance of there already being data in the pipeline when zfs receive uses F_SETPIPE_SZ.  This means that the chances of encountering the conditions that trigger the particular kernel bug are also elevated.
ZFS is integrating a patch to no longer use F_SETPIPE_SZ in zfs receive.  I have applied that on my local end to see what happens, and hopefully in a day or two will know for sure if it resolves things.
In the meantime, I hope you enjoyed this little exploration.  It resulted in a new bug report to Rust as well as digging up an existing kernel bug.  And, interestingly, no bugs in filespooler.  Sometimes the thing that changed isn’t the source of the bug!


        20 June, 2022 04:31PM

         by John Goerzen












      Iustin Pop






        Experiment: A week of running


        My sports friends know that I wasn’t able to really run in many, many
years, due to a recurring injury that was not fully diagnosed and
which, after many sessions with the doctor, ended up with OK-ish state
for day-to-day life but also with these words: “Maybe, running is just
not for you?”
The year 2012 was my “running year”. I went to a number of races,
wrote blog posts, then slowly started running only rarely, then a few
years later I was really only running once in a while, and coupled
with a number of bad ideas of the type “lets run today after a long
break, but a lot”, I started injuring my foot.
Add a few more years, some more kilograms on my body, a one event of
jumping with a kid on my shoulders and landing on my bad foot, and the
setup was complete.
Doctor visits, therapy, slow improvements, but not really solving the
problem. 6 months breaks, small attempts at running, pain again,
repeat, pain again, etc. It ended up with me acknowledging that yes,
maybe running is not for me, and I should really give it up.
Incidentally, in 2021, as part of me trying to improve my health/diet,
I tried some thing that is not important for this post and for the
first time in a long time, I was fully, 100%, pain free in my leg
during day-to-day activities. Huh, maybe this is not purely related to
running? From that point on, my foot became, very slowly, better. I
started doing short runs (2-3km), especially on holidays where I can’t
bike, and if I was careful, it didn’t go too bad. But I knew I can’t
run, so these were rare events.
In April this year, on vacation, I run a couple of times - 20km
distance. In May, 12km. Then, there was a Garmin Badge I really
wanted, so against my good judgement, I did a run/walk (2:1 ratio) the
previous weekend, and to my surprise, no unwanted side-effect. And I
got an idea: what if I do short run/walks an entire week? When does my
foot “break”?
I mean, by now I knew that a short (3-4, maybe 5km) run that has
pauses doesn’t negatively impact my foot. What about the 2nd one? Or
the 3rd one? When does it break? Is it distance, or something else?
The other problem was - when to run? I mean, on top of hybrid work
model. When working from home, all good, but when working from the
office? So the other, somewhat more impossible task for me, was to
wake up early and run before 8 AM. Clearly destined to fail!
But, the following day (Monday), I did wake up and 3km. Then Tuesday
again, 3.3km (and later, one hour of biking). Wed - 3.3km. Thu -
4.40km, at 4:1 ratio (2m:30s). Friday, 3.7km (4:1), plus a very long
for me (112km) bike ride.
By this time, I was physically dead. Not my foot, just my entire
body. On Saturday morning, Training Peaks said my form is -52, and it
starts warning below -15. I woke up late and groggy, and I had to
extra motivate myself to go for the last, 5.3km run, to round up the
week.
On Friday and Saturday, my problem leg did start to… how to say,
remind me it is problematic? But not like previously, no waking in the
morning with a stiff tendon. No, just… not fully happy. And, to my
surprise, correlated again with my consumption of problematic food (I
was getting hungrier and hungrier, and eating too much of things I
should keep an eye on).
At this point, with the week behind me:

am ultra-surprised that my foot is not in pieces (yet?)
am still pretty tired (form: -48), but I did manage to run again after
a day of pause from running (and my foot is still OK-ish).
am confused as to what are really my problems…
am convinced that I have some way of running a bit, if I take it
careful (which is hard!)
am really, really hungry; well, not anymore, I ate like a pig for the
last two days.
beat my all-time Garmin record for “weekly intensity minutes” (1174,
damn, 1 more minute and would have been rounder number)…

Did my experiment make me wiser? Not really. Happier? Yes, 100%. I
plan to buy some new running clothes, my current ones are really old.
But did I really understand how my body function? A loud no. Sigh.
The next challenge will be, how to manage my time across multiple
sports (and work, and family, and other hobbies). Still, knowing that
I can anytime go for 25-35 minutes of running, without preparation, is
very reassuring.
Freedom, health and injury-free sports to everyone!


        20 June, 2022 01:17PM












      Petter Reinholdtsen






        My free software activity of late (2022)


        I guess it is time to bring some light on the various free software
and open culture activities and projects I have worked on or been
involved in the last year and a half.

First, lets mention the book
releases I managed to
publish.  The Cory Doctorow book "Hvordan knuse
overvåkningskapitalismen" argue that it is not the magic machine
learning of the big technology companies that causes the surveillance
capitalism to thrive, it is the lack of trust busting to enforce
existing anti-monopoly laws.  I also published a family of
dictionaries for machinists, one sorted on the English words, one
sorted on the Norwegian and the last sorted on the North Sámi words.
A bit on the back burner but not forgotten is the Debian
Administrators Handbook, where a new edition is being worked on.  I
have not spent as much time as I want to help bring it to completion,
but hope I will get more spare time to look at it before the end of
the year.

With my Debian had I have spent time on several projects, both
updating existing packages, helping to bring in new packages and
working with upstream projects to try to get them ready to go into
Debian.  The list is rather long, and I will only mention my own
isenkram, openmotor, vlc bittorrent plugin, xprintidle, norwegian
letter style for latex, bs1770gain, and recordmydesktop.  In addition
to these I have sponsored several packages into Debian, like audmes.

The last year I have looked at several infrastructure projects for
collecting meter data and video surveillance recordings.  This include
several ONVIF related tools like onvifviewer and zoneminder as well as
rtl-433, wmbusmeters and rtl-wmbus.

In parallel with this I have looked at fabrication related free
software solutions like pycam and LinuxCNC.  The latter recently
gained improved translation support using po4a and weblate, which was
a harder nut to crack that I had anticipated when I started.

Several hours have been spent translating free software to
Norwegian Bokmål on the Weblate hosted service.  Do not have a
complete list, but you will find my contributions in at least gnucash,
minetest and po4a.

I also spent quite some time on the Norwegian archiving specification
Noark 5, and its companion project Nikita implementing the API
specification for Noark 5.

Recently I have been looking into free software tools to do company
accounting here in Norway., which present an interesting mix between
law, rules, regulations, format specifications and API interfaces.

I guess I should also mention the Norwegian community driven
government interfacing projects Mimes Brønn and Fiksgatami, which have
ended up in a kind of limbo while the future of the projects is being
worked out.

These are just a few of the projects I have been involved it, and
would like to give more visibility.  I'll stop here to avoid delaying
this post.

As usual, if you use Bitcoin and want to show your support of my
activities, please send Bitcoin donations to my address
15oWEoG9dUPovwmUL9KWAnYRtNJEkP1u1b.


        20 June, 2022 12:30PM












      Jamie McClelland






        A very liberal spam assassin rule


        I just sent myself a test message via Powerbase (a
hosted CiviCRM project for community organizers) and it
didn’t arrive. Wait, nope, there it is in my junk folder with a spam score of
6!
X-Spam-Status: Yes, score=6.093 tagged_above=-999 required=5
	tests=[BAYES_00=-1.9, DKIM_SIGNED=0.1, DKIM_VALID=-0.1,
	DKIM_VALID_AU=-0.1, DKIM_VALID_EF=-0.1, DMARC_MISSING=0.1,
	HTML_MESSAGE=0.001, KAM_WEBINAR=3.5, KAM_WEBINAR2=3.5,
	NO_DNS_FOR_FROM=0.001, SPF_HELO_NONE=0.001, ST_KGM_DEALS_SUB_11=1.1,
	T_SCC_BODY_TEXT_LINE=-0.01] autolearn=no autolearn_force=no
What just happened?
A careful look at the scores suggest that the KAM_WEBINAR and KAM_WEBINAR2
rules killed me. I’ve never heard of them (this email came through a system I’m
not administering). So, I did some searching and found a page with the
rules:
# SEMINARS AND WORKSHOPS SPAM
header   __KAM_WEBINAR1 From =~ /education|career|manage|learning|webinar|project|efolder/i
header   __KAM_WEBINAR2 Subject =~ /last chance|increase productivity|workplace morale|payroll dept|trauma.training|case.study|issues|follow.up|service.desk|vip.(lunch|breakfast)|manage.your|private.business|professional.checklist|customers.safer|great.timesaver|prep.course|crash.course|hunger.to.learn|(keys|tips).(to|for).smarter/i
header   __KAM_WEBINAR3 Subject =~ /webinar|strateg|seminar|owners.meeting|webcast|our.\d.new|sales.video/i
body     __KAM_WEBINAR4 /executive.education|contactid|register now|\d+.minute webinar|management.position|supervising.skills|discover.tips|register.early|take.control|marketing.capabilit|drive.more.sales|leveraging.cloud|solution.provider|have.a.handle|plan.to.divest|being.informed|upcoming.webinar|spearfishing.email|increase.revenue|industry.podcast|\d+.in.depth.tips|early.bird.offer|pmp.certified|lunch.briefing/i

meta     KAM_WEBINAR (__KAM_WEBINAR1 + __KAM_WEBINAR2 + __KAM_WEBINAR3 + __KAM_WEBINAR4 >= 3)
describe KAM_WEBINAR Spam for webinars
score    KAM_WEBINAR 3.5

meta     KAM_WEBINAR2 (__KAM_WEBINAR1 + __KAM_WEBINAR2 + __KAM_WEBINAR3 + __KAM_WEBINAR4 >= 4)
describe KAM_WEBINAR2 Spam for webinars
score    KAM_WEBINAR2 3.5
For those of you who don’t care to parse those regular expressions, here’s a summary:

There are four tests. If you fail 3 or more, you get 3.5 points, if you fail
4 you get another 3.5 points (my email failed all 4).
Here is how I failed them:

The from address can’t have a bunch of words, including “project.” My from address includes my organization’s name: The Progressive Technology Project.
The subject line cannot include a number of strings, including “last chance.” My subject line was “Last change to register for our webinar.”
The subject line cannot include a number of other strings, including “webinar” (and also webcast and even strategy). My subject line was “Last chance to register for our webinar.”
The body of the message cannot include a bunch of strings, including “register now.” Well, you won’t be suprised to know that my email contained the string “Register now.”



Hm. I’m glad I can now fix our email, but this doesn’t work so well for people
with a name that includes “project” that like to organize webinars for which you
have to register.


        20 June, 2022 12:27PM







    June 19, 2022










      Dirk Eddelbuettel






        #38: Faster Feedback Systems


        Engineers build systems. Good engineers always stress and focus efficiency of these systems.
Two recent examples of engineering thinking follow. One was in a video / podcast interview with Martin Thompson (who is a noted high-performance code expert) I came across recently. The overall focus of the hour-long interview is on ‘managing software complexity’. Around minute twenty-two, the conversation turns to feedback loops and systems, and a strong preference for simple and fast systems for more immediate feedback. An important topic indeed.
The second example connects to this and permeates many tweets and other writings by Erik Bernhardsson. He had an earlier 2017 post on ‘Optimizing for iteration speed’, as well as a 17 May 2022 tweet on minimizing feedback loop size, another 28 Mar 2022 tweet reply on shorter feedback loops, then a 14 Feb 2022 post on problems with slow feedback loops, as well as a 13 Jan 2022 post on a priority for tighter feedback loops, and lastly a 23 Jul 2021 post on fast feedback cycles. You get the idea: Erik really digs faster feedback loops. Nobody likes to wait: immediatecy wins each time.
A few years ago, I had touched on this topic with two posts on how to make (R) package compilation (and hence installation) faster. One idea (which I still use whenever I must compile) was in post #11 on caching compilation. Another idea was in post #13: make it faster by not doing it, in this case via binary installation which skip the need for compilation (and which is what I aim for with, say, CI dependencies). Several subsequent posts can be found by scrolling down the r^4 blog section: we stressed the use of the amazing Rutter PPA ‘c2d4u’ for CRAN binaries (often via Rocker containers, the (post #28) promise of RSPM, and the (post #29) awesomeness of bspm. And then in the more recent post #34 from last December we got back to a topic which ties all these things together: Dependencies. We quoted Mies van der Rohe: Less is more. Especially when it comes to dependencies as these elongate the feedback loop and thereby delay feedback.
Our most recent post #37 on r2u connects these dots. Access to a complete set of CRAN binaries with full-dependency resolution accelerates use and installation. This of course also covers testing and continuous integration. Why wait minutes to recompile the same packages over and over when you can install the full Tidyverse in 18 seconds or the brms package and all it needs in 13 seconds as shown in the two gifs also on the r2u documentation site.
You can even power up the example setup of the second gif via this gitpod link giving you a full Ubuntu 22.04 session in your browser to try this: so go forth and install something from CRAN with ease! The benefit of a system such our r2u CRAN binaries is clear: faster feedback loops. This holds whether you work with few or many dependencies, tiny or tidy. Faster matters, and feedback can be had sooner.
And with the title of this post we now get a rallying cry to advocate for faster feedback systems: “FFS”.

This post by Dirk Eddelbuettel originated on his Thinking inside the box blog. Please report excessive re-aggregation in third-party for-profit settings.



        19 June, 2022 03:46PM














      John Goerzen






        Pipes, deadlocks, and strace annoyingly fixing them


        This is a complex tale I will attempt to make simple(ish).  I’ve (re)learned more than I cared to about the details of pipes, signals, and certain system calls – and the solution is still elusive.
For some time now, I have been using NNCP to back up my files.  These backups are sent to my backup system, which effectively does this to process them (each ZFS send is piped to a shell script that winds up running this):
gpg -q -d | zstdcat -T0 | zfs receive -u -o readonly=on "$STORE/$DEST"

This processes tens of thousands of zfs sends per week.  Recently, having written Filespooler, I switched to sending the backups using Filespooler over NNCP.  Now fspl (the Filespooler executable) opens the file for each stream and then connects it to what amounts to this pipeline:
bash -c 'gpg -q -d 2>/dev/null | zstdcat -T0' | zfs receive -u -o readonly=on "$STORE/$DEST"

Actually, to be more precise, it spins up the bash part of it, reads a few bytes from it, and then connects it to the zfs receive.
And this works well — almost always.  In something like 1/1000 of the cases, it deadlocks, and I still don’t know why.  But I can talk about the journey of trying to figure it out (and maybe some of you will have some ideas).
Filespooler is written in Rust, and uses Rust’s Command system.  Effectively what happens is this:

The fspl process has a File handle, which after forking but before invoking bash, it dup2’s to stdin.
The connection between bash and zfs receive is a standard Unix pipe.

I cannot get the problem to duplicate when I run the entire thing under strace -f.  So I am left trying to peek at it from the outside.  What happens if I try to attach to each component with strace -p?

bash is blocking in wait4(), which is expected.
gpg is blocking in write().
If I attach to zstdcat with strace -p, then all of a sudden the deadlock is cleared and everything resumes and completes normally.
Attaching to zfs receive with strace -p causes no output at all from strace for a few seconds, then zfs just writes “cannot receive incremental stream: incomplete stream” and exits with error code 1.

So the plot thickens!  Why would connecting to zstdcat and zfs receive cause them to actually change behavior?  strace works by using the ptrace system call, and ptrace in a number of cases requires sending SIGSTOP to a process.  In a complicated set of circumstances, a system call may return EINTR when a SIGSTOP is received, with the idea that the system call should be retried.  I can’t see, from either zstdcat or zfs, if this is happening, though.
So I thought, “how about having Filespooler manually copy data from bash to zfs receive in a read/write loop instead of having them connected directly via a pipe?”  That is, there would be two pipes going there: one where Filespooler reads from the bash command, and one where it writes to zfs.  If nothing else, I could instrument it with debugging.
And so I did, and I found that when it deadlocked, it was deadlocking on write — but with no discernible pattern as to where or when.  So I went back to directly connected.
In analyzing straces, I found a Rust bug which I reported in which it is failing to close the read end of a pipe in the parent post-fork.  However, having implemented a workaround for this, it doesn’t prevent the deadlock so this is orthogonal to the issue at hand.
Among the two strange things here are things returning to normal when I attach strace to zstdcat, and things crashing when I attach strace to zfs.  I decided to investigate the latter.
It turns out that the ZFS code that is reading from stdin during zfs receive is in the kernel module, not userland.  Here is the part that is triggering the “imcomplete stream” error:
                int err = zfs_file_read(fp, (char *)buf + done,
                    len - done, &resid);
                if (resid == len - done) {
                        /*
                         * Note: ECKSUM or ZFS_ERR_STREAM_TRUNCATED indicates
                         * that the receive was interrupted and can
                         * potentially be resumed.
                         */
                        err = SET_ERROR(ZFS_ERR_STREAM_TRUNCATED);
                }

resid is an output parameter with the number of bytes remaining from a short read, so in this case, if the read produced zero bytes, then it sets that error.  What’s zfs_file_read then?
It boils down to a thin wrapper around kernel_read().  This winds up calling __kernel_read(), which calls read_iter on the pipe, which is pipe_read().  That’s where I don’t have the knowledge to get into the weeds right now.
So it seems likely to me that the problem has something to do with zfs receive.  But, what, and why does it only not work in this one very specific situation, and only so rarely?  And why does attaching strace to zstdcat make it all work again?  I’m indeed puzzled!
Update 2022-06-20: See the followup post which identifies this as likely a kernel bug and explains why this particular use of Filespooler made it easier to trigger.


        19 June, 2022 03:46AM

         by John Goerzen







    June 18, 2022










      Bastian Venthur






        blag is now available in Debian


        Last year, I wrote my own blog-aware static site generator in
Python. I called it “blag” – named after the blag of the
webcomic xkcd. Now I finally got around packaging- and uploading blag to
Debian. It passed the NEW queue and is now part of the distribution. That
means if you’re using Debian, you can install it via:
sudo aptitude install blag


Ubuntu will probably follow soon. For every other system, blag is also
available on PyPI:
pip install blag


To get started, you can
mkdir blog && cd blog
blag quickstart                        # fill out some info
nvim content/hello-world.md            # write some content
blag build                             # build the website


Blag is aware of articles and pages: the difference is that articles
are part of the blog and will be added to the atom feed, the archive and
aggregated in the tag pages. Pages are just rendered out to HTML. Articles and
pages can be freely mixed in the content directory, what differentiates an
article from a page is the existence of the dade metadata element:
title: My first article
description: Short description of the article
date: 2022-06-18 23:00
tags: blogging, markdown


## Hello World!

Lorem ipsum.

[...]


blag also comes with a dev-server that rebuilds the website automatically on
every change detected, you can start it using:
blag serve


The default theme looks quite ugly, and you probably want to create your own
styling to make it more beautiful. The process is not very difficult if you’re
familiar with jinja templating. Help on that can be found in the
“Templating” section of the online documentation, the offline
version in the blag-doc package, or the man page, respectively.
Speaking of the blag-doc package: packaging it was surprisingly tricky, and
it also took me a lot of trial and error to realize that dh_sphinxdocs alone
does not automatically put the generated html output into the appropriate
package, you rather have to list them in the package.docs-file (i.e.
blag-doc.docs) so dh_installdocs can install them properly.


        18 June, 2022 04:00PM

         by Bastian Venthur







    June 17, 2022






      Antoine Beaupré






        Matrix notes


        I have some concerns about Matrix (the protocol, not the movie that
came out recently, although I do have concerns about that as
well). I've been watching the project for a long time, and it seems
more a promising alternative to many protocols like IRC, XMPP, and
Signal.

This review may sound a bit negative, because it focuses on those
concerns. I am the operator of an IRC network and people keep asking
me to bridge it with Matrix. I have myself considered just giving up
on IRC and converting to Matrix. This space is a living document
exploring my research of that problem space. The TL;DR: is that no,
I'm not setting up a bridge just yet, and I'm still on IRC.

This article was written over the course of the last three months, but
I have been watching the Matrix project for years (my logs seem to say
2016 at least). The article is rather long. It will likely take you
half an hour to read, so copy this over to your ebook reader,
your tablet, or dead trees, and lean back and relax as I show you
around the Matrix. Or, alternatively, just jump to a section that
interest you, most likely the conclusion.

Introduction to Matrix

Matrix is an "open standard for interoperable, decentralised,
real-time communication over IP. It can be used to power Instant
Messaging, VoIP/WebRTC signalling, Internet of Things communication -
or anywhere you need a standard HTTP API for publishing and
subscribing to data whilst tracking the conversation history".

It's also (when compared with XMPP) "an eventually consistent
global JSON database with an HTTP API and pubsub semantics - whilst
XMPP can be thought of as a message passing protocol."

According to their FAQ, the project started in 2014, has about
20,000 servers, and millions of users. Matrix works over HTTPS but
over a special port: 8448.

Security and privacy

I have some concerns about the security promises of Matrix. It's
advertised as a "secure" with "E2E [end-to-end] encryption", but how
does it actually work?

Data retention defaults

One of my main concerns with Matrix is data retention, which is a key
part of security in a threat model where (for example) an hostile
state actor wants to surveil your communications and can seize your
devices.

On IRC, servers don't actually keep messages all that long: they pass
them along to other servers and clients as fast as they can, only keep
them in memory, and move on to the next message. There are no concerns
about data retention on messages (and their metadata) other than the
network layer. (I'm ignoring the issues with user registration, which
is a separate, if valid, concern.)  Obviously, an hostile server
could log everything passing through it, but IRC federations are
normally tightly controlled. So, if you trust your IRC operators, you
should be fairly safe. Obviously, clients can (and often do, even if
OTR is configured!) log all messages, but this is generally not
the default. Irssi, for example, does not log by
default. IRC bouncers are more likely to log to disk, of course,
to be able to do what they do.

Compare this to Matrix: when you send a message to a Matrix
homeserver, that server first stores it in its internal SQL
database. Then it will transmit that message to all clients connected
to that server and room, and to all other servers that have clients
connected to that room. Those remote servers, in turn, will keep a
copy of that message and all its metadata in their own database, by
default forever. On encrypted rooms those messages are encrypted, but
not their metadata.

There is a mechanism to expire entries in Synapse, but it is not
enabled by default. So one should generally assume that a message
sent on Matrix is never expired.

GDPR in the federation

But even if that setting was enabled by default, how do you control
it? This is a fundamental problem of the federation: if any user is
allowed to join a room (which is the default), those user's servers
will log all content and metadata from that room. That includes
private, one-on-one conversations, since those are essentially rooms
as well.

In the context of the GDPR, this is really tricky: who is the
responsible party (known as the "data controller") here? It's
basically any yahoo who fires up a home server and joins a room.

In a federated network, one has to wonder whether GDPR enforcement is
even possible at all. But in Matrix in particular, if you want to
enforce your right to be forgotten in a given room, you would have to:


enumerate all the users that ever joined the room while you were
there
discover all their home servers
start a GDPR procedure against all those servers



I recognize this is a hard problem to solve while still keeping an
open ecosystem. But I believe that Matrix should have much stricter
defaults towards data retention than right now. Message expiry should
be enforced by default, for example. (Note that there are also
redaction policies that could be used to implement part of the GDPR
automatically, see the privacy policy discussion below on that.)

Also keep in mind that, in the brave new peer-to-peer world that
Matrix is heading towards, the boundary between server and client is
likely to be fuzzier, which would make applying the GDPR even more difficult.

Update: this comment links to this post (in german) which
apparently studied the question and concluded that Matrix is not
GDPR-compliant.

In fact, maybe Synapse should be designed so that there's no
configurable flag to turn off data retention. A bit like how most
system loggers in UNIX (e.g. syslog) come with a log retention system
that typically rotate logs after a few weeks or month. Historically,
this was designed to keep hard drives from filling up, but it also has
the added benefit of limiting the amount of personal information kept
on disk in this modern day. (Arguably, syslog doesn't rotate logs on
its own, but, say, Debian GNU/Linux, as an installed system, does have
log retention policies well defined for installed packages, and those
can be discussed. And "no expiry" is definitely a bug.

Matrix.org privacy policy

When I first looked at Matrix, five years ago, Element.io was called
Riot.im and had a rather dubious privacy policy:

We currently use cookies to support our use of Google Analytics on
the Website and Service. Google Analytics collects information about
how you use the Website and Service.

[...]

This helps us to provide you with a good experience when you
browse our Website and use our Service and also allows us to improve
our Website and our Service.

When I asked Matrix people about why they were using Google Analytics,
they explained this was for development purposes and they were aiming
for velocity at the time, not privacy (paraphrasing here).

They also included a "free to snitch" clause:

If we are or believe that we are under a duty to disclose or share
your personal data, we will do so in order to comply with any legal
obligation, the instructions or requests of a governmental authority
or regulator, including those outside of the UK.

Those are really broad terms, above and beyond what is typically
expected legally.

Like the current retention policies, such user tracking and
... "liberal" collaboration practices with the state set a bad
precedent for other home servers.

Thankfully, since the above policy was published (2017), the GDPR was
"implemented" (2018) and it seems like both the Element.io
privacy policy and the Matrix.org privacy policy have been
somewhat improved since.

Notable points of the new privacy policies:


2.3.1.1: the "federation" section actually outlines that
"Federated homeservers and Matrix clients which respect the Matrix
protocol are expected to honour these controls and
redaction/erasure requests, but other federated homeservers are
outside of the span of control of Element, and we cannot guarantee
how this data will be processed"
2.6: users under the age of 16 should not use the
matrix.org service
2.10: Upcloud, Mythic Beast, Amazon, and CloudFlare possibly
have access to your data (it's nice to at least mention this in the
privacy policy: many providers don't even bother admitting to this
kind of delegation)
Element 2.2.1: mentions many more third parties (Twilio,
Stripe, Quaderno, LinkedIn, Twitter, Google, Outplay,
PipeDrive, HubSpot, Posthog, Sentry, and Matomo
(phew!) used when you are paying Matrix.org for hosting



I'm not super happy with all the trackers they have on the Element
platform, but then again you don't have to use that service. Your
favorite homeserver (assuming you are not on Matrix.org) probably has
their own Element deployment, hopefully without all that garbage.

Overall, this is all a huge improvement over the previous privacy
policy, so hats off to the Matrix people for figuring out a reasonable
policy in such a tricky context. I particularly like this bit:

We will forget your copy of your data upon your request. We will
also forward your request to be forgotten onto federated
homeservers. However - these homeservers are outside our span of
control, so we cannot guarantee they will forget your data.

It's great they implemented those mechanisms and, after all, if
there's an hostile party in there, nothing can prevent them from using
screenshots to just exfiltrate your data away from the client side
anyways, even with services typically seen as more secure, like
Signal.

As an aside, I also appreciate that Matrix.org has a fairly decent
code of conduct, based on the TODO CoC which checks all the
boxes in the geekfeminism wiki.

Metadata handling

Overall, privacy protections in Matrix mostly concern message
contents, not metadata. In other words, who's talking with who, when
and from where is not well protected. Compared to a tool like Signal,
which goes through great lengths to anonymize that data with features
like private contact discovery, disappearing messages,
sealed senders, and private groups, Matrix is definitely
behind. (Note: there is an issue open about message lifetimes in
Element since 2020, but it's not at even at the MSC stage yet.)

This is a known issue (opened in 2019) in Synapse, but this is
not just an implementation issue, it's a flaw in the protocol
itself. Home servers keep join/leave of all rooms, which gives clear
text information about who is talking to. Synapse logs may also
contain privately identifiable information that home server admins
might not be aware of in the first place. Those log rotation policies
are separate from the server-level retention policy, which may be
confusing for a novice sysadmin.

Combine this with the federation: even if you trust your home server
to do the right thing, the second you join a public room with
third-party home servers, those ideas kind of get thrown out because
those servers can do whatever they want with that information. Again,
a problem that is hard to solve in any federation.

To be fair, IRC doesn't have a great story here either: any client
knows not only who's talking to who in a room, but also typically
their client IP address. Servers can (and often do) obfuscate
this, but often that obfuscation is trivial to reverse. Some servers
do provide "cloaks" (sometimes automatically), but that's kind of a
"slap-on" solution that actually moves the problem elsewhere: now the
server knows a little more about the user.

Overall, I would worry much more about a Matrix home server seizure
than a IRC or Signal server seizure. Signal does get subpoenas,
and they can only give out a tiny bit of information about their
users: their phone number, and their registration, and last connection
date. Matrix carries a lot more information in its database.

Amplification attacks on URL previews

I (still!) run an Icecast server and sometimes share links to it
on IRC which, obviously, also ends up on (more than one!) Matrix home
servers because some people connect to IRC using Matrix. This, in
turn, means that Matrix will connect to that URL to generate a link
preview.

I feel this outlines a security issue, especially because those
sockets would be kept open seemingly forever. I tried to warn the
Matrix security team but somehow, I don't think this issue was taken
very seriously. Here's the disclosure timeline:


January 18: contacted Matrix security
January 19: response: already reported as a bug
January 20: response: can't reproduce
January 31: timeout added, considered solved
January 31: I respond that I believe the security issue is
underestimated, ask for clearance to disclose
February 1: response: asking for two weeks delay after the next
release (1.53.0) including another patch, presumably in two
weeks' time
February 22: Matrix 1.53.0 released
April 14: I notice the release, ask for clearance again
April 14: response: referred to the public disclosure



There are a couple of problems here:


the bug was publicly disclosed in September 2020, and not
considered a security issue until I notified them, and even then,
I had to insist
no clear disclosure policy timeline was proposed or seems
established in the project (there is a security disclosure
policy but it doesn't include any predefined timeline)
I wasn't informed of the disclosure
the actual solution is a size limit (10MB, already implemented), a
time limit (30 seconds, implemented in PR 11784), and a
content type allow list (HTML, "media" or JSON, implemented in PR
11936), and I'm not sure it's adequate
(pure vanity:) I did not make it to their Hall of fame



I'm not sure those solutions are adequate because they all seem to
assume a single home server will pull that one URL for a little while
then stop. But in a federated network, many (possibly thousands)
home servers may be connected in a single room at once. If an attacker
drops a link into such a room, all those servers would connect to
that link all at once. This is an amplification attack: a small
amount of traffic will generate a lot more traffic to a single
target. It doesn't matter there are size or time limits: the
amplification is what matters here.

It should also be noted that clients that generate link previews
have more amplification because they are more numerous than
servers. And of course, the default Matrix client (Element) does
generate link previews as well.

That said, this is possibly not a problem specific to Matrix: any
federated service that generates link previews may suffer from this.

I'm honestly not sure what the solution is here. Maybe moderation?
Maybe link previews are just evil? All I know is there was this weird
bug in my Icecast server and I tried to ring the bell about it, and it
feels it was swept under the rug. Somehow I feel this is bound to blow
up again in the future, even with the current mitigation.

Moderation

In Matrix like elsewhere, Moderation is a hard problem. There is a
detailed moderation guide and much of this problem space is
actively worked on in Matrix right now. A fundamental problem with
moderating a federated space is that a user banned from a room can
rejoin the room from another server. This is why spam is such a
problem in Email, and why IRC networks have stopped federating ages
ago (see the IRC history for that fascinating story).

The mjolnir bot

The mjolnir moderation bot is designed to help with some of those
things. It can kick and ban users, redact all of a user's message (as
opposed to one by one), all of this across multiple rooms. It can also
subscribe to a federated block list published by matrix.org to block
known abusers (users or servers). Bans are pretty flexible and
can operate at the user, room, or server level.

Matrix people suggest making the bot admin of your channels, because
you can't take back admin from a user once given.

The command-line tool

There's also a new command line tool designed to do things like:


System notify users (all users/users from a list, specific user)
delete sessions/devices not seen for X days
purge the remote media cache
select rooms with various criteria (external/local/empty/created by/encrypted/cleartext)
purge history of theses rooms
shutdown rooms



This tool and Mjolnir are based on the admin API built into
Synapse.

Rate limiting

Synapse has pretty good built-in rate-limiting which blocks
repeated login, registration, joining, or messaging attempts. It may
also end up throttling servers on the federation based on those
settings.

Fundamental federation problems

Because users joining a room may come from another server, room
moderators are at the mercy of the registration and moderation
policies of those servers. Matrix is like IRC's +R mode ("only
registered users can join") by default, except that anyone can
register their own homeserver, which makes this limited.

Server admins can block IP addresses and home servers, but those tools
are not easily available to room admins. There is an API
(m.room.server_acl in /devtools) but it is not reliable
(thanks Austin Huang for the clarification).

Matrix has the concept of guest accounts, but it is not used very
much, and virtually no client or homeserver supports it. This contrasts with the way
IRC works: by default, anyone can join an IRC network even without
authentication. Some channels require registration, but in general you
are free to join and look around (until you get blocked, of course).

I have seen anecdotal evidence (CW: Twitter, nitter link) that "moderating bridges is hell", and
I can imagine why. Moderation is already hard enough on one
federation, when you bridge a room with another network, you inherit
all the problems from that network but without the entire abuse
control tools from the original network's API...

Room admins

Matrix, in particular, has the problem that room administrators (which
have the power to redact messages, ban users, and promote other users)
are bound to their Matrix ID which is, in turn, bound to their home
servers. This implies that a home server administrators could (1)
impersonate a given user and (2) use that to hijack the room. So in
practice, the home server is the trust anchor for rooms, not the user
themselves.

That said, if server B administrator hijack user joe on server B,
they will hijack that room on that specific server. This will not
(necessarily) affect users on the other servers, as servers could
refuse parts of the updates or ban the compromised account (or
server).

It does seem like a major flaw that room credentials are bound to
Matrix identifiers, as opposed to the E2E encryption credentials.  In
an encrypted room even with fully verified members, a compromised or
hostile home server can still take over the room by impersonating an
admin. That admin (or even a newly minted user) can then send events
or listen on the conversations.

This is even more frustrating when you consider that Matrix events are
actually signed and therefore have some authentication attached
to them, acting like some sort of Merkle tree (as it contains a link
to previous events). That signature, however, is made from the
homeserver PKI keys, not the client's E2E keys, which makes E2E feel
like it has been "bolted on" later.

Availability

While Matrix has a strong advantage over Signal in that it's
decentralized (so anyone can run their own homeserver,), I couldn't
find an easy way to run a "multi-primary" setup, or even a "redundant"
setup (even if with a single primary backend), short of going full-on
"replicate PostgreSQL and Redis data", which is not typically for the
faint of heart.

How this works in IRC

On IRC, it's quite easy to setup redundant nodes. All you need is:


a new machine (with it's own public address with an open port)
a shared secret (or certificate) between that machine and an
existing one on the network
a connect {} block on both servers



That's it: the node will join the network and people can connect to it
as usual and share the same user/namespace as the rest of the
network. The servers take care of synchronizing state: you do not need
to worry about replicating a database server.

(Now, experienced IRC people will know there's a catch here: IRC
doesn't have authentication built in, and relies on "services" which
are basically bots that authenticate users (I'm simplifying, don't
nitpick). If that service goes down, the network still works, but
then people can't authenticate, and they can start doing nasty things
like steal people's identity if they get knocked offline. But still:
basic functionality still works: you can talk in rooms and with users
that are on the reachable network.)

User identities

Matrix is more complicated. Each "home server" has its own identity
namespace: a specific user (say @anarcat:matrix.org) is bound to
that specific home server. If that server goes down, that user is
completely disconnected. They could register a new account elsewhere
and reconnect, but then they basically lose all their configuration:
contacts, joined channels are all lost.

(Also notice how the Matrix IDs don't look like a typical user address
like an email in XMPP. They at least did their homework and got the
allocation for the scheme.)

Rooms

Users talk to each other in "rooms", even in one-to-one
communications. (Rooms are also used for other things like "spaces",
they're basically used for everything, think "everything is a file"
kind of tool.) For rooms, home servers act more like IRC nodes in that
they keep a local state of the chat room and synchronize it with other
servers. Users can keep talking inside a room if the server that
originally hosts the room goes down. Rooms can have a local,
server-specific "alias" so that, say, #room:matrix.org is also
visible as #room:example.com on the example.com home server. Both
addresses refer to the same room underlying room.

(Finding this in the Element settings is not obvious though, because
that "alias" are actually called a "local address" there. So to create
such an alias (in Element), you need to go in the room settings'
"General" section, "Show more" in "Local address", then add the alias
name (e.g. foo), and then that room will be available on your
example.com homeserver as #foo:example.com.)

So a room doesn't belong to a server, it belongs to the federation,
and anyone can join the room from any serer (if the room is public, or
if invited otherwise). You can create a room on server A and when a
user from server B joins, the room will be replicated on server B as
well. If server A fails, server B will keep relaying traffic to
connected users and servers.

A room is therefore not fundamentally addressed with the above alias,
instead ,it has a internal Matrix ID, which basically a random
string. It has a server name attached to it, but that was made just to
avoid collisions. That can get a little confusing. For example, the
#fractal:gnome.org room is an alias on the gnome.org server, but
the room ID is !hwiGbsdSTZIwSRfybq:matrix.org. That's because the
room was created on matrix.org, but the preferred branding is
gnome.org now.

As an aside, rooms, by default, live forever, even after the last user
quits. There's an admin API to delete rooms and a tombstone
event to redirect to another one, but neither have a GUI yet. The
latter is part of MSC1501 ("Room version upgrades") which allows
a room admin to close a room, with a message and a pointer to another
room.

Spaces

Discovering rooms can be tricky: there is a per-server room
directory, but Matrix.org people are trying to deprecate it in favor
of "Spaces". Room directories were ripe for abuse: anyone can create a
room, so anyone can show up in there. It's possible to restrict who
can add aliases, but anyways directories were seen as too limited.

In contrast, a "Space" is basically a room that's an index of other
rooms (including other spaces), so existing moderation and
administration mechanism that work in rooms can (somewhat) work in
spaces as well. This enables a room directory that works across
federation, regardless on which server they were originally created.

New users can be added to a space or room automatically in
Synapse. (Existing users can be told about the space with a server
notice.) This gives admins a way to pre-populate a list of rooms on a
server, which is useful to build clusters of related home servers,
providing some sort of redundancy, at the room -- not user -- level.

Home servers

So while you can workaround a home server going down at the room
level, there's no such thing at the home server level, for user
identities. So if you want those identities to be stable in the long
term, you need to think about high availability. One limitation is
that the domain name (e.g. matrix.example.com) must never change in
the future, as renaming home servers is not supported.

The documentation used to say you could "run a hot spare" but that has
been removed. Last I heard, it was not possible to run a
high-availability setup where multiple, separate locations could
replace each other automatically. You can have high performance
setups where the load gets distributed among workers, but those
are based on a shared database (Redis and PostgreSQL) backend.

So my guess is it would be possible to create a "warm" spare server of
a matrix home server with regular PostgreSQL replication, but
that is not documented in the Synapse manual. This sort of setup
would also not be useful to deal with networking issues or denial of
service attacks, as you will not be able to spread the load over
multiple network locations easily. Redis and PostgreSQL heroes are
welcome to provide their multi-primary solution in the comments. In
the meantime, I'll just point out this is a solution that's handled
somewhat more gracefully in IRC, by having the possibility of
delegating the authentication layer.

Delegations

If you do not want to run a Matrix server yourself, it's possible to
delegate the entire thing to another server. There's a server
discovery API which uses the .well-known pattern (or SRV
records, but that's "not recommended" and a bit confusing) to
delegate that service to another server. Be warned that the server
still needs to be explicitly configured for your domain. You can't
just put:

{ "m.server": "matrix.org:443" }


... on https://example.com/.well-known/matrix/server and start using
@you:example.com as a Matrix ID. That's because Matrix doesn't
support "virtual hosting" and you'd still be connecting to rooms and
people with your matrix.org identity, not example.com as you would
normally expect. This is also why you cannot rename your home
server.

The server discovery API is what allows servers to find each
other. Clients, on the other hand, use the client-server discovery
API: this is what allows a given client to find your home server
when you type your Matrix ID on login.

Performance

The high availability discussion brushed over the performance of
Matrix itself, but let's now dig into that.

Horizontal scalability

There were serious scalability issues of the main Matrix server,
Synapse, in the past. So the Matrix team has been working hard to
improve its design. Since Synapse 1.22 the home server can
horizontally scale to multiple workers (see this blog post for details)
which can make it easier to scale large servers.

Other implementations

There are other promising home servers implementations from a
performance standpoint (dendrite, Golang, entered beta in late
2020; conduit, Rust, beta; others), but none of those
are feature-complete so there's a trade-off to be made there.  Synapse
is also adding a lot of feature fast, so it's an open question whether
the others will ever catch up. (I have heard that Dendrite might
actually surpass Synapse in features within a few years, which would
put Synapse in a more "LTS" situation.)

Latency

Matrix can feel slow sometimes. For example, joining the "Matrix HQ"
room in Element (from matrix.debian.social) takes a few minutes
and then fails. That is because the home server has to sync the
entire room state when you join the room. There was promising work on
this announced in the lengthy 2021 retrospective, and some of
that work landed (partial sync) in the 1.53 release already.
Other improvements coming include sliding sync, lazy loading
over federation, and fast room joins. So that's actually
something that could be fixed in the fairly short term.

But in general, communication in Matrix doesn't feel as "snappy" as on
IRC or even Signal. It's hard to quantify this without instrumenting a
full latency test bed (for example the tools I used in the terminal
emulators latency tests), but
even just typing in a web browser feels slower than typing in a xterm
or Emacs for me.

Even in conversations, I "feel" people don't immediately respond as
fast. In fact, this could be an interesting double-blind experiment to
make: have people guess whether they are talking to a person on
Matrix, XMPP, or IRC, for example. My theory would be that people
could notice that Matrix users are slower, if only because of the TCP
round-trip time each message has to take.

Transport

Some courageous person actually made some tests of various
messaging platforms on a congested network. His evaluation was
basically:


Briar: uses Tor, so unusable except locally
Matrix: "struggled to send and receive messages", joining a room
takes forever as it has to sync all history, "took 20-30 seconds
for my messages to be sent and another 20 seconds for further
responses"
XMPP: "worked in real-time, full encryption, with nearly zero
lag"



So that was interesting. I suspect IRC would have also fared better,
but that's just a feeling.

Other improvements to the transport layer include support for
websocket and the CoAP proxy work from 2019 (targeting
100bps links), but both seem stalled at the time of writing. The
Matrix people have also announced the pinecone p2p overlay
network which aims at solving large, internet-scale routing
problems. See also this talk at FOSDEM 2022.

Usability

Onboarding and workflow

The workflow for joining a room, when you use Element web, is not
great:


click on a link in a web browser
land on (say) https://matrix.to/#/#matrix-dev:matrix.org
offers "Element", yeah that's sounds great, let's click "Continue"
land on
https://app.element.io/#/room%2F%23matrix-dev%3Amatrix.org and
then you need to register, aaargh



As you might have guessed by now, there is a specification to
solve this, but web browsers need to adopt it as well, so that's far
from actually being solved. At least browsers generally know about the
matrix: scheme, it's just not exactly clear what they should do with
it, especially when the handler is just another web page (e.g. Element
web).

In general, when compared with tools like Signal or WhatsApp, Matrix
doesn't fare so well in terms of user discovery. I probably have some
of my normal contacts that have a Matrix account as well, but there's
really no way to know. It's kind of creepy when Signal tells you
"this person is on Signal!" but it's also pretty cool that it works,
and they actually implemented it pretty well.

Registration is also less obvious: in Signal, the app confirms your
phone number automatically. It's friction-less and quick. In Matrix,
you need to learn about home servers, pick one, register (with a
password! aargh!), and then setup encryption keys (not default),
etc. It's a lot more friction.

And look, I understand: giving away your phone number is a huge
trade-off. I don't like it either. But it solves a real problem and
makes encryption accessible to a ton more people. Matrix does have
"identity servers" that can serve that purpose, but I don't feel
confident sharing my phone number there. It doesn't help that the
identity servers don't have private contact discovery: giving them
your phone number is a more serious security compromise than with
Signal.

There's a catch-22 here too: because no one feels like giving away
their phone numbers, no one does, and everyone assumes that stuff
doesn't work anyways. Like it or not, Signal forcing people to
divulge their phone number actually gives them critical mass that
means actually a lot of my relatives are on Signal and I don't have
to install crap like WhatsApp to talk with them.

5 minute clients evaluation

Throughout all my tests I evaluated a handful of Matrix clients,
mostly from Flathub because almost none of them are packaged in
Debian.

Right now I'm using Element, the flagship client from Matrix.org, in a
web browser window, with the PopUp Window extension. This makes
it look almost like a native app, and opens links in my main browser
window (instead of a new tab in that separate window), which is
nice. But I'm tired of buying memory to feed my web browser, so this
indirection has to stop. Furthermore, I'm often getting completely
logged off from Element, which means re-logging in, recovering my
security keys, and reconfiguring my settings. That is extremely
annoying.

Coming from Irssi, Element is really "GUI-y" (pronounced
"gooey"). Lots of clickety happening. To mark conversations as read,
in particular, I need to click-click-click on all the tabs that have
some activity. There's no "jump to latest message" or "mark all as
read" functionality as far as I could tell. In Irssi the former is
built-in (alt-a) and I made a custom /READ command for
the latter:

/ALIAS READ script exec \$_->activity(0) for Irssi::windows


And yes, that's a Perl script in my IRC client. I am not aware of any
Matrix client that does stuff like that, except maybe Weechat, if we
can call it a Matrix client, or Irssi itself, now that it has a
Matrix plugin (!).

As for other clients, I have looked through the Matrix Client
Matrix (confusing right?) to try to figure out which one to try,
and, even after selecting Linux as a filter, the chart is just too
wide to figure out anything. So I tried those, kind of randomly:


Fractal
Mirage
Nheko
Quaternion



Unfortunately, I lost my notes on those, I don't actually remember
which one did what. I still have a session open with Mirage, so I
guess that means it's the one I preferred, but I remember they were
also all very GUI-y.

Maybe I need to look at weechat-matrix or gomuks. At least Weechat
is scriptable so I could continue playing the power-user. Right now my
strategy with messaging (and that includes microblogging like Twitter
or Mastodon) is that everything goes through my IRC client, so Weechat
could actually fit well in there. Going with gomuks, on the other
hand, would mean running it in parallel with Irssi or ... ditching
IRC, which is a leap I'm not quite ready to take just yet.

Oh, and basically none of those clients (except Nheko and Element)
support VoIP, which is still kind of a second-class citizen in
Matrix. It does not support large multimedia rooms, for example:
Jitsi was used for FOSDEM instead of the native videoconferencing
system.

Bots

This falls a little aside the "usability" section, but I didn't know
where to put this... There's a few Matrix bots out there, and you are
likely going to be able to replace your existing bots with Matrix
bots. It's true that IRC has a long and impressive history with lots
of various bots doing various things, but given how young Matrix is,
there's still a good variety:


maubot: generic bot with tons of usual plugins like sed, dice,
karma, xkcd, echo, rss, reminder, translate, react, exec,
gitlab/github webhook receivers, weather, etc
opsdroid: framework to implement "chat ops" in Matrix,
connects with Matrix, GitHub, GitLab, Shell commands, Slack, etc
matrix-nio: another framework, used to build lots more
bots like:


hemppa: generic bot with various functionality like weather,
RSS feeds, calendars, cron jobs, OpenStreetmaps lookups, URL
title snarfing, wolfram alpha, astronomy pic of the day, Mastodon
bridge, room bridging, oh dear
devops: ping, curl, etc
podbot: play podcast episodes from AntennaPod
cody: Python, Ruby, Javascript REPL
eno: generic bot, "personal assistant"


mjolnir: moderation bot
hookshot: bridge with GitLab/GitHub
matrix-monitor-bot: latency monitor



One thing I haven't found an equivalent for is Debian's
MeetBot. There's an archive bot but it doesn't have topics
or a meeting chair, or HTML logs.

Working on Matrix

As a developer, I find Matrix kind of intimidating. The specification
is huge. The official specification itself looks somewhat
digestable: it's only 6 APIs so that looks, at first, kind of
reasonable. But whenever you start asking complicated questions about
Matrix, you quickly fall into the Matrix Spec Change
specification (which, yes, is a separate specification). And there are
literally hundreds of MSCs flying around. It's hard to tell
what's been adopted and what hasn't, and even harder to figure out if
your specific client has implemented it.

(One trendy answer to this problem is to "rewrite it in rust": Matrix
are working on implementing a lot of those specifications in a
matrix-rust-sdk that's designed to take the implementation
details away from users.)

Just taking the latest weekly Matrix report, you find that
three new MSCs proposed, just last week! There's even a graph that
shows the number of MSCs is progressing steadily, at 600+ proposals
total, with the majority (300+) "new". I would guess the "merged" ones
are at about 150.

That's a lot of text which includes stuff like 3D worlds which,
frankly, I don't think you should be working on when you have such
important security and usability problems. (The internet as a whole,
arguably, doesn't fare much better. RFC600 is a really obscure
discussion about "INTERFACING AN ILLINOIS PLASMA TERMINAL TO THE
ARPANET". Maybe that's how many MSCs will end up as well, left
forgotten in the pits of history.)

And that's the thing: maybe the Matrix people have a different
objective than I have. They want to connect everything to everything,
and make Matrix a generic transport for all sorts of applications,
including virtual reality, collaborative editors, and so on.

I just want secure, simple messaging. Possibly with good file
transfers, and video calls. That it works with existing stuff is good,
and it should be federated to remove the "Signal point of
failure". So I'm a bit worried with the direction all those MSCs are
taking, especially when you consider that clients other than Element
are still struggling to keep up with basic features like end-to-end
encryption or room discovery, never mind voice or spaces...

Conclusion

Overall, Matrix is somehow in the space XMPP was a few years ago. It
has a ton of features, pretty good clients, and a large
community. It seems to have gained some of the momentum that XMPP has
lost. It may have the most potential to replace Signal if something
bad would happen to it (like, I don't know, getting banned or
going nuts with cryptocurrency)...

But it's really not there yet, and I don't see Matrix trying to get
there either, which is a bit worrisome.

Looking back at history

I'm also worried that we are repeating the errors of the past. The
history of federated services is really fascinating:. IRC, FTP, HTTP,
and SMTP were all created in the early days of the internet, and are
all still around (except, arguably, FTP, which was removed from major
browsers recently). All of them had to face serious challenges in
growing their federation.

IRC had numerous conflicts and forks, both at the technical level
but also at the political level. The history of IRC is really
something that anyone working on a federated system should study in
detail, because they are bound to make the same mistakes if they are
not familiar with it. The "short" version is:


1988: Finnish researcher publishes first IRC source code
1989: 40 servers worldwide, mostly universities
1990: EFnet ("eris-free network") fork which blocks the "open
relay", named Eris - followers of Eris form the A-net, which
promptly dissolves itself, with only EFnet remaining
1992: Undernet fork, which offered authentication ("services"),
routing improvements and timestamp-based channel synchronisation
1994: DALnet fork, from Undernet, again on a technical disagreement
1995: Freenode founded
1996: IRCnet forks from EFnet, following a flame war of historical
proportion, splitting the network between Europe and the Americas
1997: Quakenet founded
1999: (XMPP founded)
2001: 6 million users, OFTC founded
2002: DALnet peaks at 136,000 users
2003: IRC as a whole peaks at 10 million users, EFnet peaks at
141,000 users
2004: (Facebook founded), Undernet peaks at 159,000 users
2005: Quakenet peaks at 242,000 users, IRCnet peaks at 136,000
(Youtube founded)
2006: (Twitter founded)
2009: (WhatsApp, Pinterest founded)
2010: (TextSecure AKA Signal, Instagram founded)
2011: (Snapchat founded)
~2013: Freenode peaks at ~100,000 users
2016: IRCv3 standardisation effort started (TikTok founded)
2021: Freenode self-destructs, Libera chat founded
2022: Libera peaks at 50,000 users, OFTC peaks at 30,000 users



(The numbers were taken from the Wikipedia page and
Netsplit.de. Note that I also include other networks launch in
parenthesis for context.)

Pretty dramatic, don't you think? Eventually, somehow, IRC became
irrelevant for most people: few people are even aware of it now. With
less than a million users active, it's smaller than Mastodon, XMPP, or
Matrix at this point.1 If I were to venture a guess, I'd say that
infighting, lack of a standardization body, and a somewhat annoying
protocol meant the network could not grow. It's also possible that the
decentralised yet centralised structure of IRC networks limited their
reliability and growth.

But large social media companies have also taken over the space:
observe how IRC numbers peak around the time the wave of large social
media companies emerge, especially Facebook (2.9B users!!) and Twitter
(400M users).

Where the federated services are in history

Right now, Matrix, and Mastodon (and email!) are at the
"pre-EFnet" stage: anyone can join the federation. Mastodon has
started working on a global block list of fascist servers which is
interesting, but it's still an open federation. Right now, Matrix is
totally open, but matrix.org publishes a (federated) block list
of hostile servers (#matrix-org-coc-bl:matrix.org, yes, of course
it's a room).

Interestingly, Email is also in that stage, where there are block
lists of spammers, and it's a race between those blockers and
spammers. Large email providers, obviously, are getting closer to the
EFnet stage: you could consider they only accept email from themselves
or between themselves. It's getting increasingly hard to deliver mail
to Outlook and Gmail for example, partly because of bias against small
providers, but also because they are including more and more
machine-learning tools to sort through email and those systems are,
fundamentally, unknowable. It's not quite the same as splitting the
federation the way EFnet did, but the effect is similar.

HTTP has somehow managed to live in a parallel universe, as it's
technically still completely federated: anyone can start a web server
if they have a public IP address and anyone can connect to it. The
catch, of course, is how you find the darn thing. Which is how Google
became one of the most powerful corporations on earth, and how they
became the gatekeepers of human knowledge online.

I have only briefly mentioned XMPP here, and my XMPP fans will
undoubtedly comment on that, but I think it's somewhere in the middle
of all of this. It was co-opted by Facebook and Google, and
both corporations have abandoned it to its fate. I remember fondly the
days where I could do instant messaging with my contacts who had a
Gmail account. Those days are gone, and I don't talk to anyone over
Jabber anymore, unfortunately. And this is a threat that Matrix still
has to face.

It's also the threat Email is currently facing. On the one hand
corporations like Facebook want to completely destroy it and have
mostly succeeded: many people just have an email account to
register on things and talk to their friends over Instagram or
(lately) TikTok (which, I know, is not Facebook, but they started that
fire).

On the other hand, you have corporations like Microsoft and Google who
are still using and providing email services — because, frankly, you
still do need email for stuff, just like fax is still around —
but they are more and more isolated in their own silo. At this point,
it's only a matter of time they reach critical mass and just decide
that the risk of allowing external mail coming in is not worth the
cost. They'll simply flip the switch and work on an allow-list
principle. Then we'll have closed the loop and email will be
dead, just like IRC is "dead" now.

I wonder which path Matrix will take. Could it liberate us from these
vicious cycles?

Update: this generated some discussions on lobste.rs.




According to Wikipedia, there are currently about 500
  distinct IRC networks operating, on about 1,000 servers,
  serving over 250,000 users. In contrast, Mastodon seems to be
  around 5 million users, Matrix.org claimed at FOSDEM
  2021 to have about 28 million globally visible accounts,
  and Signal lays claim to over 40 million souls. XMPP claims
  to have "millions" of users on the xmpp.org homepage but
  the FAQ says they don't actually know. On the proprietary
  silo side of the fence, this page says


 Facebook: 2.9 billion users
 WhatsApp: 2B
 Instagram: 1.4B
 TikTok: 1B
 Snapchat: 500M
 Pinterest: 480M
 Twitter: 397M



  Notable omission from that list: Youtube, with its mind-boggling
  2.6 billion users...

  Those are not the kind of numbers you just "need to convince a
  brother or sister" to grow the network...↩




        17 June, 2022 03:34PM







    June 16, 2022






      Dima Kogan






        Ricoh GR IIIx 802.11 reverse engineering



I just got a fancy new camera: Ricoh GR IIIx. It's pretty great, and I strongly
recommend it to anyone that wants a truly pocketable camera with fantastic image
quality and full manual controls. One annoyance is the connectivity. It does
have both Bluetooth and 802.11, but the only official method of using them is
some dinky closed phone app. This is silly. I just did some reverse-engineering,
and I now have a functional shell script to download the last few images via
802.11. This is more convenient than plugging in a wire or pulling out the
memory card. Fortunately, Ricoh didn't bend over backwards to make the reversing
difficult, so to figure it out I didn't even need to download the phone app, and
sniff the traffic.



When you turn on the 802.11 on the camera, it says stuff about essid and
password, so clearly the camera runs its own access point. Not ideal, but it's
good-enough. I connected, and ran nmap to find hosts and open ports: only port
80 on 192.168.0.1 is open. Pointing curl at it yields some error, so I need to
figure out the valid endpoints. I downloaded the firmware binary, and tried to
figure out what's in it:


dima@shorty:/tmp$ binwalk fwdc243b.bin

DECIMAL       HEXADECIMAL     DESCRIPTION
--------------------------------------------------------------------------------
3036150       0x2E53F6        Cisco IOS microcode, for "8"
3164652       0x3049EC        Certificate in DER format (x509 v3), header length: 4, sequence length: 5412
5472143       0x537F8F        Copyright string: "Copyright ("
6128763       0x5D847B        PARity archive data - file number 90
10711634      0xA37252        gzip compressed data, maximum compression, from Unix, last modified: 2022-02-15 05:47:23
13959724      0xD5022C        MySQL ISAM compressed data file Version 11
24829873      0x17ADFB1       MySQL MISAM compressed data file Version 4
24917663      0x17C369F       MySQL MISAM compressed data file Version 4
24918526      0x17C39FE       MySQL MISAM compressed data file Version 4
24921612      0x17C460C       MySQL MISAM compressed data file Version 4
24948153      0x17CADB9       MySQL MISAM compressed data file Version 4
25221672      0x180DA28       MySQL MISAM compressed data file Version 4
25784158      0x1896F5E       Cisco IOS microcode, for "\"
26173589      0x18F6095       MySQL MISAM compressed data file Version 4
28297588      0x1AFC974       MySQL ISAM compressed data file Version 6
28988307      0x1BA5393       MySQL ISAM compressed data file Version 3
28990184      0x1BA5AE8       MySQL MISAM index file Version 3
29118867      0x1BC5193       MySQL MISAM index file Version 3
29449193      0x1C15BE9       JPEG image data, JFIF standard 1.01
29522133      0x1C278D5       JPEG image data, JFIF standard 1.08
29522412      0x1C279EC       Copyright string: "Copyright ("
29632931      0x1C429A3       JPEG image data, JFIF standard 1.01
29724094      0x1C58DBE       JPEG image data, JFIF standard 1.01



The gzip chunk looks like what I want:


dima@shorty:/tmp$ tail -c+10711635 fwdc243b.bin> /tmp/tst.gz


dima@shorty:/tmp$ < /tmp/tst.gz gunzip | file -

/dev/stdin: ASCII cpio archive (SVR4 with no CRC)


dima@shorty:/tmp$ < /tmp/tst.gz gunzip > tst.cpio



OK, we have some .cpio thing. It's plain-text. I grep around it in, looking
for GET and POST and such, and I see various URI-looking things at
/v1/..... Grepping for that I see


dima@shorty:/tmp$ strings tst.cpio | grep /v1/

GET /v1/debug/revisions
GET /v1/ping
GET /v1/photos
GET /v1/props
PUT /v1/params/device
PUT /v1/params/lens
PUT /v1/params/camera
GET /v1/liveview
GET /v1/transfers
POST /v1/device/finish
POST /v1/device/wlan/finish
POST /v1/lens/focus
POST /v1/camera/shoot
POST /v1/camera/shoot/compose
POST /v1/camera/shoot/cancel
GET /v1/photos/{}/{}
GET /v1/photos/{}/{}/info
PUT /v1/photos/{}/{}/transfer
/v1/photos/<string>/<string>
/v1/photos/<string>/<string>/info
/v1/photos/<string>/<string>/transfer
/v1/device/finish
/v1/device/wlan/finish
/v1/lens/focus
/v1/camera/shoot
/v1/camera/shoot/compose
/v1/camera/shoot/cancel
/v1/changes
/v1/changes message received.
/v1/changes issue event.
/v1/changes new websocket connection.
/v1/changes websocket connection closed. reason({})
/v1/transfers, transferState({}), afterIndex({}), limit({})



Jackpot. I pointed curl at most of these, and they do interesting things.
Generally they all spit out JSON. /v1/liveview sends out a sequence of JPEG
images. The thing I care about is /v1/photos/DIRECTORY/FILE and
/v1/photos/DIRECTORY/FILE/info. The result is a script I just wrote to connect
to the camera, download N images, and connect back to the original access
point:



https://github.com/dkogan/ricoh-download



Kinda crude, but works for now. I'll improve it with time.



After I did this I found an old thread from 2015 where somebody was using an
apparently-compatible camera, and wrote a fancier tool:



https://www.pentaxforums.com/forums/184-pentax-k-s1-k-s2/295501-k-s2-wifi-laptop-2.html



        16 June, 2022 10:04PM

         by Dima Kogan


















      Dirk Eddelbuettel






        RcppArmadillo 0.11.2.0.0 on CRAN: New Upstream



Armadillo is a powerful and expressive C++ template library for linear algebra and scientific computing. It aims towards a good balance between speed and ease of use, has a syntax deliberately close to Matlab, and is useful for algorithm development directly in C++, or quick conversion of research code into production environments. RcppArmadillo integrates this library with the R environment and language–and is widely used by (currently) 991 other packages on CRAN, downloaded over 25 million times (per the partial logs from the cloud mirrors of CRAN), and the CSDA paper (preprint / vignette) by Conrad and myself has been cited 476 times according to Google Scholar.
This release brings a second upstream fix by Conrad in the release series 11.*. We once again tested this very rigorously via a complete reverse-depedency check (for which results are always logged here). It so happens that CRAN then had a spurious error when re-checking on upload, and it took a fews days to square this as everybody remains busy – but the release prepared on June 10 is now on CRAN.
The full set of changes (since the last CRAN release 0.11.1.1.0) follows.

Changes in RcppArmadillo version 0.11.2.0.0 (2022-06-10)

Upgraded to Armadillo release 11.2 (Classic Roast)

faster handling of sparse submatrix column views by norm(), accu(), nonzeros()
extended randu() and randn() to allow specification of distribution parameters
internal refactoring, leading to faster compilation times



Courtesy of my CRANberries, there is a diffstat report relative to previous release. More detailed information is on the RcppArmadillo page. Questions, comments etc should go to the rcpp-devel mailing list off the R-Forge page.
If you like this or other open-source work I do, you can sponsor me at GitHub.

This post by Dirk Eddelbuettel originated on his Thinking inside the box blog. Please report excessive re-aggregation in third-party for-profit settings.



        16 June, 2022 12:11AM









     Search






     A complete feed is available in any of your favourite syndication formats linked by the buttons below.











    Last updated: 16 Jul 2022 17:00All times are UTC.
    Contact: Debian Planet Maintainers

   Planetarium

    Planet Debian
Planet Debian Derivatives
Planet Debian Spanish
Planet Debian French



    Hidden Feeds
    You currently have hidden entries. Show all

   Subscriptions


     (feed)

    Abhijith PA (feed)

    Abiola Ajadi (feed)

    Adam Rosi-Kessel (feed)

    Adnan Hodzic (feed)

    Agathe Porte (feed)

    Aigars Mahinovs (feed)

    Alastair McKinstry (feed)

    Alberto García (feed)

    Albiona Hoti (feed)

    Alejandro Rios P. (feed)

    Alessio Treglia (feed)

    Alex Muntada (feed)

    Alexander Reichle-Schmehl (feed)

    Alexander Wirt (feed)

    Alexandre Viau (feed)

    Aloïs Micard (feed)

    Ana Beatriz Guerrero Lopez (feed)

    Anastasia Tsikoza (feed)

    Andrea Veri (feed)

    Andreas Barth (feed)

    Andreas Bombe (feed)

    Andreas Metzler (feed)

    Andreas Rönnquist (feed)

    Andreas Schuldei (feed)

    Andree Leidenfrost (feed)

    Andrej Belym (feed)

    Andrej Shadura (feed)

    Andrew Cater (feed)

    Andrew Pollock (feed)

    Andy Simpkins (feed)

    Anisa Kuci (feed)

    Antoine Beaupré (feed)

    Anton Gladky (feed)

    Antonio Terceiro (feed)

    Antti-Juhani Kaijanaho (feed)

    Anuradha Weeraman (feed)

    Arianit Dobroshi (feed)

    Arnaud Quette (feed)

    Arnaud Rebillout (feed)

    Aron Xu (feed)

    Arthur Del Esposte (feed)

    Arthur Diniz (feed)

    Arturo Borrero González (feed)

    Athos Ribeiro (feed)

    Aurelien Jarno (feed)

    Axel Beckert (feed)

    Ayoyimika Ajibade (feed)

    Balasankar 'Balu' C (feed)

    Bastian Blank (feed)

    Bastian Venthur (feed)

    Bdale Garbee (feed)

    Ben Armstrong (feed)

    Ben Hutchings (feed)

    Benjamin Drung (feed)

    Benjamin Kerensa (feed)

    Benjamin Mako Hill (feed)

    Bernd Zeimetz (feed)

    Bernhard R. Link (feed)

    Biella Coleman (feed)

    Birger Schacht (feed)

    Bits from Debian (feed)

    Blars Blarson (feed)

    Brett Parker (feed)

    Brice Goglin (feed)

    Bálint Réczey (feed)

    C.J. Adams-Collier (feed)

    Caleb Adepitan (feed)

    Candy Tsai (feed)

    Carl Chenet (feed)

    Carlos Villegas (feed)

    Charles Plessy (feed)

    Chris Butler (feed)

    Chris Lamb (feed)

    Chris Lawrence (feed)

    Christian Kastner (feed)

    Christian Perrier (feed)

    Christine Spang (feed)

    Christoph Berg (feed)

    Christoph Egger (feed)

    Christoph Göhre (feed)

    Clint Adams (feed)

    Colin Watson (feed)

    Craig Sanders (feed)

    Craig Small (feed)

    Cyril Brulebois (feed)

    Dan Weber (feed)

    Daniel Baumann (feed)

    Daniel Burrows (feed)

    Daniel Kahn Gillmor (feed)

    Daniel Lange (feed)

    Daniel Leidert (feed)

    Daniel Silverstone (feed)

    Dariusz Dwornikowski (feed)

    David Bremner (feed)

    David Kalnischkies (feed)

    David Moreno (feed)

    David Nusinow (feed)

    David Paleino (feed)

    David Pashley (feed)

    David Watson (feed)

    David Wendt Jr. (feed)

    Davide Viti (feed)

    DebConf team (feed)

    Debian GSoC Kotlin project blog (feed)

    Debian Java Packaging Team (feed)

    Debian Med (feed)

    Debian Project Leader (feed)

    Debian Social Team (feed)

    Debian Sysadmin Team (feed)

    Debian XMPP Team (feed)

    Debichem Team (feed)

    Deepanshu Gajbhiye (feed)

    Didier Raboud (feed)

    Dima Kogan (feed)

    Dimitri John Ledkov (feed)

    Dirk Eddelbuettel (feed)

    Dmitry Shachnev (feed)

    Dogukan Celik (feed)

    Dominique Dumont (feed)

    Don Armstrong (feed)

    Ean Schuessler (feed)

    Eddy Petrișor (feed)

    Eduard Sanou (feed)

    Eduardo Marcel Macan (feed)

    Edward Betts (feed)

    Elana Hashman (feed)

    Elizabeth Ferdman (feed)

    Emanuele Rocca (feed)

    Emilio Pozuelo Monfort (feed)

    Emmanuel Kasper (feed)

    Enrico Zini (feed)

    Eriberto Mota (feed)

    Eric Dorland (feed)

    Erich Schubert (feed)

    Eugen Stan (feed)

    Eugene V. Lyubimkin (feed)

    Evgeni Golov (feed)

    Fathi Boudra (feed)

    Felipe Augusto van de Wiel (feed)

    Felipe Sateler (feed)

    Fernanda Weiden (feed)

    Filippo Giunchedi (feed)

    Floris Stoica-Marcu (feed)

    Foteini Tsiami (feed)

    Francesca Ciceri (feed)

    François Marier (feed)

    Gergely Nagy (feed)

    Giacomo Catenazzi (feed)

    Giovanni Mascellani (feed)

    Giuseppe Iuculano (feed)

    Gregor Herrmann (feed)

    Gregory Colpart (feed)

    Grzegorz B. Prokopski (feed)

    Guido Günther (feed)

    Gunnar Wolf (feed)

    Gustavo Franco (feed)

    Gustavo R. Montesino (feed)

    Hideki Yamane (feed)

    Holger Levsen (feed)

    Héctor Orón Martínez (feed)

    Iain R. Learmonth (feed)

    Ian Campbell (feed)

    Ian Jackson (feed)

    Ian Wienand (feed)

    Igor Genibel (feed)

    Ingo Juergensmann (feed)

    Iustin Pop (feed)

    Jacob Adams (feed)

    Jaldhar Vyas (feed)

    James Bromberger (feed)

    James McCoy (feed)

    James Morrison (feed)

    Jameson Rollins (feed)

    Jamie McClelland (feed)

    Jaminy Prabaharan (feed)

    Jan Dittberner (feed)

    Jan Wagner (feed)

    Jeff Bailey (feed)

    Jeff Licquia (feed)

    Jelmer Vernooij (feed)

    Jeremy Bicha (feed)

    Jingjie Jiang (feed)

    Jo Shields (feed)

    Joachim Breitner (feed)

    Joerg Jaspert (feed)

    Joey Hess (feed)

    Joey Schulze (feed)

    Johannes Schauer (feed)

    John Goerzen (feed)

    John Sullivan (feed)

    Jona Azizaj (feed)

    Jonas Meurer (feed)

    Jonas Smedegaard (feed)

    Jonathan Carter (feed)

    Jonathan Dowland (feed)

    Jonathan McDowell (feed)

    Jonathan Wiltshire (feed)

    Jonathan Yu (feed)

    Jonny Lamb (feed)

    Jordi Mallach (feed)

    Jose Luis Rivas (feed)

    Jose M. Calhariz (feed)

    Joseph Bisch (feed)

    Josselin Mouette (feed)

    Juan Luis Belmonte (feed)

    Julian Andres Klode (feed)

    Julien Danjou (feed)

    Julien Viard de Galbert (feed)

    Junichi Uekawa (feed)

    Jurij Smakov (feed)

    Justus Winter (feed)

    Kai Wasserbäch (feed)

    Kai-Chung Yan (feed)

    Kartik Mistry (feed)

    Kees Cook (feed)

    Keith Packard (feed)

    Kentaro Hayashi (feed)

    Konstantinos Margaritis (feed)

    Krzysztof Tyszecki (feed)

    Kumar Appaiah (feed)

    Kunal Mehta (feed)

    Kurt Kremitzki (feed)

    Kurt Roeckx (feed)

    Laura Arjona Reina (feed)

    Leandro Doctors (feed)

    Leandro Gómez (feed)

    Leo 'costela' Antunes (feed)

    Lior Kaplan (feed)

    Lisandro Damián Nicanor Pérez Meyer (feed)

    Louis-Philippe Véronneau (feed)

    Luca Bruno (feed)

    Luca Falavigna (feed)

    Luca Favatella (feed)

    Lucas Kanashiro (feed)

    Lucas Nussbaum (feed)

    Luciano Prestes Cavalcanti (feed)

    Luke Faraone (feed)

    Lunar (feed)

    MJ Ray (feed)

    Manoj Srivastava (feed)

    Manuel A. Fernandez Montecelo (feed)

    Marc 'HE' Brockschmidt (feed)

    Marcela Tiznado (feed)

    Marco d'Itri (feed)

    Margarita Manterola (feed)

    Maria Glukhova (feed)

    Mark Brown (feed)

    Marko Lalic (feed)

    Markus Koschany (feed)

    Martin G. Loschwitz (feed)

    Martin Meredith (feed)

    Martin Michlmayr (feed)

    Martin Pitt (feed)

    Martin Zobel-Helas (feed)

    Martin-Éric Racine (feed)

    Martina Ferrari (feed)

    Masayuki Hatta (feed)

    Mateus Bellomo (feed)

    Matheus Morais (feed)

    Mathieu Parent (feed)

    Matrix on Debian blog (feed)

    Matt Brown (feed)

    Matt Hope (feed)

    Matt Zimmerman (feed)

    Matthew Garrett (feed)

    Matthew Palmer (feed)

    Matthias Klumpp (feed)

    Matthias Urlichs (feed)

    Matthieu Caneill (feed)

    Maximilian Attems (feed)

    Mehdi Dogguy (feed)

    Meike Reichle (feed)

    Melissa Wen (feed)

    Mesutcan Kurt (feed)

    Michael Ablassmeier (feed)

    Michael Banck (feed)

    Michael Banck (feed)

    Michael Casadevall (feed)

    Michael Meskes (feed)

    Michael Prokop (feed)

    Michael Stapelberg (feed)

    Michael Vogt (feed)

    Michal Čihař (feed)

    Miguel Gaiowski (feed)

    Miguel Gea (feed)

    Mike Beattie (feed)

    Mike Gabriel (feed)

    Mike Hommey (feed)

    Minkush Jain (feed)

    Mirco Bauer (feed)

    Miriam Ruiz (feed)

    Mohammed Sameer (feed)

    Molly de Blanc (feed)

    Moray Allan (feed)

    Muammar El Khatib (feed)

    NOKUBI Takatsugu (feed)

    Neil McGovern (feed)

    Neil Williams (feed)

    NeuroDebian (feed)

    Nico Golde (feed)

    Nicolas Dandrimont (feed)

    Niels Thykier (feed)

    Noah Meyerhans (feed)

    Norbert Tretkowski (feed)

    Norman García (feed)

    Obey Arthur Liu (feed)

    Olivier Berger (feed)

    Olivier Berger (feed)

    Olivier Grégoire (feed)

    Olly Betts (feed)

    Ondřej Čertík (feed)

    Osamu Aoki (feed)

    Pablo S. Torralba (feed)

    Patrick Matthäi (feed)

    Patrick Schoenfeld (feed)

    Patryk Cisek (feed)

    Pau Garcia i Quiles (feed)

    Paul Sarbinowski (feed)

    Paul Tagliamonte (feed)

    Paul Tagliamonte (feed)

    Paul Tagliamonte (feed)

    Paul Wise (feed)

    Paul van Tilburg (feed)

    Paulo Henrique de Lima Santana (feed)

    Pavit Kaur (feed)

    Pete Nuttall (feed)

    Peter Palfrader (feed)

    Petter Reinholdtsen (feed)

    Phil Hands (feed)

    Philipp Kern (feed)

    Piotr Galiszewski (feed)

    Qendresa Hoti (feed)

    Rémi Vanicat (feed)

    Raphaël Hertzog (feed)

    Raphael Geissert (feed)

    Reinhard Tartler (feed)

    Renata D'Avila (feed)

    Reproducible Builds (feed)

    Reproducible Builds (diffoscope) (feed)

    Rhonda D'Vine (feed)

    Ricardo Mones (feed)

    Richard Darst (feed)

    Richard Hartmann (feed)

    Rigved Rakshit (feed)

    Riku Voipio (feed)

    Ritesh Raj Sarraf (feed)

    Rob Bradford (feed)

    Rob Taylor (feed)

    Robert Collins (feed)

    Robert Edmonds (feed)

    Robert McQueen (feed)

    Robert Millan (feed)

    Rodrigo Siqueira (feed)

    Rogério Brito (feed)

    Roland Mas (feed)

    Romain Perier (feed)

    Ross Gammon (feed)

    Ruby Team (feed)

    Rudy Godoy (feed)

    Russ Allbery (feed)

    Russell Coker (feed)

    Ryan Kavanagh (feed)

    Ryan Murray (feed)

    Sam Hartman (feed)

    Sam Hocevar (feed)

    Sandro Knauß (feed)

    Sandro Tosi (feed)

    Santiago García Mantiñán (feed)

    Satyam Zode (feed)

    Scarlett Gately Moore (feed)

    Scott Kitterman (feed)

    Sean Harshbarger (feed)

    Sean Whitton (feed)

    Sergey Davidoff (feed)

    Sergio Alberti (feed)

    Sergio Durigan Junior (feed)

    Sergio Talens-Oliag (feed)

    Sha Liu (feed)

    Shashank Kumar (feed)

    Shawn Landden (feed)

    Shirish Agarwal (feed)

    Siegfried Gevatter (feed)

    Simon Désaulniers (feed)

    Simon Huggins (feed)

    Simon Josefsson (feed)

    Simon Kainz (feed)

    Simon Law (feed)

    Simon McVittie (feed)

    Simon Quigley (feed)

    Simon Richter (feed)

    Sjoerd Simons (feed)

    Soeren Sonnenburg (feed)

    Sorina Sandu (feed)

    Stefano Zacchiroli (feed)

    Stein Magnus Jodal (feed)

    Steinar H. Gunderson (feed)

    Stephan Lachnit (feed)

    Steve Kemp (feed)

    Steve Langasek (feed)

    Steve McIntyre (feed)

    Sune Vuorela (feed)

    Sven Hoexter (feed)

    Sylvain Beucler (feed)

    Sylvain Le Gall (feed)

    Sylvestre Ledru (feed)

    Tanguy Ortolo (feed)

    Taowa (feed)

    Thadeu Lima de Souza Cascardo (feed)

    Theppitak Karoonboonyanan (feed)

    Thibaut Girka (feed)

    Thomas Girard (feed)

    Thomas Goirand (feed)

    Thomas Koch (feed)

    Thomas Lange (feed)

    Thorsten Alteholz (feed)

    Tiago Bortoletto Vaz (feed)

    Tianon Gravi (feed)

    Tim Retout (feed)

    Timo Jyrinki (feed)

    Todd Troxell (feed)

    Tollef Fog Heen (feed)

    Ulrich Dangel (feed)

    Ulrike Uhlig (feed)

    Urvika Gola (feed)

    Utkarsh Gupta (feed)

    Uwe Hermann (feed)

    Uwe Kleine-König (feed)

    Vagrant Cascadian (feed)

    Valerie Young (feed)

    Vasudev Kamath (feed)

    Vincent Bernat (feed)

    Vincent Cheng (feed)

    Vincent Fourmond (feed)

    Vincent Sanders (feed)

    Vipin Nair (feed)

    Vishal Gupta (feed)

    William (Bill) Blough (feed)

    Wouter Verhelst (feed)

    Y Giridhar Appaji Nag (feed)

    Yves-Alexis Perez (feed)

    Zlatan Todorić (feed)

    intrigeri (feed)

    kpcyrd (feed)

    loldebian - Can I has a RC bug? (feed)
